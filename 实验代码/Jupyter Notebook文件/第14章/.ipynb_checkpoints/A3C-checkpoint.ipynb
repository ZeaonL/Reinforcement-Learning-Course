{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Asynchronous Advantage Actor-Critic</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在基于随机策略的AC（Actor-Critic，AC）框架深度强化学习系列方法中，一个最核心的算法是由Mnih等人提出的异步优势行动者评论家算法（Asynchronous Advantage Actor Cirtic，A3C）。该算法基于异步强化学习（Asynchronous Reinforcement Learning，ARL）思想，在AC框架中加入异步操作，使多个AC网络异步并行地工作，加快算法运行速度，使深度强化学习算法能够在CPU上快速地运行。此外，A3C算法不再使用经验回放机制，节省了内存，实现了完全在线式的强化学习方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;但由于A3C采用了多个异步并行的网络结构，所以参数多，占用内存空间大。为了解决A3C的缺陷，又提出了优势AC算法（Advantage Actor Cirtic，A2C）算法，该算法不再使用异步并行的网络结构，只用一个AC网络，其异步并行结构只用于收集样本，每次样本收集完成后再进行更新网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行动者-评论家架构\n",
    "&emsp;&emsp;行动者-评论家(Actor-Critic，AC)方法是一类结合了值函数方法和策略梯度方法的学习方法。AC算法采用一种独立的存储结构来分别表示策略函数和值函数，如图ccc.1所示。表示策略函数的结构被称为行动者，其根据当前的环境状态信息来选择Agent所执行的动作。表示值函数的结构被称为评论家，其通过计算值函数来评价行动者选择的动作的好与坏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;图ccc.1中的TD误差表示的是当前状态的1-步回报或者n-步回报与当前状态的值函数之间的差值，TD误差的计算公式如下：\n",
    "$$\\delta_t = \\sum_{i=0}^{n-1} \\gamma^{i}r_{t+i} + \\gamma^{n}V(s_{t+n}) - V(s_t) \\tag{ccc.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;其中$\\delta_t$表示的是TD误差，$r_{t+i}$表示在状态$s_{t+i}$根据策略$\\pi$采取动作$a_{t+i}$所获得的立即奖赏，$V(S_t)$表示在状态$s_t$的期望回报值，$n=1$时表示的是1-步回报，$n=k$时表示的是$k$-步回报。TD误差可以用来评估当前正被选择的动作$a_t$的好与坏。当TD误差为正时，则表明未来选择动作$a_t$的趋势应该加强；当TD误差为负时，则表明未来选择动作$a_t$的趋势应该减弱。假设动作产生于Gibbs软最大化方法：\n",
    "$$\\pi_{t}(s,a) = Pr\\{a_t=a|s_t=s\\} = \\frac{e^{p(s,a)}}{\\sum_{a'}e^{p(s,a')}} \\tag{ccc.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：$p(s_t,a_t)=p(s_t,a_t)+\\beta\\delta_t$，$\\beta$为正的步长参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='./Pictures/ccc.1.png' width='400'></center>\n",
    "<center>图ccc.1 AC结构示意图</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;AC算法有两个显著的优点：<br>\n",
    "&emsp;&emsp;（1）它是一种策略梯度算法，与值函数方法相比，AC方法在选择动作时所需的计算量相对较小。因为在值函数方法中，需要计算出当前状态下所有可能的动作的函数值，并且值函数方法无法用在连续动作空间任务中。AC方法的策略是明确的，行动者的动作选择仅通过策略参数直接选择，不需要计算当前状态下的所有行动的函数值。既使动作空间是连续的情况，AC算法在选择动作时也不需要为每次的动作选择在无穷的动作空间中做大量的计算。<br>\n",
    "&emsp;&emsp;（2）AC算法通过对策略的直接更新来对策略进行改进，该方式能使Agent学习到一个确定的随机策略。而值函数方法是通过状态-动作值函数来选择动作的，Agent往往学习到的是确定的策略。AC方法甚至可以用来解决非MDP问题。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C算法\n",
    "&emsp;&emsp;深度Q网络方法通过利用深度神经网络强大的特征识别能力，使强化学习算法在大规模状态空间任务、高维状态空间任务以及连续动作空间控制任务中取得了令人瞩目的成果。深度Q网络方法将Agent与环境交互获得的数据存储在经验回放池中，每次通过选取一定小批量的数据进行更新。该方式可以打破数据之间的相关性，提升DQN算法的性能。但是经验重放机制需要更多的存储资源和计算资源，并且要求使用异策略算法。异步深度强化学习方法，如A3C，通过将异步方法引入深度强化学习方法中替代经验重放机制，利用多线程技术使多个模型同时训练，来打破数据间的相关性，提升算法的学习效果、学习速度和学习稳定性。<br>\n",
    "&emsp;&emsp;Mnih等人提出的A3C算法，其核心思想在于：通过创建多个Agent，在多个环境实例中并行且异步地执行和学习。A3C算法中异步方法的引入使得同策略和异策略的强化学习算法均能用于深度强化学习中。通过A3C架构，可以将1-步Sarsa、1-步 Q-learning、n-步Q-learning等经典算法扩展为多线程异步学习算法。A3C算法能够运行在单个机器的多个CPU线程上，而不必使用参数服务器的分布式系统，这样就可以避免通信开销和利用lock-free的高效数据同步方法。另外A3C算法既可以处理离散动作空间任务，又可以处理连续动作空间任务。由于算法采用并行异步方式，在学习过程中可以大幅度减少训练时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法的核心思想\n",
    "&emsp;&emsp;A3C算法利用异步方法，减少了算法对存储资源和计算资源的开销，同时加快了Agent的学习速度。尤其是在处理高维状态空间任务以及大规模状态空间任务时，使用异步方法的A3C算法相比于使用经验重放机制的深度强化学习算法，能够更快地使Agent获得更好的学习效果。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 异步算法\n",
    "&emsp;&emsp;DQN仅处理一个Agent与环境的交互信息。A3C在AC框架中加入了异步学习的机制，利用多个Agent与多个环境进行交互，使得训练时可以使用多线程的CPU，而不是只依赖于GPU来处理图像网络。<br>\n",
    "&emsp;&emsp;A3C异步架构如图ccc.2所示，它主要由环境（Environment）、工作组（Worker）和全局网络（Global Network）组成。工作组代表不同线程的Agent，每个工作组对应一个独立的Agent，并拥有属于自己的网络模型，分别与一个独立的环境进行交互。\n",
    "<center><img src='./Pictures/ccc.2.png' width='1000'></center>\n",
    "<center>图ccc.2 A3C异步架构图</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个工作组的网络与全局网络都共用一个网络结构，每个网络有两个输出端，一个通过softmax输出随机策略，另一个通过线性函数输出状态值函数。如图ccc.3所示。\n",
    "<center><img src='./Pictures/ccc.3.png' width='600'></center>\n",
    "<center>图ccc.3 A3C网络架构<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;A3C的工作过程如下：<br>\n",
    "&emsp;&emsp;__（1）初始化线程和网络：__ 先初始化一个全局网络，包括一个策略网络和一个价值网络；然后再创建多个与全局网络相同的子线程，即工作组，然后将全局网络的参数拷贝到各个工作组中。在实际应用时，还会创建一个全局目标价值网络，用于构成工作组网络的双价值网络架构。<br>\n",
    "&emsp;&emsp;__（2）训练工作组网络：__ 每个工作组采用不同的策略，同独立的环境进行实时交互，产生不同的经验。利用这些经验，每个工作组计算各自网络的损失函数梯度和策略梯度，并更新相关梯度信息。<br>\n",
    "&emsp;&emsp;__（3）更新全局网络：__ 由于每个工作组训练的时间存在差异，所以通常可以设定，当有一个工作组优先完成训练时（假定它是最优工作组），就利用这个工作组的梯度信息，对全局网络的参数进行更新；同时，再将全局网络的参数拷贝至当前？？工作组中，以此保证所有工作组的网络参数都是最新的。同时，初始化所有工作组的梯度信息。<br>\n",
    "&emsp;&emsp;__（4）算法终止：__ 循环（2）（3）过程直到全局网络收敛，算法结束。<br>\n",
    "&emsp;&emsp;不同的工作组使用不同的探索策略，能够学习到不同的经验，保证了算法的有效探索性。通过并行的工作组采样到的经验，进行独立的训练学习，从而降低了样本相关性，而无须采用经验回放机制。在达到同样的效果时，A3C比DQN更加节省时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 价值网络-评论家\n",
    "&emsp;&emsp;回顾DQN损失函数：\n",
    "$$L(w)=\\mathbb{E}_{\\pi_w}[(r+\\gamma\\max_{a}Q(s',a,w')-Q(s,a,w))^2] \\tag{ccc.3}$$\n",
    "&emsp;&emsp;A3C引入优势函数思想，于是将目标Q值替换为目标V值：\n",
    "$$L(w)=\\mathbb{E}_{\\pi_w}[(r+\\gamma V(s',w') - V(s,w))^2] \\tag{ccc.4}$$\n",
    "&emsp;&emsp;该损失函数是基于Q-learning预测算法的，其缺点是过度考虑每一步环境的变化，使得算法学习速度较慢。在表格法算法中，我们介绍了一种$n$-步TD算法，它能够更好地模拟历史经验，降低方差，提高算法性能。\n",
    "&emsp;&emsp;采用函数逼近方法，基于状态值函数的$n$-步回报$G_{t:t+n}$计算公式为：\n",
    "$$G_{t:t+n}=r_{t+1} + \\gamma r_{t+2} + ... + \\gamma^{n-1} r_{t+n} + \\gamma^n v_{t+n-1}(s_{t+n}) , n \\geq 1, 0 \\leq t < T-n \\tag{ccc.5}$$\n",
    "&emsp;&emsp;A3C算法采用$n$-步回报，构建损失函数：\n",
    "$$L(w)=\\mathbb{E}_{\\pi_w}[(G_{t:t+n}-V(s,w))^2] \\tag{ccc.6}$$\n",
    "&emsp;&emsp;然后采用SGD更新价值网络参数$w$:\n",
    "$$\\begin{aligned}w_{t+1} &= w_t + \\beta(G_{t:t+n} - V(S_t,w_t))\\nabla_w V(S_t,w_t) \\\\\n",
    "&= w_t + \\beta A(S_t,A_t,w_t) \\nabla_w V(S_t,w_t)\n",
    "\\end{aligned} \\tag{ccc.7}$$\n",
    "&emsp;&emsp;其中，$n$-步TD误差被替换为优势函数$A(s,a,w)$;$\\beta$为价值网络学习步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略网络-行动者\n",
    "&emsp;&emsp;策略网络使用$n$-步回报$G_{t:t+n}$替代动作值函数估计值，构建SPG方程如下所示：\n",
    "$$\\begin{aligned}\n",
    "\\nabla_{\\theta} \\hat{J}(\\theta) &= \\mathbb{E}_{S_t \\sim \\rho^{\\pi_{\\theta}}, A_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\log \\pi(A_t|S_t,\\theta)(G_{t:t+n}-V(S_t,w))] \\\\\n",
    "&= \\mathbb{E}_{S_t \\sim \\rho^{\\pi_{\\theta}}, A_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\log \\pi(A_t|S_t,\\theta)A(S_t,A_t,w)]\n",
    "\\end{aligned} \\tag{ccc.8}$$\n",
    "&emsp;&emsp;然后使用SGD法更新行动者参数$\\theta$:\n",
    "$$\\theta_{t+1}=\\theta_t + \\alpha \\nabla_{\\theta} \\log \\pi(A_t|S_t,\\theta_t)A(S_t,A_t,w_t) \\tag{ccc.9}$$\n",
    "&emsp;&emsp;其中，$\\alpha$为策略网络学习步长。A3C引入优势函数$A(s,a,w)$能够更好地对动作值进行估计，减少评估策略梯度时的偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 策略熵\n",
    "&emsp;&emsp;在实际计算过程中，A3C将策略熵$H(\\pi(\\cdot|S_t,\\theta))$加入到目标函数中：\n",
    "$$\\nabla_{\\theta} \\hat{J}(\\theta) = \\mathbb{E}_{S_t \\sim \\rho^{\\pi_{\\theta}}, A_t \\sim \\pi_{\\theta}}[\\nabla_{\\theta} \\log \\pi(A_t|S_t,\\theta)\\hat{A}(S_t,A_t,w) + \\beta \\nabla_{\\theta} H(\\pi(\\cdot|S_t,\\theta))] \\tag{ccc.10}$$\n",
    "其中，$\\beta$为温度参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;直观上，加上该正则项后目标函数更倾向于寻找熵更大的，即形状更为“扁平”的策略函数，增加了探索性，这样就不容易在训练过程中聚集到某一种策略（或者说动作）上，也避免容易收敛到次优解的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C算法如下：\n",
    "<center><img src='./Pictures/A3C+entropy.png'></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相应的模块\n",
    "import random\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActorCritic网络定义\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, device):\n",
    "        super().__init__()\n",
    "        self.act_limit = torch.as_tensor(act_limit, dtype=torch.float32, device=device)\n",
    "        self.value_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.value_layer2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.policy_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.mu_layer = nn.Linear(256, act_dim)\n",
    "        self.sigma_layer = nn.Linear(256, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        value = F.relu6(self.value_layer1(obs))\n",
    "        value = self.value_layer2(value)\n",
    "        policy = F.relu6(self.policy_layer1(obs))\n",
    "        mu = torch.tanh(self.mu_layer(policy)) * self.act_limit\n",
    "        sigma = F.softplus(self.sigma_layer(policy))\n",
    "        return value, mu, sigma\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        _, mu, sigma = self.forward(obs)\n",
    "        pi = Normal(mu, sigma)\n",
    "        return pi.sample().cpu().numpy()\n",
    "\n",
    "    def loss_func(self, states, actions, v_t, beta):\n",
    "        values, mu, sigma = self.forward(states)\n",
    "        td = v_t - values\n",
    "        value_loss = torch.squeeze(td ** 2)\n",
    "\n",
    "        pi = Normal(mu, sigma)\n",
    "        log_prob = pi.log_prob(actions).sum(axis=-1)\n",
    "        entropy = pi.entropy().sum(axis=-1)\n",
    "        policy_loss = -(log_prob * torch.squeeze(td.detach()) + beta * entropy)\n",
    "        return (value_loss + policy_loss).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=3, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=1, bias=True)\n",
      "Linear(in_features=3, out_features=256, bias=True)\n",
      "Linear(in_features=256, out_features=5, bias=True)\n",
      "Linear(in_features=256, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个行动者评论家网络,查看网络结构\n",
    "ac = ActorCritic(3, 5, 2, 'cpu')\n",
    "for ch in ac.children():\n",
    "    print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, id, device, env_name, seed, obs_dim, act_dim, act_limit,\n",
    "                 global_network, global_optimizer,\n",
    "                 gamma, beta, global_T, global_T_MAX, t_MAX,\n",
    "                 global_episode, global_return_display, global_return_record):\n",
    "        super().__init__()\n",
    "        self.id = id\n",
    "        self.device = device\n",
    "        self.env = gym.make(env_name)\n",
    "        self.env.seed(seed)\n",
    "        self.local_network = ActorCritic(obs_dim, act_dim, act_limit, self.device).to(self.device)\n",
    "        self.global_network = global_network\n",
    "        self.global_optimizer = global_optimizer\n",
    "        self.gamma, self.beta = gamma, beta\n",
    "        self.global_T, self.global_T_MAX, self.t_MAX = global_T, global_T_MAX, t_MAX\n",
    "        self.global_episode = global_episode\n",
    "        self.global_return_display = global_return_display\n",
    "        self.global_return_record = global_return_record\n",
    "\n",
    "    def update_global(self, states, actions, rewards, next_states, done, gamma, beta, optimizer):\n",
    "        if done:\n",
    "            R = 0\n",
    "        else:\n",
    "            R, mu, sigma = self.local_network.forward(next_states[-1])\n",
    "        length = rewards.size()[0]\n",
    "        v_t = torch.zeros([length, 1], dtype=torch.float32, device=self.device)\n",
    "        for i in range(length, 0, -1):\n",
    "            R = rewards[i - 1] + gamma * R\n",
    "            v_t[i - 1] = R\n",
    "        loss = self.local_network.loss_func(states, actions, v_t, beta)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for local_params, global_params in zip(self.local_network.parameters(), self.global_network.parameters()):\n",
    "            global_params._grad = local_params._grad\n",
    "        optimizer.step()\n",
    "        self.local_network.load_state_dict(self.global_network.state_dict())\n",
    "\n",
    "    def run(self):\n",
    "        t = 0\n",
    "        state, done = self.env.reset(), False\n",
    "        episode_return = 0\n",
    "        while self.global_T.value <= self.global_T_MAX:\n",
    "            t_start = t\n",
    "            buffer_states, buffer_actions, buffer_rewards, buffer_next_states = [], [], [], []\n",
    "            while not done and t - t_start != self.t_MAX:\n",
    "                if self.id == 1:\n",
    "                    render(self.env)\n",
    "                action = self.local_network.select_action(torch.as_tensor(state, dtype=torch.float32, device=self.device))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                buffer_states.append(state)\n",
    "                buffer_actions.append(action)\n",
    "                buffer_next_states.append(next_state)\n",
    "                buffer_rewards.append(reward / 10)\n",
    "                t += 1\n",
    "                with self.global_T.get_lock():\n",
    "                    self.global_T.value += 1\n",
    "                state = next_state\n",
    "            self.update_global(\n",
    "                torch.as_tensor(buffer_states, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_actions, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_rewards, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_next_states, dtype=torch.float32, device=self.device),\n",
    "                done, self.gamma, self.beta, self.global_optimizer\n",
    "            )\n",
    "            if done:\n",
    "                with self.global_episode.get_lock():\n",
    "                    self.global_episode.value += 1\n",
    "                    self.global_return_record.append(episode_return)\n",
    "                    if self.global_episode.value == 1:\n",
    "                        self.global_return_display.value = episode_return\n",
    "                    else:\n",
    "                        self.global_return_display.value *= 0.99\n",
    "                        self.global_return_display.value += 0.01 * episode_return\n",
    "                        print('Process: ', self.id, '\\tepisode: ', self.global_episode.value, '\\tepisode_return: ', self.global_return_display.value)\n",
    "                episode_return = 0\n",
    "                state, done = self.env.reset(), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = 'cpu'\n",
    "    env_name = 'Pendulum-v0'\n",
    "    num_processes = 16\n",
    "    gamma = 0.9\n",
    "    beta = 0.01\n",
    "    lr = 1e-4\n",
    "    T_MAX = 1000000\n",
    "    t_MAX = 5\n",
    "\n",
    "    # mp.set_start_method('spawn')\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    act_limit = env.action_space.high\n",
    "    global_network = ActorCritic(obs_dim, act_dim, act_limit, device).to(device)\n",
    "    global_network.share_memory()\n",
    "    optimizer = Adam(global_network.parameters(), lr=lr)\n",
    "    global_episode = mp.Value('i', 0)\n",
    "    global_T = mp.Value('i', 0)\n",
    "    global_return_display = mp.Value('d', 0)\n",
    "    global_return_record = mp.Manager().list()\n",
    "\n",
    "    workers = [Worker(i, device, env_name, obs_dim, act_dim, act_limit, \\\n",
    "                      global_network, optimizer, gamma, beta,\n",
    "                      global_T, T_MAX, t_MAX, global_episode,\n",
    "                      global_return_display, global_return_record) for i in range(num_processes)]\n",
    "    # print(workers)\n",
    "    [worker.start() for worker in workers]\n",
    "    [worker.join() for worker in workers]\n",
    "\n",
    "    torch.save(global_network, 'a3c_model.pth')\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    save_name = 'a3c_gamma=' + str(gamma) + '_beta=' + str(beta) + '_' + env_name\n",
    "    save_data = np.array(global_return_record)\n",
    "    plt.plot(save_data)\n",
    "    np.save(save_name + '.npy', save_data)\n",
    "    plt.ylabel('return')\n",
    "    plt.xlabel('episode')\n",
    "    plt.savefig(save_name + '.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ！！！ jupyter不能够执行多线程，只能保存成文件执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting A3C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile A3C.py\n",
    "\n",
    "# 导入相应的模块\n",
    "import random\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "    \n",
    "# ActorCritic网络定义\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, device):\n",
    "        super().__init__()\n",
    "        self.act_limit = torch.as_tensor(act_limit, dtype=torch.float32, device=device)\n",
    "        self.value_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.value_layer2 = nn.Linear(256, 1)\n",
    "        self.policy_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.mu_layer = nn.Linear(256, act_dim)\n",
    "        self.sigma_layer = nn.Linear(256, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        value = F.relu6(self.value_layer1(obs))\n",
    "        value = self.value_layer2(value)\n",
    "        policy = F.relu6(self.policy_layer1(obs))\n",
    "        mu = torch.tanh(self.mu_layer(policy)) * self.act_limit\n",
    "        sigma = F.softplus(self.sigma_layer(policy))\n",
    "        return value, mu, sigma\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        _, mu, sigma = self.forward(obs)\n",
    "        pi = Normal(mu, sigma)\n",
    "        return pi.sample().cpu().numpy()\n",
    "\n",
    "    def loss_func(self, states, actions, v_t, beta):\n",
    "        values, mu, sigma = self.forward(states)\n",
    "        td = v_t - values\n",
    "        value_loss = torch.squeeze(td ** 2)\n",
    "\n",
    "        pi = Normal(mu, sigma)\n",
    "        log_prob = pi.log_prob(actions).sum(axis=-1)\n",
    "        entropy = pi.entropy().sum(axis=-1)\n",
    "        policy_loss = -(log_prob * torch.squeeze(td.detach()) + beta * entropy)\n",
    "        return (value_loss + policy_loss).mean()\n",
    "\n",
    "# Worker\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, id, device, env_name, obs_dim, act_dim, act_limit,\n",
    "                 global_network, global_optimizer,\n",
    "                 gamma, beta, global_T, global_T_MAX, t_MAX,\n",
    "                 global_episode, global_return_display, global_return_record, global_return_display_record):\n",
    "        super().__init__()\n",
    "        self.id = id\n",
    "        self.device = device\n",
    "        self.env = gym.make(env_name)\n",
    "        # self.env.seed(seed)\n",
    "        self.local_network = ActorCritic(obs_dim, act_dim, act_limit, self.device).to(self.device)\n",
    "        self.global_network = global_network\n",
    "        self.global_optimizer = global_optimizer\n",
    "        self.gamma, self.beta = gamma, beta\n",
    "        self.global_T, self.global_T_MAX, self.t_MAX = global_T, global_T_MAX, t_MAX\n",
    "        self.global_episode = global_episode\n",
    "        self.global_return_display = global_return_display\n",
    "        self.global_return_record = global_return_record\n",
    "        self.global_return_display_record = global_return_display_record\n",
    "\n",
    "    def update_global(self, states, actions, rewards, next_states, done, gamma, beta, optimizer):\n",
    "        if done:\n",
    "            R = 0\n",
    "        else:\n",
    "            R, mu, sigma = self.local_network.forward(next_states[-1])\n",
    "        length = rewards.size()[0]\n",
    "        v_t = torch.zeros([length, 1], dtype=torch.float32, device=self.device)\n",
    "        for i in range(length, 0, -1):\n",
    "            R = rewards[i - 1] + gamma * R\n",
    "            v_t[i - 1] = R\n",
    "        loss = self.local_network.loss_func(states, actions, v_t, beta)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for local_params, global_params in zip(self.local_network.parameters(), self.global_network.parameters()):\n",
    "            global_params._grad = local_params._grad\n",
    "        optimizer.step()\n",
    "        self.local_network.load_state_dict(self.global_network.state_dict())\n",
    "\n",
    "    def run(self):\n",
    "        t = 0\n",
    "        state, done = self.env.reset(), False\n",
    "        episode_return = 0\n",
    "        while self.global_T.value <= self.global_T_MAX:\n",
    "            t_start = t\n",
    "            buffer_states, buffer_actions, buffer_rewards, buffer_next_states = [], [], [], []\n",
    "            while not done and t - t_start != self.t_MAX:\n",
    "                action = self.local_network.select_action(torch.as_tensor(state, dtype=torch.float32, device=self.device))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                buffer_states.append(state)\n",
    "                buffer_actions.append(action)\n",
    "                buffer_next_states.append(next_state)\n",
    "                buffer_rewards.append(reward / 10)\n",
    "                t += 1\n",
    "                with self.global_T.get_lock():\n",
    "                    self.global_T.value += 1\n",
    "                state = next_state\n",
    "            self.update_global(\n",
    "                torch.as_tensor(buffer_states, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_actions, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_rewards, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_next_states, dtype=torch.float32, device=self.device),\n",
    "                done, self.gamma, self.beta, self.global_optimizer\n",
    "            )\n",
    "            if done:\n",
    "                with self.global_episode.get_lock():\n",
    "                    self.global_episode.value += 1\n",
    "                    self.global_return_record.append(episode_return)\n",
    "                    if self.global_episode.value == 1:\n",
    "                        self.global_return_display.value = episode_return\n",
    "                    else:\n",
    "                        self.global_return_display.value *= 0.99\n",
    "                        self.global_return_display.value += 0.01 * episode_return\n",
    "                        self.global_return_display_record.append(self.global_return_display.value)\n",
    "                        if self.global_episode.value % 10 == 0:\n",
    "                            print('Process: ', self.id, '\\tepisode: ', self.global_episode.value, '\\tepisode_return: ', self.global_return_display.value)\n",
    "                episode_return = 0\n",
    "                state, done = self.env.reset(), False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    device = 'cpu'\n",
    "    env_name = 'Pendulum-v0'\n",
    "    num_processes = 8\n",
    "    gamma = 0.9\n",
    "    beta = 0.01\n",
    "    lr = 1e-4\n",
    "    T_MAX = 1000000\n",
    "    t_MAX = 5\n",
    "\n",
    "    # mp.set_start_method('spawn')\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    act_limit = env.action_space.high\n",
    "    global_network = ActorCritic(obs_dim, act_dim, act_limit, device).to(device)\n",
    "    global_network.share_memory()\n",
    "    optimizer = Adam(global_network.parameters(), lr=lr)\n",
    "    global_episode = mp.Value('i', 0)\n",
    "    global_T = mp.Value('i', 0)\n",
    "    global_return_display = mp.Value('d', 0)\n",
    "    global_return_record = mp.Manager().list()\n",
    "    global_return_display_record = mp.Manager().list()\n",
    "\n",
    "    workers = [Worker(i, device, env_name, obs_dim, act_dim, act_limit, \\\n",
    "                      global_network, optimizer, gamma, beta,\n",
    "                      global_T, T_MAX, t_MAX, global_episode,\n",
    "                      global_return_display, global_return_record, global_return_display_record) for i in range(num_processes)]\n",
    "    # print(workers)\n",
    "    [worker.start() for worker in workers]\n",
    "    [worker.join() for worker in workers]\n",
    "\n",
    "    torch.save(global_network, 'a3c_model.pth')\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    save_name = 'a3c_gamma=' + str(gamma) + '_beta=' + str(beta) + '_' + env_name\n",
    "    save_data = np.array(global_return_record)\n",
    "    plt.plot(np.array(global_return_display_record))\n",
    "    np.save(save_name + '.npy', save_data)\n",
    "    plt.ylabel('return')\n",
    "    plt.xlabel('episode')\n",
    "    plt.savefig(save_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:  1 \tepisode:  10 \tepisode_return:  -1775.288110784047\n",
      "Process:  4 \tepisode:  20 \tepisode_return:  -1731.457024866604\n",
      "Process:  5 \tepisode:  30 \tepisode_return:  -1706.776855146351\n",
      "Process:  2 \tepisode:  40 \tepisode_return:  -1684.9488533237538\n",
      "Process:  2 \tepisode:  50 \tepisode_return:  -1657.069744644085\n",
      "Process:  1 \tepisode:  60 \tepisode_return:  -1655.8918423810123\n",
      "Process:  4 \tepisode:  70 \tepisode_return:  -1640.8392005755657\n",
      "Process:  6 \tepisode:  80 \tepisode_return:  -1629.9883820194382\n",
      "Process:  0 \tepisode:  90 \tepisode_return:  -1619.4510765845218\n",
      "Process:  7 \tepisode:  100 \tepisode_return:  -1613.858922978226\n",
      "Process:  7 \tepisode:  110 \tepisode_return:  -1600.4306682612112\n",
      "Process:  4 \tepisode:  120 \tepisode_return:  -1598.6667865158108\n",
      "Process:  5 \tepisode:  130 \tepisode_return:  -1597.0906731675013\n",
      "Process:  1 \tepisode:  140 \tepisode_return:  -1585.51191943571\n",
      "Process:  1 \tepisode:  150 \tepisode_return:  -1573.9357486134338\n",
      "Process:  7 \tepisode:  160 \tepisode_return:  -1576.5379968334007\n",
      "Process:  3 \tepisode:  170 \tepisode_return:  -1560.9946046721013\n",
      "Process:  0 \tepisode:  180 \tepisode_return:  -1550.6623738379049\n",
      "Process:  6 \tepisode:  190 \tepisode_return:  -1550.7960625918506\n",
      "Process:  4 \tepisode:  200 \tepisode_return:  -1550.880928583127\n",
      "Process:  0 \tepisode:  210 \tepisode_return:  -1543.649475667243\n",
      "Process:  6 \tepisode:  220 \tepisode_return:  -1539.9246844399495\n",
      "Process:  6 \tepisode:  230 \tepisode_return:  -1537.3726306525882\n",
      "Process:  5 \tepisode:  240 \tepisode_return:  -1535.43315073005\n",
      "Process:  2 \tepisode:  250 \tepisode_return:  -1527.3880432286824\n",
      "Process:  6 \tepisode:  260 \tepisode_return:  -1524.8201021820769\n",
      "Process:  0 \tepisode:  270 \tepisode_return:  -1518.9620885892666\n",
      "Process:  1 \tepisode:  280 \tepisode_return:  -1504.9964191150364\n",
      "Process:  7 \tepisode:  290 \tepisode_return:  -1491.6178939183008\n",
      "Process:  2 \tepisode:  300 \tepisode_return:  -1489.094900461722\n",
      "Process:  4 \tepisode:  310 \tepisode_return:  -1481.3063961499802\n",
      "Process:  6 \tepisode:  320 \tepisode_return:  -1472.1208371123162\n",
      "Process:  1 \tepisode:  330 \tepisode_return:  -1467.8495302787276\n",
      "Process:  1 \tepisode:  340 \tepisode_return:  -1472.880907373568\n",
      "Process:  2 \tepisode:  350 \tepisode_return:  -1476.130022140623\n",
      "Process:  1 \tepisode:  360 \tepisode_return:  -1474.7674983967213\n",
      "Process:  5 \tepisode:  370 \tepisode_return:  -1467.8616557473042\n",
      "Process:  7 \tepisode:  380 \tepisode_return:  -1467.442412354593\n",
      "Process:  4 \tepisode:  390 \tepisode_return:  -1457.9537251261931\n",
      "Process:  2 \tepisode:  400 \tepisode_return:  -1443.4711540040746\n",
      "Process:  0 \tepisode:  410 \tepisode_return:  -1445.5177195488045\n",
      "Process:  0 \tepisode:  420 \tepisode_return:  -1430.689952776064\n",
      "Process:  6 \tepisode:  430 \tepisode_return:  -1430.151613921494\n",
      "Process:  0 \tepisode:  440 \tepisode_return:  -1426.3918001844452\n",
      "Process:  3 \tepisode:  450 \tepisode_return:  -1425.6582876462012\n",
      "Process:  6 \tepisode:  460 \tepisode_return:  -1430.2321885651054\n",
      "Process:  4 \tepisode:  470 \tepisode_return:  -1426.2831965668586\n",
      "Process:  1 \tepisode:  480 \tepisode_return:  -1419.3187723264678\n",
      "Process:  6 \tepisode:  490 \tepisode_return:  -1411.3264851579731\n",
      "Process:  3 \tepisode:  500 \tepisode_return:  -1400.1225384538557\n",
      "Process:  1 \tepisode:  510 \tepisode_return:  -1400.6564996243433\n",
      "Process:  4 \tepisode:  520 \tepisode_return:  -1390.3911123388116\n",
      "Process:  3 \tepisode:  530 \tepisode_return:  -1396.2548139538621\n",
      "Process:  6 \tepisode:  540 \tepisode_return:  -1408.003707480863\n",
      "Process:  5 \tepisode:  550 \tepisode_return:  -1396.3683310532183\n",
      "Process:  2 \tepisode:  560 \tepisode_return:  -1391.1255102005643\n",
      "Process:  1 \tepisode:  570 \tepisode_return:  -1390.196766185536\n",
      "Process:  6 \tepisode:  580 \tepisode_return:  -1370.7099509978316\n",
      "Process:  5 \tepisode:  590 \tepisode_return:  -1364.3445318494378\n",
      "Process:  0 \tepisode:  600 \tepisode_return:  -1345.176394990179\n",
      "Process:  7 \tepisode:  610 \tepisode_return:  -1340.6712678513027\n",
      "Process:  1 \tepisode:  620 \tepisode_return:  -1329.5260573347916\n",
      "Process:  6 \tepisode:  630 \tepisode_return:  -1331.4260327514341\n",
      "Process:  1 \tepisode:  640 \tepisode_return:  -1322.7387077266073\n",
      "Process:  7 \tepisode:  650 \tepisode_return:  -1308.582251790088\n",
      "Process:  1 \tepisode:  660 \tepisode_return:  -1298.3328856097846\n",
      "Process:  0 \tepisode:  670 \tepisode_return:  -1293.6168367671846\n",
      "Process:  3 \tepisode:  680 \tepisode_return:  -1298.4513574865678\n",
      "Process:  6 \tepisode:  690 \tepisode_return:  -1307.8684397333516\n",
      "Process:  3 \tepisode:  700 \tepisode_return:  -1302.4262852586417\n",
      "Process:  6 \tepisode:  710 \tepisode_return:  -1301.2212943414788\n",
      "Process:  1 \tepisode:  720 \tepisode_return:  -1307.389649395792\n",
      "Process:  5 \tepisode:  730 \tepisode_return:  -1311.5975421680305\n",
      "Process:  5 \tepisode:  740 \tepisode_return:  -1314.801455920941\n",
      "Process:  1 \tepisode:  750 \tepisode_return:  -1314.8252235661605\n",
      "Process:  4 \tepisode:  760 \tepisode_return:  -1308.2779357876477\n",
      "Process:  4 \tepisode:  770 \tepisode_return:  -1291.543537262718\n",
      "Process:  0 \tepisode:  780 \tepisode_return:  -1291.709861762653\n",
      "Process:  2 \tepisode:  790 \tepisode_return:  -1287.4674363167421\n",
      "Process:  1 \tepisode:  800 \tepisode_return:  -1281.6957046396867\n",
      "Process:  3 \tepisode:  810 \tepisode_return:  -1271.9185897118352\n",
      "Process:  3 \tepisode:  820 \tepisode_return:  -1258.5077574464956\n",
      "Process:  3 \tepisode:  830 \tepisode_return:  -1243.6392422975148\n",
      "Process:  1 \tepisode:  840 \tepisode_return:  -1248.9622560134956\n",
      "Process:  1 \tepisode:  850 \tepisode_return:  -1231.4109133588395\n",
      "Process:  5 \tepisode:  860 \tepisode_return:  -1229.4470066844053\n",
      "Process:  7 \tepisode:  870 \tepisode_return:  -1213.944069560098\n",
      "Process:  2 \tepisode:  880 \tepisode_return:  -1202.0172174559589\n",
      "Process:  4 \tepisode:  890 \tepisode_return:  -1198.3936726220566\n",
      "Process:  1 \tepisode:  900 \tepisode_return:  -1180.8967345084857\n",
      "Process:  5 \tepisode:  910 \tepisode_return:  -1187.4516047919985\n",
      "Process:  7 \tepisode:  920 \tepisode_return:  -1178.6328872955964\n",
      "Process:  1 \tepisode:  930 \tepisode_return:  -1161.3465370109088\n",
      "Process:  2 \tepisode:  940 \tepisode_return:  -1168.7935851341883\n",
      "Process:  3 \tepisode:  950 \tepisode_return:  -1162.4751137558617\n",
      "Process:  3 \tepisode:  960 \tepisode_return:  -1173.613969009557\n",
      "Process:  0 \tepisode:  970 \tepisode_return:  -1178.4268969331188\n",
      "Process:  4 \tepisode:  980 \tepisode_return:  -1159.250446349713\n",
      "Process:  7 \tepisode:  990 \tepisode_return:  -1138.6535635214411\n",
      "Process:  2 \tepisode:  1000 \tepisode_return:  -1128.561875726353\n",
      "Process:  0 \tepisode:  1010 \tepisode_return:  -1098.2643398898497\n",
      "Process:  1 \tepisode:  1020 \tepisode_return:  -1087.2302716079041\n",
      "Process:  3 \tepisode:  1030 \tepisode_return:  -1078.4217168395571\n",
      "Process:  6 \tepisode:  1040 \tepisode_return:  -1079.2962852373832\n",
      "Process:  3 \tepisode:  1050 \tepisode_return:  -1076.4491639096436\n",
      "Process:  5 \tepisode:  1060 \tepisode_return:  -1056.3037327242407\n",
      "Process:  2 \tepisode:  1070 \tepisode_return:  -1051.4228319705203\n",
      "Process:  2 \tepisode:  1080 \tepisode_return:  -1037.908171581929\n",
      "Process:  5 \tepisode:  1090 \tepisode_return:  -1029.4182774745284\n",
      "Process:  6 \tepisode:  1100 \tepisode_return:  -1007.2246784779749\n",
      "Process:  7 \tepisode:  1110 \tepisode_return:  -1011.2214860424455\n",
      "Process:  0 \tepisode:  1120 \tepisode_return:  -1005.0759554481486\n",
      "Process:  2 \tepisode:  1130 \tepisode_return:  -979.5562758477528\n",
      "Process:  6 \tepisode:  1140 \tepisode_return:  -950.3302100745804\n",
      "Process:  5 \tepisode:  1150 \tepisode_return:  -944.5348310816807\n",
      "Process:  2 \tepisode:  1160 \tepisode_return:  -930.0183732187243\n",
      "Process:  5 \tepisode:  1170 \tepisode_return:  -917.394076688775\n",
      "Process:  2 \tepisode:  1180 \tepisode_return:  -905.6666503462089\n",
      "Process:  4 \tepisode:  1190 \tepisode_return:  -907.1638714401809\n",
      "Process:  2 \tepisode:  1200 \tepisode_return:  -893.9555684704903\n",
      "Process:  0 \tepisode:  1210 \tepisode_return:  -866.9964623325541\n",
      "Process:  6 \tepisode:  1220 \tepisode_return:  -856.6315762439056\n",
      "Process:  4 \tepisode:  1230 \tepisode_return:  -850.0877933218404\n",
      "Process:  4 \tepisode:  1240 \tepisode_return:  -832.9377744027489\n",
      "Process:  1 \tepisode:  1250 \tepisode_return:  -815.1192284853664\n",
      "Process:  5 \tepisode:  1260 \tepisode_return:  -777.5156820692341\n",
      "Process:  5 \tepisode:  1270 \tepisode_return:  -760.5740177463223\n",
      "Process:  6 \tepisode:  1280 \tepisode_return:  -763.708520035733\n",
      "Process:  3 \tepisode:  1290 \tepisode_return:  -746.484383801563\n",
      "Process:  1 \tepisode:  1300 \tepisode_return:  -719.9750846850259\n",
      "Process:  4 \tepisode:  1310 \tepisode_return:  -680.3327969449921\n",
      "Process:  6 \tepisode:  1320 \tepisode_return:  -673.206026415588\n",
      "Process:  1 \tepisode:  1330 \tepisode_return:  -650.4676612581894\n",
      "Process:  6 \tepisode:  1340 \tepisode_return:  -652.6309640300756\n",
      "Process:  2 \tepisode:  1350 \tepisode_return:  -661.0743978958901\n",
      "Process:  1 \tepisode:  1360 \tepisode_return:  -647.2312560184926\n",
      "Process:  4 \tepisode:  1370 \tepisode_return:  -621.5939752874301\n",
      "Process:  3 \tepisode:  1380 \tepisode_return:  -604.5387614169487\n",
      "Process:  5 \tepisode:  1390 \tepisode_return:  -578.7695906063418\n",
      "Process:  2 \tepisode:  1400 \tepisode_return:  -567.5146290433775\n",
      "Process:  0 \tepisode:  1410 \tepisode_return:  -571.137237890881\n",
      "Process:  6 \tepisode:  1420 \tepisode_return:  -561.2225371905258\n",
      "Process:  5 \tepisode:  1430 \tepisode_return:  -556.4945251296466\n",
      "Process:  2 \tepisode:  1440 \tepisode_return:  -534.2251477435397\n",
      "Process:  3 \tepisode:  1450 \tepisode_return:  -515.5715632143012\n",
      "Process:  4 \tepisode:  1460 \tepisode_return:  -523.1862549751148\n",
      "Process:  6 \tepisode:  1470 \tepisode_return:  -513.2222476625856\n",
      "Process:  5 \tepisode:  1480 \tepisode_return:  -503.3184617551549\n",
      "Process:  3 \tepisode:  1490 \tepisode_return:  -491.58735335069235\n",
      "Process:  0 \tepisode:  1500 \tepisode_return:  -501.5119364141098\n",
      "Process:  3 \tepisode:  1510 \tepisode_return:  -490.63282589267925\n",
      "Process:  6 \tepisode:  1520 \tepisode_return:  -492.50150932411276\n",
      "Process:  0 \tepisode:  1530 \tepisode_return:  -478.13213159534746\n",
      "Process:  6 \tepisode:  1540 \tepisode_return:  -468.73710835492204\n",
      "Process:  7 \tepisode:  1550 \tepisode_return:  -469.08707681164213\n",
      "Process:  1 \tepisode:  1560 \tepisode_return:  -457.45894184901084\n",
      "Process:  5 \tepisode:  1570 \tepisode_return:  -464.02019920335687\n",
      "Process:  3 \tepisode:  1580 \tepisode_return:  -452.81567220003757\n",
      "Process:  7 \tepisode:  1590 \tepisode_return:  -448.92128734522095\n",
      "Process:  2 \tepisode:  1600 \tepisode_return:  -445.84705855771676\n",
      "Process:  1 \tepisode:  1610 \tepisode_return:  -452.02044235434926\n",
      "Process:  5 \tepisode:  1620 \tepisode_return:  -440.0414925988952\n",
      "Process:  4 \tepisode:  1630 \tepisode_return:  -426.7652359186763\n",
      "Process:  4 \tepisode:  1640 \tepisode_return:  -420.93646268661024\n",
      "Process:  4 \tepisode:  1650 \tepisode_return:  -425.79859477656504\n",
      "Process:  4 \tepisode:  1660 \tepisode_return:  -417.81116125798405\n",
      "Process:  2 \tepisode:  1670 \tepisode_return:  -415.18865091437465\n",
      "Process:  3 \tepisode:  1680 \tepisode_return:  -400.35011221698386\n",
      "Process:  0 \tepisode:  1690 \tepisode_return:  -406.51848214579115\n",
      "Process:  6 \tepisode:  1700 \tepisode_return:  -409.8978840986345\n",
      "Process:  5 \tepisode:  1710 \tepisode_return:  -399.5630565674718\n",
      "Process:  3 \tepisode:  1720 \tepisode_return:  -386.08161059019506\n",
      "Process:  4 \tepisode:  1730 \tepisode_return:  -383.5772330012804\n",
      "Process:  3 \tepisode:  1740 \tepisode_return:  -374.3255236379362\n",
      "Process:  6 \tepisode:  1750 \tepisode_return:  -391.9144208624332\n",
      "Process:  6 \tepisode:  1760 \tepisode_return:  -391.11243186995074\n",
      "Process:  7 \tepisode:  1770 \tepisode_return:  -390.2004340871232\n",
      "Process:  7 \tepisode:  1780 \tepisode_return:  -389.8851615038005\n",
      "Process:  2 \tepisode:  1790 \tepisode_return:  -403.6476136699528\n",
      "Process:  4 \tepisode:  1800 \tepisode_return:  -405.08103072526137\n",
      "Process:  6 \tepisode:  1810 \tepisode_return:  -396.50840246449735\n",
      "Process:  4 \tepisode:  1820 \tepisode_return:  -392.68790171089444\n",
      "Process:  7 \tepisode:  1830 \tepisode_return:  -385.8095760181229\n",
      "Process:  6 \tepisode:  1840 \tepisode_return:  -386.96914962055325\n",
      "Process:  5 \tepisode:  1850 \tepisode_return:  -377.9755581505792\n",
      "Process:  6 \tepisode:  1860 \tepisode_return:  -383.21981636259613\n",
      "Process:  6 \tepisode:  1870 \tepisode_return:  -384.99879662421324\n",
      "Process:  6 \tepisode:  1880 \tepisode_return:  -384.0889696130707\n",
      "Process:  7 \tepisode:  1890 \tepisode_return:  -376.8867160383415\n",
      "Process:  7 \tepisode:  1900 \tepisode_return:  -376.9807883274479\n",
      "Process:  5 \tepisode:  1910 \tepisode_return:  -368.6598631024961\n",
      "Process:  1 \tepisode:  1920 \tepisode_return:  -372.697802410657\n",
      "Process:  2 \tepisode:  1930 \tepisode_return:  -362.17992897030916\n",
      "Process:  0 \tepisode:  1940 \tepisode_return:  -355.2530234490024\n",
      "Process:  1 \tepisode:  1950 \tepisode_return:  -362.0711259556462\n",
      "Process:  1 \tepisode:  1960 \tepisode_return:  -367.565629260028\n",
      "Process:  7 \tepisode:  1970 \tepisode_return:  -366.02843891508644\n",
      "Process:  2 \tepisode:  1980 \tepisode_return:  -361.7082345034899\n",
      "Process:  6 \tepisode:  1990 \tepisode_return:  -347.943507611953\n",
      "Process:  3 \tepisode:  2000 \tepisode_return:  -351.5449389686619\n",
      "Process:  2 \tepisode:  2010 \tepisode_return:  -344.0938067866325\n",
      "Process:  3 \tepisode:  2020 \tepisode_return:  -335.1006870291122\n",
      "Process:  3 \tepisode:  2030 \tepisode_return:  -336.24557205589946\n",
      "Process:  7 \tepisode:  2040 \tepisode_return:  -329.1408071733999\n",
      "Process:  0 \tepisode:  2050 \tepisode_return:  -332.69075818352576\n",
      "Process:  2 \tepisode:  2060 \tepisode_return:  -330.92687604516044\n",
      "Process:  5 \tepisode:  2070 \tepisode_return:  -340.07390048567\n",
      "Process:  4 \tepisode:  2080 \tepisode_return:  -362.41930220317795\n",
      "Process:  4 \tepisode:  2090 \tepisode_return:  -367.45177823274463\n",
      "Process:  7 \tepisode:  2100 \tepisode_return:  -373.9526642906803\n",
      "Process:  3 \tepisode:  2110 \tepisode_return:  -364.58220610007464\n",
      "Process:  1 \tepisode:  2120 \tepisode_return:  -351.16552782269673\n",
      "Process:  3 \tepisode:  2130 \tepisode_return:  -348.5058250981696\n",
      "Process:  7 \tepisode:  2140 \tepisode_return:  -344.60129665534606\n",
      "Process:  7 \tepisode:  2150 \tepisode_return:  -338.70170398706733\n",
      "Process:  3 \tepisode:  2160 \tepisode_return:  -337.65799487520536\n",
      "Process:  7 \tepisode:  2170 \tepisode_return:  -323.8978162765438\n",
      "Process:  6 \tepisode:  2180 \tepisode_return:  -316.79687121303397\n",
      "Process:  3 \tepisode:  2190 \tepisode_return:  -313.3806401418962\n",
      "Process:  2 \tepisode:  2200 \tepisode_return:  -314.15678323101105\n",
      "Process:  3 \tepisode:  2210 \tepisode_return:  -316.59794557273585\n",
      "Process:  3 \tepisode:  2220 \tepisode_return:  -324.24392505356525\n",
      "Process:  3 \tepisode:  2230 \tepisode_return:  -332.3338286084218\n",
      "Process:  6 \tepisode:  2240 \tepisode_return:  -329.41781646930474\n",
      "Process:  6 \tepisode:  2250 \tepisode_return:  -320.7596330631591\n",
      "Process:  6 \tepisode:  2260 \tepisode_return:  -313.8884936112596\n",
      "Process:  5 \tepisode:  2270 \tepisode_return:  -309.11782476793667\n",
      "Process:  3 \tepisode:  2280 \tepisode_return:  -298.9920891306378\n",
      "Process:  7 \tepisode:  2290 \tepisode_return:  -300.53681440332826\n",
      "Process:  6 \tepisode:  2300 \tepisode_return:  -289.5519172020751\n",
      "Process:  5 \tepisode:  2310 \tepisode_return:  -277.0978035008869\n",
      "Process:  5 \tepisode:  2320 \tepisode_return:  -272.3718362554721\n",
      "Process:  1 \tepisode:  2330 \tepisode_return:  -265.22589766870595\n",
      "Process:  7 \tepisode:  2340 \tepisode_return:  -272.20382033447123\n",
      "Process:  7 \tepisode:  2350 \tepisode_return:  -279.3081428638584\n",
      "Process:  6 \tepisode:  2360 \tepisode_return:  -294.6702159257108\n",
      "Process:  3 \tepisode:  2370 \tepisode_return:  -324.2006510190224\n",
      "Process:  5 \tepisode:  2380 \tepisode_return:  -335.6073147366795\n",
      "Process:  5 \tepisode:  2390 \tepisode_return:  -354.424000960385\n",
      "Process:  5 \tepisode:  2400 \tepisode_return:  -355.6525135954146\n",
      "Process:  4 \tepisode:  2410 \tepisode_return:  -339.1639035012964\n",
      "Process:  6 \tepisode:  2420 \tepisode_return:  -335.5087149268276\n",
      "Process:  1 \tepisode:  2430 \tepisode_return:  -328.90369874587395\n",
      "Process:  4 \tepisode:  2440 \tepisode_return:  -328.5973278856418\n",
      "Process:  1 \tepisode:  2450 \tepisode_return:  -335.31685964965374\n",
      "Process:  6 \tepisode:  2460 \tepisode_return:  -333.21488115318766\n",
      "Process:  6 \tepisode:  2470 \tepisode_return:  -327.8443565305534\n",
      "Process:  5 \tepisode:  2480 \tepisode_return:  -334.97497130400404\n",
      "Process:  6 \tepisode:  2490 \tepisode_return:  -337.2356284467864\n",
      "Process:  3 \tepisode:  2500 \tepisode_return:  -330.0082188804435\n",
      "Process:  6 \tepisode:  2510 \tepisode_return:  -328.96269192558844\n",
      "Process:  6 \tepisode:  2520 \tepisode_return:  -319.3804312578104\n",
      "Process:  3 \tepisode:  2530 \tepisode_return:  -305.7451272589912\n",
      "Process:  2 \tepisode:  2540 \tepisode_return:  -304.7710607611819\n",
      "Process:  2 \tepisode:  2550 \tepisode_return:  -302.57247750495213\n",
      "Process:  2 \tepisode:  2560 \tepisode_return:  -296.96144475938434\n",
      "Process:  7 \tepisode:  2570 \tepisode_return:  -292.5654051016333\n",
      "Process:  1 \tepisode:  2580 \tepisode_return:  -290.57315967418344\n",
      "Process:  5 \tepisode:  2590 \tepisode_return:  -295.5778624245592\n",
      "Process:  4 \tepisode:  2600 \tepisode_return:  -298.9788910799943\n",
      "Process:  7 \tepisode:  2610 \tepisode_return:  -294.8123639040624\n",
      "Process:  3 \tepisode:  2620 \tepisode_return:  -290.3947514138654\n",
      "Process:  7 \tepisode:  2630 \tepisode_return:  -288.72234863097697\n",
      "Process:  1 \tepisode:  2640 \tepisode_return:  -296.7622147103073\n",
      "Process:  2 \tepisode:  2650 \tepisode_return:  -306.6097256246568\n",
      "Process:  3 \tepisode:  2660 \tepisode_return:  -295.20006545682344\n",
      "Process:  0 \tepisode:  2670 \tepisode_return:  -297.30750409828966\n",
      "Process:  6 \tepisode:  2680 \tepisode_return:  -288.27109990083363\n",
      "Process:  5 \tepisode:  2690 \tepisode_return:  -280.76671741092963\n",
      "Process:  0 \tepisode:  2700 \tepisode_return:  -292.99856630127\n",
      "Process:  5 \tepisode:  2710 \tepisode_return:  -290.00659214564087\n",
      "Process:  2 \tepisode:  2720 \tepisode_return:  -285.35716254533486\n",
      "Process:  7 \tepisode:  2730 \tepisode_return:  -292.5961029166324\n",
      "Process:  1 \tepisode:  2740 \tepisode_return:  -285.7729718709646\n",
      "Process:  7 \tepisode:  2750 \tepisode_return:  -278.5732178959598\n",
      "Process:  6 \tepisode:  2760 \tepisode_return:  -273.10142752496984\n",
      "Process:  1 \tepisode:  2770 \tepisode_return:  -271.7448751535852\n",
      "Process:  5 \tepisode:  2780 \tepisode_return:  -276.9480276542506\n",
      "Process:  7 \tepisode:  2790 \tepisode_return:  -273.18982322423705\n",
      "Process:  3 \tepisode:  2800 \tepisode_return:  -273.4361710023867\n",
      "Process:  7 \tepisode:  2810 \tepisode_return:  -277.6754220983903\n",
      "Process:  6 \tepisode:  2820 \tepisode_return:  -297.50700632626166\n",
      "Process:  7 \tepisode:  2830 \tepisode_return:  -290.693861876598\n",
      "Process:  5 \tepisode:  2840 \tepisode_return:  -290.6293203265841\n",
      "Process:  1 \tepisode:  2850 \tepisode_return:  -295.25037614935013\n",
      "Process:  3 \tepisode:  2860 \tepisode_return:  -292.7248972338519\n",
      "Process:  4 \tepisode:  2870 \tepisode_return:  -289.0096798218856\n",
      "Process:  0 \tepisode:  2880 \tepisode_return:  -290.5129340472988\n",
      "Process:  5 \tepisode:  2890 \tepisode_return:  -283.7188722739728\n",
      "Process:  2 \tepisode:  2900 \tepisode_return:  -284.58061130712747\n",
      "Process:  6 \tepisode:  2910 \tepisode_return:  -300.83002628411367\n",
      "Process:  3 \tepisode:  2920 \tepisode_return:  -297.9511876547279\n",
      "Process:  3 \tepisode:  2930 \tepisode_return:  -291.88851901551095\n",
      "Process:  2 \tepisode:  2940 \tepisode_return:  -294.2135183350544\n",
      "Process:  3 \tepisode:  2950 \tepisode_return:  -322.0562670179347\n",
      "Process:  7 \tepisode:  2960 \tepisode_return:  -356.3720657597785\n",
      "Process:  0 \tepisode:  2970 \tepisode_return:  -346.02006518231303\n",
      "Process:  2 \tepisode:  2980 \tepisode_return:  -345.9979728375005\n",
      "Process:  3 \tepisode:  2990 \tepisode_return:  -350.2694708161283\n",
      "Process:  0 \tepisode:  3000 \tepisode_return:  -354.78610261326094\n",
      "Process:  6 \tepisode:  3010 \tepisode_return:  -363.4100734031201\n",
      "Process:  5 \tepisode:  3020 \tepisode_return:  -347.7675483148812\n",
      "Process:  1 \tepisode:  3030 \tepisode_return:  -348.52404056872996\n",
      "Process:  6 \tepisode:  3040 \tepisode_return:  -349.72505811795077\n",
      "Process:  1 \tepisode:  3050 \tepisode_return:  -340.70873225132095\n",
      "Process:  6 \tepisode:  3060 \tepisode_return:  -324.77377411075844\n",
      "Process:  0 \tepisode:  3070 \tepisode_return:  -317.13262032796086\n",
      "Process:  1 \tepisode:  3080 \tepisode_return:  -309.7690989466704\n",
      "Process:  1 \tepisode:  3090 \tepisode_return:  -308.2952524019186\n",
      "Process:  2 \tepisode:  3100 \tepisode_return:  -297.87730830377404\n",
      "Process:  0 \tepisode:  3110 \tepisode_return:  -287.23420830381156\n",
      "Process:  0 \tepisode:  3120 \tepisode_return:  -282.88688955584536\n",
      "Process:  5 \tepisode:  3130 \tepisode_return:  -280.9300336352838\n",
      "Process:  6 \tepisode:  3140 \tepisode_return:  -284.70429989524735\n",
      "Process:  1 \tepisode:  3150 \tepisode_return:  -287.65533223309734\n",
      "Process:  5 \tepisode:  3160 \tepisode_return:  -277.1627573126782\n",
      "Process:  5 \tepisode:  3170 \tepisode_return:  -273.54514081871196\n",
      "Process:  3 \tepisode:  3180 \tepisode_return:  -287.16248322932154\n",
      "Process:  5 \tepisode:  3190 \tepisode_return:  -279.4817116323133\n",
      "Process:  3 \tepisode:  3200 \tepisode_return:  -277.97820175596956\n",
      "Process:  5 \tepisode:  3210 \tepisode_return:  -279.1348575459017\n",
      "Process:  5 \tepisode:  3220 \tepisode_return:  -279.80487922487146\n",
      "Process:  6 \tepisode:  3230 \tepisode_return:  -275.594870532284\n",
      "Process:  2 \tepisode:  3240 \tepisode_return:  -272.1290595766542\n",
      "Process:  4 \tepisode:  3250 \tepisode_return:  -267.3222159987225\n",
      "Process:  5 \tepisode:  3260 \tepisode_return:  -257.83513359258296\n",
      "Process:  4 \tepisode:  3270 \tepisode_return:  -265.5734808222871\n",
      "Process:  0 \tepisode:  3280 \tepisode_return:  -270.4685945364066\n",
      "Process:  2 \tepisode:  3290 \tepisode_return:  -274.4605833254765\n",
      "Process:  1 \tepisode:  3300 \tepisode_return:  -272.8165700406743\n",
      "Process:  4 \tepisode:  3310 \tepisode_return:  -273.8465503330169\n",
      "Process:  7 \tepisode:  3320 \tepisode_return:  -270.13336472221465\n",
      "Process:  5 \tepisode:  3330 \tepisode_return:  -267.1218557630612\n",
      "Process:  1 \tepisode:  3340 \tepisode_return:  -264.65105819366994\n",
      "Process:  6 \tepisode:  3350 \tepisode_return:  -266.06034738059617\n",
      "Process:  2 \tepisode:  3360 \tepisode_return:  -273.71235273219\n",
      "Process:  4 \tepisode:  3370 \tepisode_return:  -274.38797756228774\n",
      "Process:  4 \tepisode:  3380 \tepisode_return:  -266.90287298365064\n",
      "Process:  0 \tepisode:  3390 \tepisode_return:  -267.3907419109848\n",
      "Process:  0 \tepisode:  3400 \tepisode_return:  -271.9713164710916\n",
      "Process:  3 \tepisode:  3410 \tepisode_return:  -267.6313743758929\n",
      "Process:  1 \tepisode:  3420 \tepisode_return:  -272.00607513454895\n",
      "Process:  0 \tepisode:  3430 \tepisode_return:  -268.7233656858183\n",
      "Process:  3 \tepisode:  3440 \tepisode_return:  -266.92558272864505\n",
      "Process:  6 \tepisode:  3450 \tepisode_return:  -270.0099319213868\n",
      "Process:  7 \tepisode:  3460 \tepisode_return:  -270.1966190778633\n",
      "Process:  6 \tepisode:  3470 \tepisode_return:  -271.6385779611935\n",
      "Process:  4 \tepisode:  3480 \tepisode_return:  -270.8605804195792\n",
      "Process:  0 \tepisode:  3490 \tepisode_return:  -266.82928032629997\n",
      "Process:  4 \tepisode:  3500 \tepisode_return:  -263.2700508607072\n",
      "Process:  7 \tepisode:  3510 \tepisode_return:  -262.23315892406384\n",
      "Process:  5 \tepisode:  3520 \tepisode_return:  -261.85842348406055\n",
      "Process:  3 \tepisode:  3530 \tepisode_return:  -267.8802230144479\n",
      "Process:  5 \tepisode:  3540 \tepisode_return:  -264.12132859992687\n",
      "Process:  7 \tepisode:  3550 \tepisode_return:  -269.1367275639316\n",
      "Process:  7 \tepisode:  3560 \tepisode_return:  -264.2170686770988\n",
      "Process:  1 \tepisode:  3570 \tepisode_return:  -262.7074786649992\n",
      "Process:  2 \tepisode:  3580 \tepisode_return:  -267.9467866990117\n",
      "Process:  6 \tepisode:  3590 \tepisode_return:  -262.41641787743725\n",
      "Process:  5 \tepisode:  3600 \tepisode_return:  -265.0643940746215\n",
      "Process:  6 \tepisode:  3610 \tepisode_return:  -259.4426530474498\n",
      "Process:  1 \tepisode:  3620 \tepisode_return:  -257.44195019308296\n",
      "Process:  2 \tepisode:  3630 \tepisode_return:  -260.7243431797505\n",
      "Process:  4 \tepisode:  3640 \tepisode_return:  -255.6065776323767\n",
      "Process:  3 \tepisode:  3650 \tepisode_return:  -250.31508402519248\n",
      "Process:  0 \tepisode:  3660 \tepisode_return:  -250.63666026312129\n",
      "Process:  1 \tepisode:  3670 \tepisode_return:  -256.74961053038436\n",
      "Process:  2 \tepisode:  3680 \tepisode_return:  -258.95873235315725\n",
      "Process:  7 \tepisode:  3690 \tepisode_return:  -254.30258685436795\n",
      "Process:  4 \tepisode:  3700 \tepisode_return:  -250.45516911444952\n",
      "Process:  7 \tepisode:  3710 \tepisode_return:  -245.38410104172206\n",
      "Process:  1 \tepisode:  3720 \tepisode_return:  -244.3842551386241\n",
      "Process:  6 \tepisode:  3730 \tepisode_return:  -249.64105482407103\n",
      "Process:  4 \tepisode:  3740 \tepisode_return:  -249.3579194795504\n",
      "Process:  0 \tepisode:  3750 \tepisode_return:  -246.53157021352717\n",
      "Process:  4 \tepisode:  3760 \tepisode_return:  -246.8644924201016\n",
      "Process:  3 \tepisode:  3770 \tepisode_return:  -250.50275644222555\n",
      "Process:  6 \tepisode:  3780 \tepisode_return:  -243.04880486386492\n",
      "Process:  7 \tepisode:  3790 \tepisode_return:  -247.93500589705098\n",
      "Process:  7 \tepisode:  3800 \tepisode_return:  -251.6195076982608\n",
      "Process:  2 \tepisode:  3810 \tepisode_return:  -249.96412185177027\n",
      "Process:  0 \tepisode:  3820 \tepisode_return:  -252.29501713530618\n",
      "Process:  2 \tepisode:  3830 \tepisode_return:  -253.56756943670865\n",
      "Process:  0 \tepisode:  3840 \tepisode_return:  -250.59231327288367\n",
      "Process:  4 \tepisode:  3850 \tepisode_return:  -248.3882207335289\n",
      "Process:  4 \tepisode:  3860 \tepisode_return:  -252.626982887399\n",
      "Process:  4 \tepisode:  3870 \tepisode_return:  -252.16302463121937\n",
      "Process:  5 \tepisode:  3880 \tepisode_return:  -256.28182975404894\n",
      "Process:  6 \tepisode:  3890 \tepisode_return:  -256.9586903799652\n",
      "Process:  3 \tepisode:  3900 \tepisode_return:  -260.03595339714497\n",
      "Process:  1 \tepisode:  3910 \tepisode_return:  -259.0528150859019\n",
      "Process:  6 \tepisode:  3920 \tepisode_return:  -265.9307699778629\n",
      "Process:  1 \tepisode:  3930 \tepisode_return:  -263.7772572071376\n",
      "Process:  4 \tepisode:  3940 \tepisode_return:  -264.4932108141495\n",
      "Process:  5 \tepisode:  3950 \tepisode_return:  -259.51988900795146\n",
      "Process:  2 \tepisode:  3960 \tepisode_return:  -256.75049426849824\n",
      "Process:  6 \tepisode:  3970 \tepisode_return:  -266.18069448574505\n",
      "Process:  3 \tepisode:  3980 \tepisode_return:  -261.9178164246945\n",
      "Process:  1 \tepisode:  3990 \tepisode_return:  -258.59409349065413\n",
      "Process:  2 \tepisode:  4000 \tepisode_return:  -251.37573751548985\n",
      "Process:  4 \tepisode:  4010 \tepisode_return:  -246.6882612115187\n",
      "Process:  2 \tepisode:  4020 \tepisode_return:  -254.04185898609512\n",
      "Process:  7 \tepisode:  4030 \tepisode_return:  -263.77488972949214\n",
      "Process:  1 \tepisode:  4040 \tepisode_return:  -258.7371688422217\n",
      "Process:  3 \tepisode:  4050 \tepisode_return:  -260.5259776883429\n",
      "Process:  0 \tepisode:  4060 \tepisode_return:  -257.81847070530364\n",
      "Process:  0 \tepisode:  4070 \tepisode_return:  -257.52328776096505\n",
      "Process:  4 \tepisode:  4080 \tepisode_return:  -247.05442031084746\n",
      "Process:  0 \tepisode:  4090 \tepisode_return:  -249.1853505986912\n",
      "Process:  4 \tepisode:  4100 \tepisode_return:  -245.74829433070744\n",
      "Process:  7 \tepisode:  4110 \tepisode_return:  -246.32829067969263\n",
      "Process:  0 \tepisode:  4120 \tepisode_return:  -258.80434157899487\n",
      "Process:  6 \tepisode:  4130 \tepisode_return:  -271.77755563688214\n",
      "Process:  6 \tepisode:  4140 \tepisode_return:  -268.6747274417098\n",
      "Process:  4 \tepisode:  4150 \tepisode_return:  -265.18534595335814\n",
      "Process:  7 \tepisode:  4160 \tepisode_return:  -272.34836804667407\n",
      "Process:  2 \tepisode:  4170 \tepisode_return:  -270.701178999381\n",
      "Process:  5 \tepisode:  4180 \tepisode_return:  -263.39807062225657\n",
      "Process:  1 \tepisode:  4190 \tepisode_return:  -252.44699147718242\n",
      "Process:  1 \tepisode:  4200 \tepisode_return:  -260.7927714220815\n",
      "Process:  2 \tepisode:  4210 \tepisode_return:  -257.17815416806195\n",
      "Process:  6 \tepisode:  4220 \tepisode_return:  -254.83956076926376\n",
      "Process:  7 \tepisode:  4230 \tepisode_return:  -260.6439020803526\n",
      "Process:  6 \tepisode:  4240 \tepisode_return:  -257.91259271398104\n",
      "Process:  3 \tepisode:  4250 \tepisode_return:  -257.38230796702913\n",
      "Process:  6 \tepisode:  4260 \tepisode_return:  -257.3858093085545\n",
      "Process:  1 \tepisode:  4270 \tepisode_return:  -251.35483785907806\n",
      "Process:  4 \tepisode:  4280 \tepisode_return:  -251.25378015432275\n",
      "Process:  3 \tepisode:  4290 \tepisode_return:  -257.3115087204034\n",
      "Process:  3 \tepisode:  4300 \tepisode_return:  -263.67880745110756\n",
      "Process:  4 \tepisode:  4310 \tepisode_return:  -258.9362587913761\n",
      "Process:  7 \tepisode:  4320 \tepisode_return:  -254.15888192924638\n",
      "Process:  7 \tepisode:  4330 \tepisode_return:  -261.6120523029977\n",
      "Process:  0 \tepisode:  4340 \tepisode_return:  -257.7379789180572\n",
      "Process:  6 \tepisode:  4350 \tepisode_return:  -250.74255252565828\n",
      "Process:  2 \tepisode:  4360 \tepisode_return:  -243.16164980682535\n",
      "Process:  3 \tepisode:  4370 \tepisode_return:  -245.21686192227784\n",
      "Process:  2 \tepisode:  4380 \tepisode_return:  -248.08864888871273\n",
      "Process:  2 \tepisode:  4390 \tepisode_return:  -242.08119491933002\n",
      "Process:  5 \tepisode:  4400 \tepisode_return:  -242.70982659721028\n",
      "Process:  0 \tepisode:  4410 \tepisode_return:  -233.61128733722975\n",
      "Process:  5 \tepisode:  4420 \tepisode_return:  -237.49791239620882\n",
      "Process:  3 \tepisode:  4430 \tepisode_return:  -244.2354716768696\n",
      "Process:  7 \tepisode:  4440 \tepisode_return:  -247.8045400265915\n",
      "Process:  6 \tepisode:  4450 \tepisode_return:  -251.62679846191926\n",
      "Process:  6 \tepisode:  4460 \tepisode_return:  -252.3859849233407\n",
      "Process:  4 \tepisode:  4470 \tepisode_return:  -249.86429218440804\n",
      "Process:  5 \tepisode:  4480 \tepisode_return:  -248.594512543258\n",
      "Process:  4 \tepisode:  4490 \tepisode_return:  -254.58083164294902\n",
      "Process:  0 \tepisode:  4500 \tepisode_return:  -265.79716053372766\n",
      "Process:  7 \tepisode:  4510 \tepisode_return:  -263.0270522497749\n",
      "Process:  7 \tepisode:  4520 \tepisode_return:  -269.024716009939\n",
      "Process:  5 \tepisode:  4530 \tepisode_return:  -263.3982348159021\n",
      "Process:  7 \tepisode:  4540 \tepisode_return:  -260.012930444825\n",
      "Process:  0 \tepisode:  4550 \tepisode_return:  -261.9596568835497\n",
      "Process:  5 \tepisode:  4560 \tepisode_return:  -267.802367992319\n",
      "Process:  1 \tepisode:  4570 \tepisode_return:  -274.12745839604344\n",
      "Process:  0 \tepisode:  4580 \tepisode_return:  -275.45170557956686\n",
      "Process:  1 \tepisode:  4590 \tepisode_return:  -270.1983943514089\n",
      "Process:  3 \tepisode:  4600 \tepisode_return:  -268.0640187170979\n",
      "Process:  4 \tepisode:  4610 \tepisode_return:  -265.3591031655023\n",
      "Process:  0 \tepisode:  4620 \tepisode_return:  -264.98302984685125\n",
      "Process:  5 \tepisode:  4630 \tepisode_return:  -257.15192309555454\n",
      "Process:  4 \tepisode:  4640 \tepisode_return:  -250.99968620663674\n",
      "Process:  1 \tepisode:  4650 \tepisode_return:  -251.42345626873157\n",
      "Process:  0 \tepisode:  4660 \tepisode_return:  -252.1241063890909\n",
      "Process:  6 \tepisode:  4670 \tepisode_return:  -254.55710785413063\n",
      "Process:  3 \tepisode:  4680 \tepisode_return:  -249.14760532372466\n",
      "Process:  3 \tepisode:  4690 \tepisode_return:  -249.77054126505732\n",
      "Process:  3 \tepisode:  4700 \tepisode_return:  -247.01049712143762\n",
      "Process:  2 \tepisode:  4710 \tepisode_return:  -243.22871993568003\n",
      "Process:  0 \tepisode:  4720 \tepisode_return:  -242.50601260440536\n",
      "Process:  4 \tepisode:  4730 \tepisode_return:  -242.21709129132435\n",
      "Process:  3 \tepisode:  4740 \tepisode_return:  -247.70031728853002\n",
      "Process:  5 \tepisode:  4750 \tepisode_return:  -251.52133880688018\n",
      "Process:  6 \tepisode:  4760 \tepisode_return:  -246.47004002998818\n",
      "Process:  6 \tepisode:  4770 \tepisode_return:  -254.1483850477069\n",
      "Process:  1 \tepisode:  4780 \tepisode_return:  -254.19477057314492\n",
      "Process:  7 \tepisode:  4790 \tepisode_return:  -256.48926770033853\n",
      "Process:  4 \tepisode:  4800 \tepisode_return:  -250.68139718940094\n",
      "Process:  5 \tepisode:  4810 \tepisode_return:  -245.16976498914946\n",
      "Process:  6 \tepisode:  4820 \tepisode_return:  -245.53007595293403\n",
      "Process:  5 \tepisode:  4830 \tepisode_return:  -243.22848170987984\n",
      "Process:  1 \tepisode:  4840 \tepisode_return:  -249.25993460388503\n",
      "Process:  5 \tepisode:  4850 \tepisode_return:  -245.73951493172234\n",
      "Process:  5 \tepisode:  4860 \tepisode_return:  -243.6291926384479\n",
      "Process:  3 \tepisode:  4870 \tepisode_return:  -263.01531310740296\n",
      "Process:  2 \tepisode:  4880 \tepisode_return:  -263.88563863483233\n",
      "Process:  3 \tepisode:  4890 \tepisode_return:  -261.90402739363594\n",
      "Process:  4 \tepisode:  4900 \tepisode_return:  -253.97289972645316\n",
      "Process:  4 \tepisode:  4910 \tepisode_return:  -269.5268711135475\n",
      "Process:  6 \tepisode:  4920 \tepisode_return:  -299.19315781673305\n",
      "Process:  2 \tepisode:  4930 \tepisode_return:  -294.03502698064557\n",
      "Process:  6 \tepisode:  4940 \tepisode_return:  -279.74853005095264\n",
      "Process:  2 \tepisode:  4950 \tepisode_return:  -278.8661816164371\n",
      "Process:  2 \tepisode:  4960 \tepisode_return:  -268.1052042266119\n",
      "Process:  2 \tepisode:  4970 \tepisode_return:  -260.37803719920703\n",
      "Process:  4 \tepisode:  4980 \tepisode_return:  -257.2200619279754\n",
      "Process:  5 \tepisode:  4990 \tepisode_return:  -252.4914882658225\n"
     ]
    }
   ],
   "source": [
    "! python ./A3C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAIAAABEtEjdAAALmElEQVR4nO3dz4uc9R3A8Wc2i0FDhIIekl4Uag9ek0KhaEvAgJBci1cL/gE9+g/0UA89eBS8td7qIZH4AwsBe0suHrzYigo1otBDtKKw2e1hdI3JZjL7c+Z5z+t1WrLPzHwPM+98+D7PPDvZ2toaAGhZW/QCADh44g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7qyWyU+99957i14RHApxZ7Vs/eD1118/duzYU089Na381tbWopcGB8l7GobJZDL9wceBDJM7fD/OD8MwmUyef/75RS8HDoDJHX70ySefPPbYY4MRnvETd7jTdJfGR4NRsy0Dd5pmfW3Np4MR8/aFHUx34bdPtMLoiDvsbDq/X79+fdELgb0Qd7inl1566ezZs4teBeyFE6rEXb94ccd/P3Pp0jwPn0wmJ0+evHnz5oEuCg6duNN0r6bfYZ7E+/4qY+RdS82cWd92375PJpNHHnnkyy+/3Mei4KiJOx27zfq22X3/+OOPH3/8cZ8UxkXcGb09N/12s/s+mUw2NzddGcmIiDvjs7a2trm5Oewy62cvXx6G4dqFC/c6YEbffWeV0Vlf9AJg17a2tvaQ9dt/npH4Hb3yyisvvPDCrh4Ci2VyZ0ymTT97+fK5U6f+fObMPA+5vey327Hvs4d3HxZGxOTOOGyP6r+6fHkYhn/cuDHPo+5V9umvdju/w4iIO0vt7u0XwzPMw+0HWFLXL168u+z//e677Z9//cYbs59hxtg+5wEwXuLO0tkx61Pn33nnmdOnh2G4duHChh1wuDfniFgW81wAs7G1tT6ZbG+X/+ebb37+0EMzjp89m9+95+6EKhn23Fm8+a9rXP/p14hmlx1WmbizMPv5ZulfPvjgj08+ed/Drl24sKtLISHDnjsLMGNXfR4n1tf/+tFHcx68Y8R3e5H7q6++OufLwZKwjciROpD7wPxvY+O3b76529Hb7QdYKbZlOAoH0vRtJ9b38r6d/Z/BfW/8O72bDYyFbRkO1z53YGZ45u23D+qpZpf9888/H34Y3mEsTO4clkNq+tQ7588fYNxnO3Xq1KOPPno0rwUHRdw5YIfa9G0/e+CBYRhe/fDDPzzxxH6eZ86/pPrFF1/s51Xg6DmhyoE5mqxv+9dXXz139eqer2ic/w9knzhx4uuvv97bq8CimNzZryNu+rZfnDw57OnmjnNmfRiGl19+eRgGZWeMTO7s3aKyfruzly//7emnf/nww/McPH/WpyaTybVr187Md+N4WCrizl4sQ9an/v7pp396//19Xua4I9e2M2rizi4sT9Nv97s33/x6Y2O3XzqdTdkZO3FnLsuZ9W2/uXLlu1u3/vnss8ePHRv20fQpZSdA3LmPJc/6tt9fvfrRV1+dO3fu3Xff3fOT3Lhx4/Tp04OyM36ulmFnY2n6tn/fvPntt98++OCDk8nk1q1ba2u7/vb1dGB/7rnnXnvttUNYIBwpkzt3GlfW796BWVtbm76rP/vss1OnTs3zJNu3FvBxIMO9ZfjR4d0H5jCcuXRpx731zc3NaaNPnz49mUwmk8mLL75492FXrlxZX1+fHnD7o6DB5M7IRvVhN+dLNzc3jx8/vrGxca8D3nrrrfPnzx/QumCJiPtKG1fW93kNDKwUJ1RX0biaPsg67J64rxZZhxUh7qtiXFnXdNgncY8bV9MHWYcDIu5Z48q6psPBEvcgWQfEvWNcTR9kHQ6TuBeMK+uaDkdA3EdsXE0fZB2OkLiPkqwDs4n7yIwr65oOiyLu4zCupg+yDosm7stuXFnXdFgS4r7URlR2WYelIu7Layxll3VYQuK+pJa/7JoOy0zc2TVZh+Un7stoacd2WYexEHfuT9NhdMR96SzV2C7rMFLizg40HcZO3PkJWYcGced7sg4l4r7qNB2SxH11yTqEifsqknXIm2xtbS16DdzpkK6G1HRYHSb3lSDrsGpM7kvqoIZ3WYfVJO7Laz9913RYceK+1PbQd1kHBnFffnP2XdOB24n7OMxIvKwDdxN3gKC1RS8AgIMn7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB/wffZTF6OSwO4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x7FD5B42A8B50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 随机采取动作，Pendulum环境\n",
    "import gym\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    display.display(Image.fromarray(env.render(mode='rgb_array')))\n",
    "    display.clear_output(wait=True)\n",
    "    action = env.action_space.sample()\n",
    "    env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAIAAABEtEjdAAAVTElEQVR4nO3dbZCVdf348XN2udHlnk1QcBDFSMBJWkStacxk1jQB8QHd2qQ13YE5ZpGWSmMzDVQ+ccwip2ashpwxJxSyUm6MamwyEDQTMFIJCLnRXRd3l5XdPf8H5z/XnB8gKnv2fM9+9vV69L2uxeUzDvvmOxfXua58oVDIARBLTeoBACg/cQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIAGpB4AKm3jnDlHnJmxalWSSaD35AuFQuoZoBKObvoRJJ5IxJ1+4S3LnpF4YhB3gnv7Wc/oOwH4B1U40gn8fQDVRtyJTKbpt8SdsHpSdn8r0NeJO0BA4g7HZvNOnybuxCTN9HPiDhCQuAMEJO4AAYk7QEDiDhCQuAMEJO4AAYk7QEDiDhCQuAMEJO4AAYk7QEDiDhCQuMOxeZMqfZq4AwQk7gABiTtAQOIOEJC4AwQk7gTkBaog7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOxzBj1arUI0CPiDtAQOIOEJC4AwQk7gABiTtAQOJONBvnzEk9AqQn7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOEJC4AwQk7gABiTtAQOIOEJC4AwQk7gABDUg9AJRHZ2fn1q1bX3jhhdXbt+9obf1va+v+Q4dee+ONts7OQi63Yfbs1ANCRYk7fVWhUNi/f/8vfvGLxYsXHzp0KPU4UF3Enb5n1apVCxYs2LNnT1dXV+pZoEq55k6fsXbt2gsvvDCfz8+dO3fXrl3KDsdh5061O3z48J133nnbbbd1d3e/2a8588wz3/e+982aNauxsbHlppt6+DvOWLWqh98BkhN3qld7e/tVV121evXqo7+Uz+dHjRr1ta997bbbbjviSxsrMhtUOXGnSjU2Nq5Zs+bo82PHjn3ooYcaGhoGDRpU+amgr3DNnapzxx131NTUHF32pUuXtre3v/zyyxdddJGyw/HZuVNFtm7dOnPmzNdff7305LBhw37yk598+tOfTjUV9EV27lSLT3ziE1OmTCkt+6BBgx577LGWlhZlh3fKzp303njjjTFjxrz22mulJ7///e8vWrQon8+nmgr6NHEnsd///vdXXnll6Znp06f/6U9/GjFixAl8t41z5pRpLujbXJYhpcWLFx9R9l/+8pebNm06sbIDGTt3krnkkkvWr1+fHZ588sktLS0DBvgzCWVg504a73nPe0rLPm/evLa2NmWHchF3Epg8efLzzz+fHf785z9fsWJFwnkgHhslKm3ixIk7duwormtqav7+97+ff/75aUeCeMSdijr11FP37t1bXNfU1LS0tAwZMiTtSBCSyzJUTkNDQ1b2XC63b98+ZYdeIu5UyA033LBp06bscMeOHfX19QnngdjEnUpYs2bN3XffnR3+85//nDBhQsJ5IDxxp9c1Nzc3NjZmh+vXrz/33HMTzgP9gbjT68aPH5+tb7nllosvvjjhMNBPiDu961Of+lRbW1txPX78+CVLlqSdB/oJcacXbdq06f777y+ua2trd+7cmXYe6D/EnV40Y8aM4iKfz+/fv9/ze6FixJ3esmDBgkKhUFw3NjaOGjUq7TzQr+SzHz8oo7a2tuwDSnV1da2trZX5fcvyPPcZq1b1/JtAWnbu9IoPfvCD2XqVVkLFiTvl197enn0YdcqUKZdeemnaed4R23ZiEHfKb9q0adn6iSeeSDgJ9FviTpk1Nze/+OKLxfUVV1wxcuTIpONAPyXulNnUqVOz9YMPPphwEujPxJ0ye/nll4uLSy65pK6uLu0w0G+JO+V05ZVXZjfXLl++PO0w0J+5z52yKRQKNTX/f7tQX19/4MCBCg/gJnfI2LlTNqXvvP7Nb36TcBJA3CmbhoaGbP3hD3844SSAuFMeXV1d2aN955Tj8gjQE+JOeWzYsCFb33XXXQknAXLiTrksWLAgW5955pkJJ+lbsg98QXmJO+Xx1FNPFRfvfve7007St5Q+YQ3KSNwpg927d2frhx9+OOEkfcv69ev37NmTegpiEnfKYO3atdl60qRJCSfpW2688cZCofDtb3879SAEJO6UwT333FNc5PP5QYMGpR2mr+ju7t68eXMul/vpT3+aehYCEnfK4Mknnywuhg8fnnaSPmTZsmXFRVNTU9pJCEncKae777479Qh9xne+853iolAoLFy4MO0wxCPu9NT27duz9Wc+85mEk/Qh+/fvL332zn333ZduFmISd3rq2WefTT1C37No0aLSw+zDvVAu4k5Pbdy4sbjIHgnJW7r//vuPOPOxj30sySRE5aeRnnrmmWeKC6/meJv++Mc/vvHGG0ec9NYqykvc6ansskyVx/383/3u/N/9LvUUuVwu9/Wvf/3ok4VCobm5ueKzEJa401P//e9/i4sRI0akneTNlGa9GhL/3HPPHfP8tddeW9lBiEzc6anOzs7iYvr06UkHObZjpjxh32+66aY3+9LKlSsrOQmxiTtlM2XKlNQjHOk4ET/mlyrwjr2f/exnb/alQqHgIZGUi7hTNqeddlrqEapdR0fHwYMHj/MLvvrVr1ZsGGITd8pmyJAhqUf4P97y2kvlL86Uvn0wn88f/QseeeSRCo5DZOIOlZM9hCefzy9ZsiQ7X19fn62zzw1AT4g7ZbNv377UI/wfG2bP7uEvKK9//OMfXV1duVxu3Lhxzc3Npe+rOnDgQLap/9a3vlXJqYhK3Cmbaot7tbniiityudxnP/vZ3bt3Dx8+vKWlpfSr69atW7p0aU1NzerVqxMNSCjiTtm82e3bCR1nb17hbXtra2tTU9O6deuyZ4S98sorR/yam2+++d///veAAQPWrFlTydkISdwpm+RPEDvmjYzHjPgxT/bqfZC33HLLK6+8UvoPqtkL9gYPHpydPOuss5qamkovx8OJGZB6APq8M844Y8eOHblcrrW1NfUsx1ZMefHemApv2DNHP+m++D8tl8sNGzas9PzQoUNLX1sIJ8bOnZ4699xzi4v29va0k+SOu/veMHv2ccpegY8vHSG7iuX1VfQGcaenzjvvvOKiSh5KfgKZrnzZc7nc888/X1yU3gcJ5SLu9FRDQ0Nx0d3dnXaSzDuKdZKyl2psbEw7ACGJOz2V7dyryttMdqqyl/5FeM011ySZgdjyhUIh9Qz0edkn6X/1q19VW6o2zplzzPNpN+wrV6686qqrims/g/QGcacMsriPHj366Nu3Odo111yzfPnyXC6Xz+er53IWkbgsQxlccMEFxUVTU1PaSfqKP//5z8XFGWeckXYSohJ3yuD6668vLgqFwtFvB+VoO3fuLC4+9KEPpZ2EqMSdMpg1a1a2fumll9IN0jdkZc/lct/4xjcSTkJgrrlTHtll92nTpiV/DkGVmz9//oMPPlhc+wGkl9i5Ux4zZ84sLv71r3+lnaT6PfbYY8XFpEmT0k5CYOJOedxzzz3ZuvSyA0fLHvb7la98Je0kBCbulMeMGTOy9e23355wkiq3cOHCbH3jjTemG4TgXHOnbN71rndlN7n7c/Vmhg0b9vrrr+dyufr6+gMHDqQeh7Ds3Cmb0pd//u1vf0s4SdXq6uoqlj2Xyy1dujTtMMRm5045ZffMnH766a68H+2cc87Ztm1bce1Hj15l5045XX311cXFrl279u7dm3aYatPd3Z095vejH/1o2mEIT9wpp9/+9rfZ5v26665LO0y1ufbaa7Pd+rJly9IOQ3guy1Bm48aNy94O2tbWdvLJJ6edp0p0dnYOHDiwuB42bFh2NyT0Ejt3yqz0Q0zz589POElVKb3r8fHHH083CP2FnTvlN3HixOztz01NTSNHjkw6Tnrt7e11dXXF9eDBgw8dOpR2HvoDO3fK75lnnsnWH/jABxJOUiUuu+yybO3xDFSGuFN+w4cPf+9731tcb9my5a9//WvaedLaunVr9n+gtrbW82SoDJdl6BWvv/76sGHDiuu6urrW1ta08yQ0YsSI7J9Pm5ubR4wYkXYe+gk7d3rF0KFDv/CFLxTXbW1tc+fOTTtPKrfeemtW9unTpys7FWPnTi/K7nnP5/PNzc3Dhw9PO0+FNTU1jR49urj2rlQqzM6dXvTkk08WF4VCob6+vr/tJErfj/roo48mnIR+SNzpRTNnzvzkJz9ZXHd2dp511llp56mkWbNmHTx4sLieMWNGY2Nj2nnob8Sd3vXrX/96yJAhxfVLL73UTx71fu+9965bt664Hjx48IYNG9LOQz/kmju9rrm5edSoUdnh2rVrL7300oTz9La9e/eedtpp2U/W7t27x40bl3Yk+iE7d3rdyJEjV69enR3OmjUr8Bu0u7q6xo8fn5X9zjvvVHaSsHOnQr70pS/de++92eH27dvjfZyno6Ojrq4uuytm3rx5K1asSDsS/Za4UzlTp07dsmVLdrhnz55TTz014TzldejQodJHYE6aNGn79u0J56GfE3cq6pRTTsleHFpTU/Pqq6/G+FxPR0fHkCFDurq6ioe1tbWHDx/ObvOHynPNnYrav3//mDFjiuvu7u7Ro0cHuJNkx44dQ4cOzco+cODA9vZ2ZSctcafS9u7de/rppxfX3d3dM2fOvO+++5JO1COPPvroxIkTOzs7i4d1dXXt7e3ZezkgFXEngZ07d06cODE7vO666/roaz1uvfXWyy+/PDusr69vbW2tra1NOBIUueZOMjNmzHjqqaeyw7q6upaWlj5UxgkTJuzcuTM7HD9+/K5duxLOA6Xs3Elm48aNixYtyg7b2toGDBjwwAMPJBzpbfrLX/4yaNCg0rLPnz9f2akqdu4k9tBDD1199dWlZy644IJ169ZlDy2oKp2dnZdffvnatWtLT/7oRz9auHBhqpHgmMSd9Do6Ok455ZTsMVtFVVjMBx544OMf/3jpmbq6uv/85z+R7tYnDJdlSG/w4MEtLS3z5s0rPXn99defdNJJ2UOD03r66afr6uqOKPvFF1988OBBZac6iTvVYsWKFc8++2xdXV12pqOj48ILL6yvr1+1alWqqR555JGxY8dOnz69vb299PwTTzyxfv36mho/QVQpfzSpItOmTWttbb399ttLPwH06quvzp07N5/P//CHPzx06FBlJunu7r7rrrsGDRo0e/bsffv2Zefz+fyXv/zlQqHw/ve/vzKTwIlxzZ0qNWvWrOyR6KXGjh378MMPNzQ09MYHhbq6urZt29bY2Pi///3v6K+ed955mzdvLvtvCr3Bzp0qtXbt2ra2to985CNHnN+7d+9FF100ePDg+vr6JUuWlOu3+/GPfzxmzJhBgwZNmzbt6LJPnjz5tddeU3b6EDt3qt3hw4e/+93vfu973zvOn9Wzzz67oaHhsssua2xsnDBhwlt+z927dz/++ON/+MMfNm/evHXr1uO8uvpzn/vcD37wg/r6+hOcHhIRd/qMlStXLl68+Omnn377/8lJJ51UW1tbU1PT2tp6nIIfbfLkyd/85jc///nPv/MxoSqIO33P8uXLb7755j179ryjXr+lmpqaMWPG3HHHHV/84hfL+G0hCXGnr+ru7t6/f/+yZcuWLFnS0dFxwt+ntrZ20aJFN9xww5gxY/rQk23g+MSdIA4fPrxly5YXX3zxueee27Zt2wsvvLBv376mpqbW1tZcLjdw4MChQ4eOGzduwoQJ06ZNmzp16tlnn33OOeeU3lYPkYg7QEBuhQQISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSAgcQcISNwBAhJ3gIDEHSCg/wdwZiBYCU4gpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x7FEF34051FD0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# 这段代码用来在jupyter中代替env.render()\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "def render(env):   \n",
    "    display.display(Image.fromarray(env.render(mode='rgb_array')))\n",
    "    display.clear_output(wait=True)  \n",
    "\n",
    "a3c_model = torch.load('a3c_model.pth')\n",
    "env = gym.make('Pendulum-v0')\n",
    "state = env.reset()\n",
    "while True:\n",
    "    render(env)\n",
    "    _, mu, sigma = a3c_model.forward(torch.as_tensor(state, dtype=torch.float32))\n",
    "    action = Normal(mu, sigma).sample().cpu().numpy()\n",
    "    state, reward, done, _ = env.step(action)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
