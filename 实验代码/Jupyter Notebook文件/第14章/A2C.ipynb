{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;A2C为A3C的改进版本，也是一种简化形式。A3C用多个异步并行的工作组进行梯度累积，然后对全局网络进行异步更新，若并行的工作组过多，则网络的参数也会变得巨大，占用较多内存。为了节省内存，A2C仅使用工作组来独立采样，而不再用于累积梯度。当所有工作组的采样总量到达mini-batch大小时，就全部停止采样；全局网络再根据这些样本进行参数更新，具体更新方式与A3C一致。最后再更新工作组参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting A2C.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile A2C.py\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, device):\n",
    "        super().__init__()\n",
    "        self.act_limit = torch.as_tensor(act_limit, dtype=torch.float32, device=device)\n",
    "        self.value_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.value_layer2 = nn.Linear(256, 1)\n",
    "        self.policy_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.mu_layer = nn.Linear(256, act_dim)\n",
    "        self.sigma_layer = nn.Linear(256, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        value = F.relu6(self.value_layer1(obs))\n",
    "        value = self.value_layer2(value)\n",
    "        policy = F.relu6(self.policy_layer1(obs))\n",
    "        mu = torch.tanh(self.mu_layer(policy)) * self.act_limit\n",
    "        sigma = F.softplus(self.sigma_layer(policy))\n",
    "        return value, mu, sigma\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        _, mu, sigma = self.forward(obs)\n",
    "        pi = Normal(mu, sigma)\n",
    "        return pi.sample().cpu().numpy()\n",
    "\n",
    "    def loss_func(self, states, actions, v_t, beta):\n",
    "        values, mu, sigma = self.forward(states)\n",
    "        td = v_t - values\n",
    "        value_loss = torch.squeeze(td ** 2)\n",
    "\n",
    "        pi = Normal(mu, sigma)\n",
    "        log_prob = pi.log_prob(actions).sum(axis=-1)\n",
    "        entropy = pi.entropy().sum(axis=-1)\n",
    "        policy_loss = -(log_prob * torch.squeeze(td.detach()) + beta * entropy)\n",
    "        return (value_loss + policy_loss).mean()\n",
    "\n",
    "\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, id, device, env_name, global_network_lock,  # obs_dim, act_dim, act_limit,\n",
    "                 global_network, global_optimizer,\n",
    "                 gamma, beta, global_T, global_T_MAX, t_MAX,\n",
    "                 global_episode, global_return_display, global_return_record, global_return_display_record):\n",
    "        super().__init__()\n",
    "        self.id = id\n",
    "        self.device = device\n",
    "        self.env = gym.make(env_name)\n",
    "        self.global_network_lock = global_network_lock\n",
    "        self.global_network = global_network\n",
    "        self.global_optimizer = global_optimizer\n",
    "        self.gamma, self.beta = gamma, beta\n",
    "        self.global_T, self.global_T_MAX, self.t_MAX = global_T, global_T_MAX, t_MAX\n",
    "        self.global_episode = global_episode\n",
    "        self.global_return_display = global_return_display\n",
    "        self.global_return_record = global_return_record\n",
    "        self.global_return_display_record = global_return_display_record\n",
    "\n",
    "    def update_global(self, states, actions, rewards, next_states, done, gamma, beta, optimizer):\n",
    "        if done:\n",
    "            R = 0\n",
    "        else:\n",
    "            R, mu, sigma = self.global_network.forward(next_states[-1])\n",
    "        length = rewards.size()[0]\n",
    "        v_t = torch.zeros([length, 1], dtype=torch.float32, device=self.device)\n",
    "        for i in range(length, 0, -1):\n",
    "            R = rewards[i - 1] + gamma * R\n",
    "            v_t[i - 1] = R\n",
    "        loss = self.global_network.loss_func(states, actions, v_t, beta)\n",
    "        with self.global_network_lock.get_lock():\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        t = 0\n",
    "        state, done = self.env.reset(), False\n",
    "        episode_return = 0\n",
    "        while self.global_T.value <= self.global_T_MAX:\n",
    "            t_start = t\n",
    "            buffer_states, buffer_actions, buffer_rewards, buffer_next_states = [], [], [], []\n",
    "            while not done and t - t_start != self.t_MAX:\n",
    "                action = self.global_network.select_action(torch.as_tensor(state, dtype=torch.float32, device=self.device))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_return += reward\n",
    "                buffer_states.append(state)\n",
    "                buffer_actions.append(action)\n",
    "                buffer_next_states.append(next_state)\n",
    "                buffer_rewards.append(reward / 10)\n",
    "                t += 1\n",
    "                with self.global_T.get_lock():\n",
    "                    self.global_T.value += 1\n",
    "                state = next_state\n",
    "            self.update_global(\n",
    "                torch.as_tensor(buffer_states, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_actions, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_rewards, dtype=torch.float32, device=self.device),\n",
    "                torch.as_tensor(buffer_next_states, dtype=torch.float32, device=self.device),\n",
    "                done, self.gamma, self.beta, self.global_optimizer\n",
    "            )\n",
    "            if done:\n",
    "                with self.global_episode.get_lock():\n",
    "                    self.global_episode.value += 1\n",
    "                    self.global_return_record.append(episode_return)\n",
    "\n",
    "                    if self.global_episode.value == 1:\n",
    "                        self.global_return_display.value = episode_return\n",
    "                    else:\n",
    "                        self.global_return_display.value *= 0.99\n",
    "                        self.global_return_display.value += 0.01 * episode_return\n",
    "                        self.global_return_display_record.append(self.global_return_display.value)\n",
    "                        if self.global_episode.value % 10 == 0:\n",
    "                            print('Process: ', self.id, '\\tepisode: ', self.global_episode.value, '\\tepisode_return: ', self.global_return_display.value)\n",
    "\n",
    "                episode_return = 0\n",
    "                state, done = self.env.reset(), False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    mp.set_start_method('spawn')\n",
    "    \n",
    "    device = 'cuda:1'\n",
    "    env_name = 'Pendulum-v0'\n",
    "    num_processes = 8\n",
    "    gamma = 0.9\n",
    "    beta = 0.01\n",
    "    lr = 1e-4\n",
    "    T_MAX = 1000000\n",
    "    t_MAX = 5\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    act_limit = env.action_space.high\n",
    "    global_network_lock = mp.Value('i', 0)\n",
    "    global_network = ActorCritic(obs_dim, act_dim, act_limit, device).to(device)\n",
    "    global_network.share_memory()\n",
    "    optimizer = Adam(global_network.parameters(), lr=lr)\n",
    "    global_episode = mp.Value('i', 0)\n",
    "    global_T = mp.Value('i', 0)\n",
    "    global_return_display = mp.Value('d', 0)\n",
    "    global_return_record = mp.Manager().list()\n",
    "    global_return_display_record = mp.Manager().list()\n",
    "\n",
    "    workers = [Worker(i, device, env_name, global_network_lock,  # obs_dim, act_dim, act_limit, \\\n",
    "                      global_network, optimizer, gamma, beta,\n",
    "                      global_T, T_MAX, t_MAX, global_episode,\n",
    "                      global_return_display, global_return_record, global_return_display_record) \\\n",
    "               for i in range(num_processes)]\n",
    "    [worker.start() for worker in workers]\n",
    "    [worker.join() for worker in workers]\n",
    "\n",
    "    torch.save(global_network, 'a2c_Pendulum-v0_model.pth')\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    save_name = 'a2c_gamma=' + str(gamma) + '_beta=' + str(beta) + '_' + env_name\n",
    "    save_data = np.array(global_return_record)\n",
    "    plt.plot(np.array(global_return_display_record))\n",
    "    np.save(save_name + '.npy', save_data)\n",
    "    plt.ylabel('return')\n",
    "    plt.xlabel('episode')\n",
    "    plt.savefig(save_name + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process:  5 \tepisode:  10 \tepisode_return:  -1448.2792138810332\n",
      "Process:  7 \tepisode:  20 \tepisode_return:  -1442.8477726367014\n",
      "Process:  2 \tepisode:  30 \tepisode_return:  -1453.0968135487694\n",
      "Process:  4 \tepisode:  40 \tepisode_return:  -1479.7137683003155\n",
      "Process:  5 \tepisode:  50 \tepisode_return:  -1500.4274618269465\n",
      "Process:  7 \tepisode:  60 \tepisode_return:  -1517.0331060397266\n",
      "Process:  2 \tepisode:  70 \tepisode_return:  -1537.1683381237046\n",
      "Process:  4 \tepisode:  80 \tepisode_return:  -1555.4150097063953\n",
      "Process:  5 \tepisode:  90 \tepisode_return:  -1568.7772384386608\n",
      "Process:  7 \tepisode:  100 \tepisode_return:  -1580.5915933176216\n",
      "Process:  2 \tepisode:  110 \tepisode_return:  -1585.3569944100695\n",
      "Process:  4 \tepisode:  120 \tepisode_return:  -1591.21102506725\n",
      "Process:  5 \tepisode:  130 \tepisode_return:  -1594.8159666561571\n",
      "Process:  7 \tepisode:  140 \tepisode_return:  -1595.4378239167027\n",
      "Process:  2 \tepisode:  150 \tepisode_return:  -1595.9816799639852\n",
      "Process:  3 \tepisode:  160 \tepisode_return:  -1590.2611782901342\n",
      "Process:  5 \tepisode:  170 \tepisode_return:  -1586.375789724596\n",
      "Process:  6 \tepisode:  180 \tepisode_return:  -1582.2827833042745\n",
      "Process:  1 \tepisode:  190 \tepisode_return:  -1576.0166785348397\n",
      "Process:  3 \tepisode:  200 \tepisode_return:  -1554.4450148554104\n",
      "Process:  5 \tepisode:  210 \tepisode_return:  -1544.7069382615603\n",
      "Process:  6 \tepisode:  220 \tepisode_return:  -1543.4025175165336\n",
      "Process:  1 \tepisode:  230 \tepisode_return:  -1534.1139812066397\n",
      "Process:  0 \tepisode:  240 \tepisode_return:  -1525.8682356493298\n",
      "Process:  5 \tepisode:  250 \tepisode_return:  -1521.3731977218442\n",
      "Process:  6 \tepisode:  260 \tepisode_return:  -1519.7292619209024\n",
      "Process:  4 \tepisode:  270 \tepisode_return:  -1504.870663734103\n",
      "Process:  3 \tepisode:  280 \tepisode_return:  -1497.9879395795683\n",
      "Process:  5 \tepisode:  290 \tepisode_return:  -1501.0991636607419\n",
      "Process:  6 \tepisode:  300 \tepisode_return:  -1491.7728129433083\n",
      "Process:  4 \tepisode:  310 \tepisode_return:  -1498.165446487889\n",
      "Process:  3 \tepisode:  320 \tepisode_return:  -1483.7506254602497\n",
      "Process:  5 \tepisode:  330 \tepisode_return:  -1479.06088108027\n",
      "Process:  6 \tepisode:  340 \tepisode_return:  -1481.6688052964766\n",
      "Process:  1 \tepisode:  350 \tepisode_return:  -1477.8594438505118\n",
      "Process:  3 \tepisode:  360 \tepisode_return:  -1470.7801101697546\n",
      "Process:  5 \tepisode:  370 \tepisode_return:  -1464.6861599430458\n",
      "Process:  6 \tepisode:  380 \tepisode_return:  -1460.9688851214125\n",
      "Process:  2 \tepisode:  390 \tepisode_return:  -1451.6541714372788\n",
      "Process:  2 \tepisode:  400 \tepisode_return:  -1448.9895926433196\n",
      "Process:  5 \tepisode:  410 \tepisode_return:  -1441.421128443906\n",
      "Process:  6 \tepisode:  420 \tepisode_return:  -1438.6416511243808\n",
      "Process:  3 \tepisode:  430 \tepisode_return:  -1442.2783280669491\n",
      "Process:  1 \tepisode:  440 \tepisode_return:  -1443.3439534193385\n",
      "Process:  5 \tepisode:  450 \tepisode_return:  -1435.8118553951283\n",
      "Process:  6 \tepisode:  460 \tepisode_return:  -1430.920288114684\n",
      "Process:  2 \tepisode:  470 \tepisode_return:  -1431.9247464108207\n",
      "Process:  3 \tepisode:  480 \tepisode_return:  -1425.0749610667149\n",
      "Process:  5 \tepisode:  490 \tepisode_return:  -1415.2504412109906\n",
      "Process:  6 \tepisode:  500 \tepisode_return:  -1420.1634568442125\n",
      "Process:  2 \tepisode:  510 \tepisode_return:  -1412.2792904535409\n",
      "Process:  3 \tepisode:  520 \tepisode_return:  -1402.2511841226753\n",
      "Process:  5 \tepisode:  530 \tepisode_return:  -1403.406161493893\n",
      "Process:  6 \tepisode:  540 \tepisode_return:  -1391.6677963680993\n",
      "Process:  3 \tepisode:  550 \tepisode_return:  -1370.6988460301807\n",
      "Process:  1 \tepisode:  560 \tepisode_return:  -1359.1454726301956\n",
      "Process:  5 \tepisode:  570 \tepisode_return:  -1354.4688502997308\n",
      "Process:  7 \tepisode:  580 \tepisode_return:  -1354.5893669100387\n",
      "Process:  3 \tepisode:  590 \tepisode_return:  -1342.9099802771764\n",
      "Process:  4 \tepisode:  600 \tepisode_return:  -1325.4378449502342\n",
      "Process:  5 \tepisode:  610 \tepisode_return:  -1323.1916424919193\n",
      "Process:  6 \tepisode:  620 \tepisode_return:  -1330.4083747752825\n",
      "Process:  1 \tepisode:  630 \tepisode_return:  -1334.1720810134264\n",
      "Process:  4 \tepisode:  640 \tepisode_return:  -1325.7111513482366\n",
      "Process:  5 \tepisode:  650 \tepisode_return:  -1332.331799184198\n",
      "Process:  6 \tepisode:  660 \tepisode_return:  -1333.0988603778126\n",
      "Process:  1 \tepisode:  670 \tepisode_return:  -1329.5462703832284\n",
      "Process:  0 \tepisode:  680 \tepisode_return:  -1321.1407043974132\n",
      "Process:  5 \tepisode:  690 \tepisode_return:  -1315.440649742853\n",
      "Process:  7 \tepisode:  700 \tepisode_return:  -1314.919005665298\n",
      "Process:  1 \tepisode:  710 \tepisode_return:  -1315.0797644213903\n",
      "Process:  4 \tepisode:  720 \tepisode_return:  -1309.1677954023312\n",
      "Process:  5 \tepisode:  730 \tepisode_return:  -1310.1283031943362\n",
      "Process:  7 \tepisode:  740 \tepisode_return:  -1311.4446641403863\n",
      "Process:  1 \tepisode:  750 \tepisode_return:  -1326.9892273588148\n",
      "Process:  4 \tepisode:  760 \tepisode_return:  -1335.005213360379\n",
      "Process:  5 \tepisode:  770 \tepisode_return:  -1337.4106544016502\n",
      "Process:  7 \tepisode:  780 \tepisode_return:  -1346.507370428737\n",
      "Process:  1 \tepisode:  790 \tepisode_return:  -1345.1878880884265\n",
      "Process:  4 \tepisode:  800 \tepisode_return:  -1355.1271014442618\n",
      "Process:  5 \tepisode:  810 \tepisode_return:  -1336.1273480708887\n",
      "Process:  7 \tepisode:  820 \tepisode_return:  -1333.6976959429546\n",
      "Process:  2 \tepisode:  830 \tepisode_return:  -1331.4874293021685\n",
      "Process:  4 \tepisode:  840 \tepisode_return:  -1331.3765456562933\n",
      "Process:  5 \tepisode:  850 \tepisode_return:  -1339.3837212111216\n",
      "Process:  7 \tepisode:  860 \tepisode_return:  -1328.793572632197\n",
      "Process:  2 \tepisode:  870 \tepisode_return:  -1336.4553669634156\n",
      "Process:  4 \tepisode:  880 \tepisode_return:  -1317.246737525794\n",
      "Process:  5 \tepisode:  890 \tepisode_return:  -1312.2026310321694\n",
      "Process:  7 \tepisode:  900 \tepisode_return:  -1304.7298262864979\n",
      "Process:  2 \tepisode:  910 \tepisode_return:  -1300.9509320701407\n",
      "Process:  1 \tepisode:  920 \tepisode_return:  -1304.9110485252322\n",
      "Process:  5 \tepisode:  930 \tepisode_return:  -1283.8416461216664\n",
      "Process:  7 \tepisode:  940 \tepisode_return:  -1282.3750292489851\n",
      "Process:  3 \tepisode:  950 \tepisode_return:  -1284.0001994753482\n",
      "Process:  4 \tepisode:  960 \tepisode_return:  -1287.7378653497021\n",
      "Process:  5 \tepisode:  970 \tepisode_return:  -1291.5109232421203\n",
      "Process:  7 \tepisode:  980 \tepisode_return:  -1267.0182715690867\n",
      "Process:  3 \tepisode:  990 \tepisode_return:  -1255.195369691067\n",
      "Process:  4 \tepisode:  1000 \tepisode_return:  -1245.9143352378837\n",
      "Process:  0 \tepisode:  1010 \tepisode_return:  -1255.1848809028381\n",
      "Process:  7 \tepisode:  1020 \tepisode_return:  -1251.571381672325\n",
      "Process:  3 \tepisode:  1030 \tepisode_return:  -1243.9737591708092\n",
      "Process:  4 \tepisode:  1040 \tepisode_return:  -1233.0500490979273\n",
      "Process:  5 \tepisode:  1050 \tepisode_return:  -1234.8024950451443\n",
      "Process:  7 \tepisode:  1060 \tepisode_return:  -1236.0530994747762\n",
      "Process:  1 \tepisode:  1070 \tepisode_return:  -1225.634734021569\n",
      "Process:  3 \tepisode:  1080 \tepisode_return:  -1232.970760404977\n",
      "Process:  5 \tepisode:  1090 \tepisode_return:  -1230.1096565634798\n",
      "Process:  7 \tepisode:  1100 \tepisode_return:  -1236.0535531555508\n",
      "Process:  1 \tepisode:  1110 \tepisode_return:  -1237.9182808696414\n",
      "Process:  3 \tepisode:  1120 \tepisode_return:  -1251.651092311251\n",
      "Process:  5 \tepisode:  1130 \tepisode_return:  -1255.7292378276256\n",
      "Process:  7 \tepisode:  1140 \tepisode_return:  -1252.5249555253376\n",
      "Process:  4 \tepisode:  1150 \tepisode_return:  -1265.696099157449\n",
      "Process:  2 \tepisode:  1160 \tepisode_return:  -1264.175829163486\n",
      "Process:  5 \tepisode:  1170 \tepisode_return:  -1269.7163316357\n",
      "Process:  7 \tepisode:  1180 \tepisode_return:  -1258.5286251304954\n",
      "Process:  4 \tepisode:  1190 \tepisode_return:  -1227.2253638331576\n",
      "Process:  2 \tepisode:  1200 \tepisode_return:  -1233.4425684168796\n",
      "Process:  5 \tepisode:  1210 \tepisode_return:  -1223.2993291079333\n",
      "Process:  7 \tepisode:  1220 \tepisode_return:  -1214.8572525642933\n",
      "Process:  4 \tepisode:  1230 \tepisode_return:  -1220.2685648971803\n",
      "Process:  2 \tepisode:  1240 \tepisode_return:  -1230.5467753691144\n",
      "Process:  5 \tepisode:  1250 \tepisode_return:  -1229.3282013811115\n",
      "Process:  7 \tepisode:  1260 \tepisode_return:  -1210.6456267741994\n",
      "Process:  4 \tepisode:  1270 \tepisode_return:  -1190.1419215933581\n",
      "Process:  2 \tepisode:  1280 \tepisode_return:  -1169.6690843191082\n",
      "Process:  5 \tepisode:  1290 \tepisode_return:  -1168.4712090957855\n",
      "Process:  7 \tepisode:  1300 \tepisode_return:  -1172.8519574176464\n",
      "Process:  4 \tepisode:  1310 \tepisode_return:  -1173.6238157613352\n",
      "Process:  2 \tepisode:  1320 \tepisode_return:  -1170.0940827491238\n",
      "Process:  5 \tepisode:  1330 \tepisode_return:  -1181.8394092016256\n",
      "Process:  7 \tepisode:  1340 \tepisode_return:  -1161.6843143468698\n",
      "Process:  4 \tepisode:  1350 \tepisode_return:  -1160.5661865717582\n",
      "Process:  2 \tepisode:  1360 \tepisode_return:  -1145.1181660686639\n",
      "Process:  5 \tepisode:  1370 \tepisode_return:  -1139.2337077215823\n",
      "Process:  7 \tepisode:  1380 \tepisode_return:  -1125.5731528986016\n",
      "Process:  1 \tepisode:  1390 \tepisode_return:  -1104.5787431989822\n",
      "Process:  2 \tepisode:  1400 \tepisode_return:  -1087.7563127726462\n",
      "Process:  5 \tepisode:  1410 \tepisode_return:  -1082.779706090098\n",
      "Process:  7 \tepisode:  1420 \tepisode_return:  -1054.3890964751708\n",
      "Process:  1 \tepisode:  1430 \tepisode_return:  -1033.9762369319842\n",
      "Process:  0 \tepisode:  1440 \tepisode_return:  -1011.7494843661724\n",
      "Process:  5 \tepisode:  1450 \tepisode_return:  -973.3532003687388\n",
      "Process:  7 \tepisode:  1460 \tepisode_return:  -984.807775309418\n",
      "Process:  1 \tepisode:  1470 \tepisode_return:  -956.1113924567285\n",
      "Process:  2 \tepisode:  1480 \tepisode_return:  -945.4500151149113\n",
      "Process:  5 \tepisode:  1490 \tepisode_return:  -948.6080059570536\n",
      "Process:  7 \tepisode:  1500 \tepisode_return:  -937.4905751860817\n",
      "Process:  4 \tepisode:  1510 \tepisode_return:  -903.8027732481822\n",
      "Process:  2 \tepisode:  1520 \tepisode_return:  -892.0998712797066\n",
      "Process:  5 \tepisode:  1530 \tepisode_return:  -877.1188913382366\n",
      "Process:  7 \tepisode:  1540 \tepisode_return:  -836.9227251356074\n",
      "Process:  1 \tepisode:  1550 \tepisode_return:  -818.1012905620762\n",
      "Process:  2 \tepisode:  1560 \tepisode_return:  -820.1686838622655\n",
      "Process:  5 \tepisode:  1570 \tepisode_return:  -821.9229032587648\n",
      "Process:  7 \tepisode:  1580 \tepisode_return:  -817.4145453257304\n",
      "Process:  4 \tepisode:  1590 \tepisode_return:  -794.7178936936625\n",
      "Process:  2 \tepisode:  1600 \tepisode_return:  -760.7732794589087\n",
      "Process:  5 \tepisode:  1610 \tepisode_return:  -762.0691635955363\n",
      "Process:  7 \tepisode:  1620 \tepisode_return:  -734.577219919355\n",
      "Process:  3 \tepisode:  1630 \tepisode_return:  -728.5554291674905\n",
      "Process:  1 \tepisode:  1640 \tepisode_return:  -711.523685310339\n",
      "Process:  5 \tepisode:  1650 \tepisode_return:  -701.2742024892267\n",
      "Process:  7 \tepisode:  1660 \tepisode_return:  -673.5549695026587\n",
      "Process:  1 \tepisode:  1670 \tepisode_return:  -665.6452893499127\n",
      "Process:  3 \tepisode:  1680 \tepisode_return:  -655.918247379813\n",
      "Process:  5 \tepisode:  1690 \tepisode_return:  -632.1718799568458\n",
      "Process:  7 \tepisode:  1700 \tepisode_return:  -625.7950574616955\n",
      "Process:  1 \tepisode:  1710 \tepisode_return:  -618.5297136627082\n",
      "Process:  2 \tepisode:  1720 \tepisode_return:  -617.5298894675893\n",
      "Process:  5 \tepisode:  1730 \tepisode_return:  -600.4694337597147\n",
      "Process:  7 \tepisode:  1740 \tepisode_return:  -572.1994927802868\n",
      "Process:  4 \tepisode:  1750 \tepisode_return:  -559.5482828540277\n",
      "Process:  1 \tepisode:  1760 \tepisode_return:  -544.9632372756197\n",
      "Process:  5 \tepisode:  1770 \tepisode_return:  -564.7450212723157\n",
      "Process:  7 \tepisode:  1780 \tepisode_return:  -583.1973493434912\n",
      "Process:  3 \tepisode:  1790 \tepisode_return:  -569.5948537170176\n",
      "Process:  0 \tepisode:  1800 \tepisode_return:  -564.9875907582947\n",
      "Process:  5 \tepisode:  1810 \tepisode_return:  -553.5392535744791\n",
      "Process:  7 \tepisode:  1820 \tepisode_return:  -547.1029919316111\n",
      "Process:  3 \tepisode:  1830 \tepisode_return:  -550.4300959787889\n",
      "Process:  2 \tepisode:  1840 \tepisode_return:  -565.4828611668972\n",
      "Process:  5 \tepisode:  1850 \tepisode_return:  -564.195493965943\n",
      "Process:  7 \tepisode:  1860 \tepisode_return:  -581.2110253367706\n",
      "Process:  3 \tepisode:  1870 \tepisode_return:  -592.6940682202811\n",
      "Process:  0 \tepisode:  1880 \tepisode_return:  -615.868345918486\n",
      "Process:  5 \tepisode:  1890 \tepisode_return:  -604.0077630992697\n",
      "Process:  7 \tepisode:  1900 \tepisode_return:  -586.200983629413\n",
      "Process:  3 \tepisode:  1910 \tepisode_return:  -571.4540146732475\n",
      "Process:  1 \tepisode:  1920 \tepisode_return:  -573.1932024807579\n",
      "Process:  5 \tepisode:  1930 \tepisode_return:  -563.3533861538253\n",
      "Process:  7 \tepisode:  1940 \tepisode_return:  -547.5085279066483\n",
      "Process:  3 \tepisode:  1950 \tepisode_return:  -550.5549375332745\n",
      "Process:  0 \tepisode:  1960 \tepisode_return:  -542.604516917822\n",
      "Process:  5 \tepisode:  1970 \tepisode_return:  -550.1988071852588\n",
      "Process:  7 \tepisode:  1980 \tepisode_return:  -547.3524672371658\n",
      "Process:  3 \tepisode:  1990 \tepisode_return:  -529.2183258133268\n",
      "Process:  0 \tepisode:  2000 \tepisode_return:  -513.840075558149\n",
      "Process:  5 \tepisode:  2010 \tepisode_return:  -493.8229261996476\n",
      "Process:  7 \tepisode:  2020 \tepisode_return:  -476.41088864411364\n",
      "Process:  0 \tepisode:  2030 \tepisode_return:  -467.48809259910354\n",
      "Process:  2 \tepisode:  2040 \tepisode_return:  -472.8550217499372\n",
      "Process:  5 \tepisode:  2050 \tepisode_return:  -460.40003685902025\n",
      "Process:  7 \tepisode:  2060 \tepisode_return:  -445.9623922608324\n",
      "Process:  0 \tepisode:  2070 \tepisode_return:  -419.324033911421\n",
      "Process:  2 \tepisode:  2080 \tepisode_return:  -425.8248957423237\n",
      "Process:  5 \tepisode:  2090 \tepisode_return:  -451.0457128926541\n",
      "Process:  7 \tepisode:  2100 \tepisode_return:  -438.5873188669638\n",
      "Process:  0 \tepisode:  2110 \tepisode_return:  -443.87571807676164\n",
      "Process:  2 \tepisode:  2120 \tepisode_return:  -441.4532920366991\n",
      "Process:  5 \tepisode:  2130 \tepisode_return:  -459.3056557250829\n",
      "Process:  7 \tepisode:  2140 \tepisode_return:  -460.6525268392979\n",
      "Process:  4 \tepisode:  2150 \tepisode_return:  -447.3257808056989\n",
      "Process:  2 \tepisode:  2160 \tepisode_return:  -440.61684963576\n",
      "Process:  5 \tepisode:  2170 \tepisode_return:  -451.58411126301905\n",
      "Process:  7 \tepisode:  2180 \tepisode_return:  -437.6143252029871\n",
      "Process:  4 \tepisode:  2190 \tepisode_return:  -442.0712536672505\n",
      "Process:  2 \tepisode:  2200 \tepisode_return:  -442.80895570536296\n",
      "Process:  5 \tepisode:  2210 \tepisode_return:  -437.6043872937082\n",
      "Process:  7 \tepisode:  2220 \tepisode_return:  -432.7937989709172\n",
      "Process:  0 \tepisode:  2230 \tepisode_return:  -446.7504908259807\n",
      "Process:  1 \tepisode:  2240 \tepisode_return:  -426.5552354591921\n",
      "Process:  5 \tepisode:  2250 \tepisode_return:  -408.1021694850158\n",
      "Process:  7 \tepisode:  2260 \tepisode_return:  -390.48432061376013\n",
      "Process:  3 \tepisode:  2270 \tepisode_return:  -400.02808358809637\n",
      "Process:  0 \tepisode:  2280 \tepisode_return:  -405.78521508812895\n",
      "Process:  5 \tepisode:  2290 \tepisode_return:  -396.583310033429\n",
      "Process:  7 \tepisode:  2300 \tepisode_return:  -402.1495757126485\n",
      "Process:  1 \tepisode:  2310 \tepisode_return:  -387.6268396035204\n",
      "Process:  0 \tepisode:  2320 \tepisode_return:  -428.9162790875752\n",
      "Process:  5 \tepisode:  2330 \tepisode_return:  -437.8781900822621\n",
      "Process:  7 \tepisode:  2340 \tepisode_return:  -422.1800917042329\n",
      "Process:  3 \tepisode:  2350 \tepisode_return:  -407.4015969822204\n",
      "Process:  0 \tepisode:  2360 \tepisode_return:  -413.6600023204985\n",
      "Process:  5 \tepisode:  2370 \tepisode_return:  -405.591696947327\n",
      "Process:  7 \tepisode:  2380 \tepisode_return:  -419.59250771022295\n",
      "Process:  3 \tepisode:  2390 \tepisode_return:  -427.17730461378505\n",
      "Process:  0 \tepisode:  2400 \tepisode_return:  -436.6808437980941\n",
      "Process:  5 \tepisode:  2410 \tepisode_return:  -444.74975932696304\n",
      "Process:  7 \tepisode:  2420 \tepisode_return:  -470.3082997524982\n",
      "Process:  3 \tepisode:  2430 \tepisode_return:  -478.2441441951744\n",
      "Process:  0 \tepisode:  2440 \tepisode_return:  -468.4722107989769\n",
      "Process:  5 \tepisode:  2450 \tepisode_return:  -477.4217097754237\n",
      "Process:  7 \tepisode:  2460 \tepisode_return:  -457.7805796562413\n",
      "Process:  4 \tepisode:  2470 \tepisode_return:  -464.8743877088054\n",
      "Process:  0 \tepisode:  2480 \tepisode_return:  -467.13822239125375\n",
      "Process:  5 \tepisode:  2490 \tepisode_return:  -463.65111953703206\n",
      "Process:  7 \tepisode:  2500 \tepisode_return:  -465.62909768627947\n",
      "Process:  4 \tepisode:  2510 \tepisode_return:  -450.08266282880254\n",
      "Process:  0 \tepisode:  2520 \tepisode_return:  -430.1125633802606\n",
      "Process:  6 \tepisode:  2530 \tepisode_return:  -415.8713689094732\n",
      "Process:  7 \tepisode:  2540 \tepisode_return:  -421.8618048523469\n",
      "Process:  3 \tepisode:  2550 \tepisode_return:  -419.30208556040174\n",
      "Process:  1 \tepisode:  2560 \tepisode_return:  -415.31202348668467\n",
      "Process:  6 \tepisode:  2570 \tepisode_return:  -411.194656461656\n",
      "Process:  7 \tepisode:  2580 \tepisode_return:  -398.6868563504574\n",
      "Process:  3 \tepisode:  2590 \tepisode_return:  -406.9848329097667\n",
      "Process:  1 \tepisode:  2600 \tepisode_return:  -384.28857348930785\n",
      "Process:  6 \tepisode:  2610 \tepisode_return:  -393.68924878544334\n",
      "Process:  7 \tepisode:  2620 \tepisode_return:  -392.2847454944685\n",
      "Process:  3 \tepisode:  2630 \tepisode_return:  -397.2060509197401\n",
      "Process:  1 \tepisode:  2640 \tepisode_return:  -380.27363586611983\n",
      "Process:  6 \tepisode:  2650 \tepisode_return:  -378.82398533347254\n",
      "Process:  7 \tepisode:  2660 \tepisode_return:  -382.4164783745005\n",
      "Process:  3 \tepisode:  2670 \tepisode_return:  -376.3004698310263\n",
      "Process:  2 \tepisode:  2680 \tepisode_return:  -365.5015357605446\n",
      "Process:  6 \tepisode:  2690 \tepisode_return:  -355.92030454460786\n",
      "Process:  7 \tepisode:  2700 \tepisode_return:  -355.7554784083494\n",
      "Process:  3 \tepisode:  2710 \tepisode_return:  -360.2758267788211\n",
      "Process:  1 \tepisode:  2720 \tepisode_return:  -344.61974496365406\n",
      "Process:  6 \tepisode:  2730 \tepisode_return:  -348.51988706326017\n",
      "Process:  7 \tepisode:  2740 \tepisode_return:  -341.31559775281494\n",
      "Process:  3 \tepisode:  2750 \tepisode_return:  -348.84168919591684\n",
      "Process:  1 \tepisode:  2760 \tepisode_return:  -374.6364908134722\n",
      "Process:  6 \tepisode:  2770 \tepisode_return:  -369.1154684210707\n",
      "Process:  7 \tepisode:  2780 \tepisode_return:  -407.8105919432165\n",
      "Process:  3 \tepisode:  2790 \tepisode_return:  -397.1363429175461\n",
      "Process:  1 \tepisode:  2800 \tepisode_return:  -378.4771339142543\n",
      "Process:  6 \tepisode:  2810 \tepisode_return:  -376.7586922102362\n",
      "Process:  7 \tepisode:  2820 \tepisode_return:  -382.7646418436919\n",
      "Process:  3 \tepisode:  2830 \tepisode_return:  -395.390351003093\n",
      "Process:  0 \tepisode:  2840 \tepisode_return:  -403.113987132254\n",
      "Process:  6 \tepisode:  2850 \tepisode_return:  -402.94944520394216\n",
      "Process:  7 \tepisode:  2860 \tepisode_return:  -399.4259633019263\n",
      "Process:  3 \tepisode:  2870 \tepisode_return:  -385.426292301196\n",
      "Process:  0 \tepisode:  2880 \tepisode_return:  -381.4361079510696\n",
      "Process:  5 \tepisode:  2890 \tepisode_return:  -365.1002839858638\n",
      "Process:  7 \tepisode:  2900 \tepisode_return:  -368.2597366909979\n",
      "Process:  3 \tepisode:  2910 \tepisode_return:  -370.0962422991393\n",
      "Process:  1 \tepisode:  2920 \tepisode_return:  -382.1982179241325\n",
      "Process:  6 \tepisode:  2930 \tepisode_return:  -370.72960130709833\n",
      "Process:  7 \tepisode:  2940 \tepisode_return:  -355.4571668247495\n",
      "Process:  3 \tepisode:  2950 \tepisode_return:  -345.47686062551384\n",
      "Process:  0 \tepisode:  2960 \tepisode_return:  -344.6611434563689\n",
      "Process:  5 \tepisode:  2970 \tepisode_return:  -359.000418631029\n",
      "Process:  7 \tepisode:  2980 \tepisode_return:  -345.9173715265067\n",
      "Process:  1 \tepisode:  2990 \tepisode_return:  -339.51635940989735\n",
      "Process:  0 \tepisode:  3000 \tepisode_return:  -342.35821071850614\n",
      "Process:  5 \tepisode:  3010 \tepisode_return:  -343.6516203546835\n",
      "Process:  7 \tepisode:  3020 \tepisode_return:  -341.10936162667144\n",
      "Process:  3 \tepisode:  3030 \tepisode_return:  -336.7675106331807\n",
      "Process:  0 \tepisode:  3040 \tepisode_return:  -325.10479778569794\n",
      "Process:  5 \tepisode:  3050 \tepisode_return:  -311.65912112781587\n",
      "Process:  7 \tepisode:  3060 \tepisode_return:  -303.44664468522546\n",
      "Process:  3 \tepisode:  3070 \tepisode_return:  -312.3010402610908\n",
      "Process:  0 \tepisode:  3080 \tepisode_return:  -313.5752301325941\n",
      "Process:  5 \tepisode:  3090 \tepisode_return:  -324.8659823980281\n",
      "Process:  7 \tepisode:  3100 \tepisode_return:  -321.96693839729306\n",
      "Process:  1 \tepisode:  3110 \tepisode_return:  -318.67609645116835\n",
      "Process:  0 \tepisode:  3120 \tepisode_return:  -301.5356679712076\n",
      "Process:  5 \tepisode:  3130 \tepisode_return:  -296.61403980287633\n",
      "Process:  7 \tepisode:  3140 \tepisode_return:  -283.43784905689637\n",
      "Process:  1 \tepisode:  3150 \tepisode_return:  -270.022092786982\n",
      "Process:  0 \tepisode:  3160 \tepisode_return:  -265.8144893108272\n",
      "Process:  5 \tepisode:  3170 \tepisode_return:  -277.69751422226216\n",
      "Process:  7 \tepisode:  3180 \tepisode_return:  -261.27581722867234\n",
      "Process:  1 \tepisode:  3190 \tepisode_return:  -258.94371254081034\n",
      "Process:  0 \tepisode:  3200 \tepisode_return:  -261.8552613381082\n",
      "Process:  5 \tepisode:  3210 \tepisode_return:  -276.55454766708004\n",
      "Process:  7 \tepisode:  3220 \tepisode_return:  -273.31517266732783\n",
      "Process:  1 \tepisode:  3230 \tepisode_return:  -262.1902776809219\n",
      "Process:  0 \tepisode:  3240 \tepisode_return:  -278.82532248117803\n",
      "Process:  2 \tepisode:  3250 \tepisode_return:  -294.02910055218615\n",
      "Process:  7 \tepisode:  3260 \tepisode_return:  -304.0318695566048\n",
      "Process:  1 \tepisode:  3270 \tepisode_return:  -297.6868279430856\n",
      "Process:  0 \tepisode:  3280 \tepisode_return:  -296.0493778489265\n",
      "Process:  2 \tepisode:  3290 \tepisode_return:  -309.8641860324217\n",
      "Process:  7 \tepisode:  3300 \tepisode_return:  -304.1192165809254\n",
      "Process:  1 \tepisode:  3310 \tepisode_return:  -305.49713889972935\n",
      "Process:  0 \tepisode:  3320 \tepisode_return:  -302.42596291544857\n",
      "Process:  5 \tepisode:  3330 \tepisode_return:  -304.54579377756374\n",
      "Process:  7 \tepisode:  3340 \tepisode_return:  -309.2064132967203\n",
      "Process:  1 \tepisode:  3350 \tepisode_return:  -297.26264809810687\n",
      "Process:  0 \tepisode:  3360 \tepisode_return:  -287.48212691226405\n",
      "Process:  2 \tepisode:  3370 \tepisode_return:  -283.80935190617873\n",
      "Process:  7 \tepisode:  3380 \tepisode_return:  -274.2317038322699\n",
      "Process:  1 \tepisode:  3390 \tepisode_return:  -266.9425692238014\n",
      "Process:  3 \tepisode:  3400 \tepisode_return:  -265.18169137667064\n",
      "Process:  2 \tepisode:  3410 \tepisode_return:  -280.1813839881763\n",
      "Process:  7 \tepisode:  3420 \tepisode_return:  -280.24983198171117\n",
      "Process:  1 \tepisode:  3430 \tepisode_return:  -274.17622150459664\n",
      "Process:  3 \tepisode:  3440 \tepisode_return:  -273.0702029147296\n",
      "Process:  2 \tepisode:  3450 \tepisode_return:  -271.6980961199851\n",
      "Process:  7 \tepisode:  3460 \tepisode_return:  -272.6196770144837\n",
      "Process:  1 \tepisode:  3470 \tepisode_return:  -272.6279206619176\n",
      "Process:  3 \tepisode:  3480 \tepisode_return:  -270.3523293180907\n",
      "Process:  2 \tepisode:  3490 \tepisode_return:  -273.4247766569883\n",
      "Process:  7 \tepisode:  3500 \tepisode_return:  -271.3473489281915\n",
      "Process:  1 \tepisode:  3510 \tepisode_return:  -271.56667847056036\n",
      "Process:  3 \tepisode:  3520 \tepisode_return:  -273.1435395533676\n",
      "Process:  2 \tepisode:  3530 \tepisode_return:  -270.44208565779684\n",
      "Process:  7 \tepisode:  3540 \tepisode_return:  -261.88750013011077\n",
      "Process:  1 \tepisode:  3550 \tepisode_return:  -257.86314865254604\n",
      "Process:  3 \tepisode:  3560 \tepisode_return:  -256.6954344014293\n",
      "Process:  2 \tepisode:  3570 \tepisode_return:  -250.9575741673991\n",
      "Process:  7 \tepisode:  3580 \tepisode_return:  -252.9677627170992\n",
      "Process:  1 \tepisode:  3590 \tepisode_return:  -250.30933540497384\n",
      "Process:  3 \tepisode:  3600 \tepisode_return:  -249.8622200261485\n",
      "Process:  2 \tepisode:  3610 \tepisode_return:  -255.07598424684934\n",
      "Process:  7 \tepisode:  3620 \tepisode_return:  -253.22910421351276\n",
      "Process:  1 \tepisode:  3630 \tepisode_return:  -257.5793418484925\n",
      "Process:  6 \tepisode:  3640 \tepisode_return:  -250.23846300836476\n",
      "Process:  2 \tepisode:  3650 \tepisode_return:  -245.4057104889145\n",
      "Process:  7 \tepisode:  3660 \tepisode_return:  -247.24201022563986\n",
      "Process:  1 \tepisode:  3670 \tepisode_return:  -251.55780137998528\n",
      "Process:  3 \tepisode:  3680 \tepisode_return:  -243.98082992142199\n",
      "Process:  2 \tepisode:  3690 \tepisode_return:  -241.70589798283572\n",
      "Process:  7 \tepisode:  3700 \tepisode_return:  -241.79452231430548\n",
      "Process:  0 \tepisode:  3710 \tepisode_return:  -249.37582535705772\n",
      "Process:  6 \tepisode:  3720 \tepisode_return:  -249.1154643113432\n",
      "Process:  2 \tepisode:  3730 \tepisode_return:  -257.00370953821357\n",
      "Process:  7 \tepisode:  3740 \tepisode_return:  -256.81899257632307\n",
      "Process:  1 \tepisode:  3750 \tepisode_return:  -266.2119767947736\n",
      "Process:  6 \tepisode:  3760 \tepisode_return:  -274.0878423171151\n",
      "Process:  2 \tepisode:  3770 \tepisode_return:  -268.62601371199827\n",
      "Process:  7 \tepisode:  3780 \tepisode_return:  -265.2476716562777\n",
      "Process:  1 \tepisode:  3790 \tepisode_return:  -259.60542753099963\n",
      "Process:  6 \tepisode:  3800 \tepisode_return:  -257.98908947815613\n",
      "Process:  2 \tepisode:  3810 \tepisode_return:  -250.67651947051633\n",
      "Process:  7 \tepisode:  3820 \tepisode_return:  -243.46679202537314\n",
      "Process:  1 \tepisode:  3830 \tepisode_return:  -267.03034352515584\n",
      "Process:  6 \tepisode:  3840 \tepisode_return:  -282.38705615519905\n",
      "Process:  2 \tepisode:  3850 \tepisode_return:  -289.1252400393648\n",
      "Process:  7 \tepisode:  3860 \tepisode_return:  -281.4035706162903\n",
      "Process:  1 \tepisode:  3870 \tepisode_return:  -276.9517098787451\n",
      "Process:  6 \tepisode:  3880 \tepisode_return:  -268.2152679042048\n",
      "Process:  2 \tepisode:  3890 \tepisode_return:  -262.0961790249848\n",
      "Process:  7 \tepisode:  3900 \tepisode_return:  -265.1803556627787\n",
      "Process:  1 \tepisode:  3910 \tepisode_return:  -275.804877594924\n",
      "Process:  6 \tepisode:  3920 \tepisode_return:  -279.44773059713293\n",
      "Process:  2 \tepisode:  3930 \tepisode_return:  -272.9497231397941\n",
      "Process:  7 \tepisode:  3940 \tepisode_return:  -278.86052169095234\n",
      "Process:  1 \tepisode:  3950 \tepisode_return:  -286.96555419468217\n",
      "Process:  6 \tepisode:  3960 \tepisode_return:  -286.77813723856246\n",
      "Process:  2 \tepisode:  3970 \tepisode_return:  -287.230139810363\n",
      "Process:  7 \tepisode:  3980 \tepisode_return:  -283.656739151981\n",
      "Process:  1 \tepisode:  3990 \tepisode_return:  -283.8029369726245\n",
      "Process:  6 \tepisode:  4000 \tepisode_return:  -282.71520816919707\n",
      "Process:  2 \tepisode:  4010 \tepisode_return:  -276.51895945274003\n",
      "Process:  7 \tepisode:  4020 \tepisode_return:  -274.0347458332051\n",
      "Process:  1 \tepisode:  4030 \tepisode_return:  -271.2631532967506\n",
      "Process:  6 \tepisode:  4040 \tepisode_return:  -271.68012110834013\n",
      "Process:  2 \tepisode:  4050 \tepisode_return:  -283.181197614961\n",
      "Process:  7 \tepisode:  4060 \tepisode_return:  -268.7879020494074\n",
      "Process:  1 \tepisode:  4070 \tepisode_return:  -272.16867246548316\n",
      "Process:  6 \tepisode:  4080 \tepisode_return:  -265.9788812650178\n",
      "Process:  2 \tepisode:  4090 \tepisode_return:  -254.30277745928703\n",
      "Process:  7 \tepisode:  4100 \tepisode_return:  -247.7508807807321\n",
      "Process:  1 \tepisode:  4110 \tepisode_return:  -242.79053201827313\n",
      "Process:  6 \tepisode:  4120 \tepisode_return:  -240.84490867836658\n",
      "Process:  2 \tepisode:  4130 \tepisode_return:  -240.77881761701997\n",
      "Process:  7 \tepisode:  4140 \tepisode_return:  -233.81988262507\n",
      "Process:  1 \tepisode:  4150 \tepisode_return:  -231.10399298928263\n",
      "Process:  6 \tepisode:  4160 \tepisode_return:  -236.19019234343799\n",
      "Process:  2 \tepisode:  4170 \tepisode_return:  -244.00935411474669\n",
      "Process:  7 \tepisode:  4180 \tepisode_return:  -260.61988082908346\n",
      "Process:  1 \tepisode:  4190 \tepisode_return:  -260.7503719614196\n",
      "Process:  6 \tepisode:  4200 \tepisode_return:  -258.1425658687405\n",
      "Process:  5 \tepisode:  4210 \tepisode_return:  -257.60505348755567\n",
      "Process:  2 \tepisode:  4220 \tepisode_return:  -268.8433186161761\n",
      "Process:  0 \tepisode:  4230 \tepisode_return:  -265.96243452327263\n",
      "Process:  6 \tepisode:  4240 \tepisode_return:  -263.39890163847315\n",
      "Process:  5 \tepisode:  4250 \tepisode_return:  -266.4909369908107\n",
      "Process:  7 \tepisode:  4260 \tepisode_return:  -268.7445201649004\n",
      "Process:  0 \tepisode:  4270 \tepisode_return:  -267.6556850683947\n",
      "Process:  6 \tepisode:  4280 \tepisode_return:  -260.7884689161607\n",
      "Process:  5 \tepisode:  4290 \tepisode_return:  -271.0892914253617\n",
      "Process:  7 \tepisode:  4300 \tepisode_return:  -271.7273978259035\n",
      "Process:  0 \tepisode:  4310 \tepisode_return:  -268.20865933441166\n",
      "Process:  6 \tepisode:  4320 \tepisode_return:  -266.3435461096036\n",
      "Process:  5 \tepisode:  4330 \tepisode_return:  -264.19577682576886\n",
      "Process:  2 \tepisode:  4340 \tepisode_return:  -256.4438262531453\n",
      "Process:  0 \tepisode:  4350 \tepisode_return:  -263.4299254422233\n",
      "Process:  6 \tepisode:  4360 \tepisode_return:  -255.76033607885245\n",
      "Process:  5 \tepisode:  4370 \tepisode_return:  -254.4122404586808\n",
      "Process:  2 \tepisode:  4380 \tepisode_return:  -256.9755475454887\n",
      "Process:  0 \tepisode:  4390 \tepisode_return:  -251.12198468731583\n",
      "Process:  1 \tepisode:  4400 \tepisode_return:  -246.11852904238242\n",
      "Process:  5 \tepisode:  4410 \tepisode_return:  -245.71376690277083\n",
      "Process:  7 \tepisode:  4420 \tepisode_return:  -240.81711897855556\n",
      "Process:  0 \tepisode:  4430 \tepisode_return:  -247.7321749756139\n",
      "Process:  1 \tepisode:  4440 \tepisode_return:  -242.89452299809875\n",
      "Process:  5 \tepisode:  4450 \tepisode_return:  -244.88065486708484\n",
      "Process:  2 \tepisode:  4460 \tepisode_return:  -245.0805600902408\n",
      "Process:  0 \tepisode:  4470 \tepisode_return:  -240.2455750553131\n",
      "Process:  1 \tepisode:  4480 \tepisode_return:  -271.7089321001339\n",
      "Process:  5 \tepisode:  4490 \tepisode_return:  -277.57048216601623\n",
      "Process:  7 \tepisode:  4500 \tepisode_return:  -289.07694057086513\n",
      "Process:  0 \tepisode:  4510 \tepisode_return:  -281.5719354870081\n",
      "Process:  1 \tepisode:  4520 \tepisode_return:  -281.5142640463632\n",
      "Process:  5 \tepisode:  4530 \tepisode_return:  -275.7561360765232\n",
      "Process:  7 \tepisode:  4540 \tepisode_return:  -261.9622352919994\n",
      "Process:  0 \tepisode:  4550 \tepisode_return:  -253.47956766314564\n",
      "Process:  6 \tepisode:  4560 \tepisode_return:  -249.4158319903678\n",
      "Process:  5 \tepisode:  4570 \tepisode_return:  -253.38537119600187\n",
      "Process:  7 \tepisode:  4580 \tepisode_return:  -245.2761405026813\n",
      "Process:  0 \tepisode:  4590 \tepisode_return:  -243.7120298056929\n",
      "Process:  6 \tepisode:  4600 \tepisode_return:  -236.93229091498205\n",
      "Process:  3 \tepisode:  4610 \tepisode_return:  -234.44761363014922\n",
      "Process:  7 \tepisode:  4620 \tepisode_return:  -238.33940616807786\n",
      "Process:  0 \tepisode:  4630 \tepisode_return:  -241.42157016226238\n",
      "Process:  6 \tepisode:  4640 \tepisode_return:  -252.93886220386162\n",
      "Process:  3 \tepisode:  4650 \tepisode_return:  -251.17676314842328\n",
      "Process:  7 \tepisode:  4660 \tepisode_return:  -258.3678635905791\n",
      "Process:  0 \tepisode:  4670 \tepisode_return:  -252.49322291531206\n",
      "Process:  6 \tepisode:  4680 \tepisode_return:  -248.1730617437984\n",
      "Process:  3 \tepisode:  4690 \tepisode_return:  -240.54851786468907\n",
      "Process:  2 \tepisode:  4700 \tepisode_return:  -242.8235031661267\n",
      "Process:  0 \tepisode:  4710 \tepisode_return:  -243.88242993585078\n",
      "Process:  1 \tepisode:  4720 \tepisode_return:  -234.2097610018921\n",
      "Process:  3 \tepisode:  4730 \tepisode_return:  -242.1800928815942\n",
      "Process:  7 \tepisode:  4740 \tepisode_return:  -244.3943607059505\n",
      "Process:  0 \tepisode:  4750 \tepisode_return:  -244.71260219967357\n",
      "Process:  1 \tepisode:  4760 \tepisode_return:  -241.30673727406403\n",
      "Process:  3 \tepisode:  4770 \tepisode_return:  -244.55036429379905\n",
      "Process:  7 \tepisode:  4780 \tepisode_return:  -240.6617675786405\n",
      "Process:  4 \tepisode:  4790 \tepisode_return:  -241.3015686493545\n",
      "Process:  1 \tepisode:  4800 \tepisode_return:  -249.71636526040317\n",
      "Process:  5 \tepisode:  4810 \tepisode_return:  -248.6716659390097\n",
      "Process:  7 \tepisode:  4820 \tepisode_return:  -255.9378254219742\n",
      "Process:  4 \tepisode:  4830 \tepisode_return:  -250.17000236745133\n",
      "Process:  6 \tepisode:  4840 \tepisode_return:  -246.62099764136428\n",
      "Process:  3 \tepisode:  4850 \tepisode_return:  -250.3961223307448\n",
      "Process:  7 \tepisode:  4860 \tepisode_return:  -251.5348589691951\n",
      "Process:  4 \tepisode:  4870 \tepisode_return:  -247.00030734926457\n",
      "Process:  6 \tepisode:  4880 \tepisode_return:  -246.70807828847703\n",
      "Process:  3 \tepisode:  4890 \tepisode_return:  -242.5505103555454\n",
      "Process:  7 \tepisode:  4900 \tepisode_return:  -242.8815880457703\n",
      "Process:  4 \tepisode:  4910 \tepisode_return:  -239.29753746680072\n",
      "Process:  6 \tepisode:  4920 \tepisode_return:  -240.75887900643633\n",
      "Process:  3 \tepisode:  4930 \tepisode_return:  -248.73931722862054\n",
      "Process:  2 \tepisode:  4940 \tepisode_return:  -244.40694138989676\n",
      "Process:  1 \tepisode:  4950 \tepisode_return:  -241.14029656030712\n",
      "Process:  6 \tepisode:  4960 \tepisode_return:  -242.86554261756194\n",
      "Process:  3 \tepisode:  4970 \tepisode_return:  -242.1275312735551\n",
      "Process:  7 \tepisode:  4980 \tepisode_return:  -242.50006277702593\n",
      "Process:  1 \tepisode:  4990 \tepisode_return:  -245.00246640066402\n"
     ]
    }
   ],
   "source": [
    "! python ./A2C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAH0CAIAAABEtEjdAAAIAklEQVR4nO3dS04bQRRAURyxo7AMWC1eRrKmzsASkKj9A8dN3TpnhCwGNbp+Kr2C3bIsDwC0/Nj6AADcnrgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0CQuAMEiTtAkLgDBIk7QJC4AwSJO0DQ49YHgO/i98vL6uc/X1/vfBL4ut2yLFufATZ2LOsfSTxjEXemdknW3+g7A3HnzryuKvsnfh82JO5wBX1nFOLOpGSaNnFnRl8pu28FhiDuAEHiDlczvPP9iTtAkLgzHXM3MxB3gCBxBwgSd4AgcQcIEneAIHEHCBJ3uJq//cv3J+7MxZI7kxB3gCBxBwgSd4AgcQcIEneAIHEHCBJ3uI4ld4Yg7kzEkjvzEHeAIHEHCBJ3gCBxBwgSd4AgcQcIEne4giV3RiHuzMKSO1MRd4AgcQcIEneAIHEHCBJ3gCBxBwgSd7iUJXcGIu5MwZI7sxF3gCBxBwgSd4AgcQcIEneAIHEHCBJ3gCBxp+8mS+5eMDEWcQcIEneAIHEHCBJ3gCBxBwgSd4AgcQcIEnc4z5I7wxF34vybDuYk7gBB4g4QJO4AQeIOECTuAEHiDhAk7nCGJXdGJO6UWXJnWuIOECTuAEHiDhAk7gBB4g4QJO4AQeIOp1hyZ1DiTpYld2Ym7gBB4g4QJO4AQeIOECTuAEHiDhAk7nCUJXfGJe40WXJncuIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTusM4LJoYm7gR5wQTiDhAk7gBB4g4QJO4AQeIOECTuAEHiDissuTM6cafGkjs8iDtAkrgDBIk7QJC4AwSJO0CQuAMEiTv8y5I7AeJOiiV3OBB3gCBxBwgSd4AgcQcIEneAIHEHCBJ3+IsldxrEnQ5L7vBG3AGCxJ1ZPO33T/v91qeAO3nc+gDw331s+uHnX8/P2x0H7sHkTtzqtG6EJ0/cKTsRcX2nTdwBgsSdrLOzueGdMHGHd14wkSHuZJ1dibEzQ5i4AwSJO2UnZnNjO23iTsfqjflqxFc/dOFOiReq9B1S7m0qU9kty7L1GeCWPve3IY3txLiWoeYTmVZ2esSdILEG1zJkXXI/42uAKnEn7ljiZZ02cQcIcucOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBB4g4QJO4AQeIOECTuAEHiDhAk7gBBfwCc9ITZ6RlsIgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x500 at 0x7F516C854990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 显示训练好的模型\n",
    "# 导入相应的模块\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# ActorCritic网络定义\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, act_limit, device):\n",
    "        super().__init__()\n",
    "        self.act_limit = torch.as_tensor(act_limit, dtype=torch.float32, device=device)\n",
    "        self.value_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.value_layer2 = nn.Linear(256, 1)\n",
    "        \n",
    "        self.policy_layer1 = nn.Linear(obs_dim, 256)\n",
    "        self.mu_layer = nn.Linear(256, act_dim)\n",
    "        self.sigma_layer = nn.Linear(256, act_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        value = F.relu6(self.value_layer1(obs))\n",
    "        value = self.value_layer2(value)\n",
    "        policy = F.relu6(self.policy_layer1(obs))\n",
    "        mu = torch.tanh(self.mu_layer(policy)) * self.act_limit\n",
    "        sigma = F.softplus(self.sigma_layer(policy))\n",
    "        return value, mu, sigma\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        _, mu, sigma = self.forward(obs)\n",
    "        pi = Normal(mu, sigma)\n",
    "        return pi.sample().cpu().numpy()\n",
    "\n",
    "    def loss_func(self, states, actions, v_t, beta):\n",
    "        values, mu, sigma = self.forward(states)\n",
    "        td = v_t - values\n",
    "        value_loss = torch.squeeze(td ** 2)\n",
    "\n",
    "        pi = Normal(mu, sigma)\n",
    "        log_prob = pi.log_prob(actions).sum(axis=-1)\n",
    "        entropy = pi.entropy().sum(axis=-1)\n",
    "        policy_loss = -(log_prob * torch.squeeze(td.detach()) + beta * entropy)\n",
    "        return (value_loss + policy_loss).mean()\n",
    "\n",
    "# 这段代码用来在jupyter中代替env.render()\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "\n",
    "def render(env):   \n",
    "    display.display(Image.fromarray(env.render(mode='rgb_array')))\n",
    "    display.clear_output(wait=True)  \n",
    "\n",
    "a2c_model = torch.load('a2c_Pendulum-v0_model.pth').to('cuda:1')\n",
    "env = gym.make('Pendulum-v0')\n",
    "state = env.reset()\n",
    "while True:\n",
    "    render(env)\n",
    "    _, mu, sigma = a2c_model.forward(torch.as_tensor(state, dtype=torch.float32, device='cuda:1'))\n",
    "    action = Normal(mu, sigma).sample().cpu().numpy()\n",
    "    state, reward, done, _ = env.step(action)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
