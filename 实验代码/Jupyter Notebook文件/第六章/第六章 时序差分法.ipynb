{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center>第六章 时序差分法<center>\n",
    "    \n",
    "&emsp;&emsp;在未知环境模型下，虽然蒙特卡罗法能够求解MDP问题，但需要等到一个情节结束后才能更新值函数。而在实际场景中，有些任务可能没有终止状态或者难以到达终止状态。针对该问题，本章介绍一种新的强化学习方法——时序差分法（Temporal-Difference，TD）。TD方法将DP的自举性和MC的采样性相结合，学习时间间隔产生的差分数据，并通过迭代更新来求解未知环境模型的MDP问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 时序差分预测\n",
    "&emsp;&emsp;利用MC解决预测问题，状态值函数增量式更新递归式为：\n",
    "\\begin{equation}\n",
    "V(S_t) \\gets V(S_t)-\\alpha[G_t-V(S_t)] \\label{6.1}\\tag{6.1}\n",
    "\\end{equation} \n",
    "&emsp;&emsp;在时序差分预测中，每前进1步或n步，就可以直接计算状态值函数。本章仅讨论单步情况下的时序差分法算法，即TD(0)（单步TD）。n-步TD（n-step TD）算法将在第7章详细阐述。<br>\n",
    "&emsp;&emsp;TD(0)算法是n-步TD算法的特例，适用于小批量状态的更新方法，其中(0)表示向前行动1步。严格来说，这里的0是指资格迹$\\lambda$=0的情况，这一点将在后续章节中详细说明。TD(0)算法的核心思想是向前行动1步后，使用得到的立即奖赏$R_{t+1}$和下一状态的状态值函数的估计值$ \\small V(S_{t+1})$来进行更新，这也是TD(0)被称为单步TD方法的原因。具体更新递归式为:\n",
    "\\begin{equation}\n",
    "V(S_t) \\gets V(S_t)-\\alpha[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)] \\label{6.2}\\tag{6.2}\n",
    "\\end{equation}\n",
    "&emsp;&emsp;对式（6.2）说明如下：<br>\n",
    "&emsp;&emsp;（1）$R_{t+1}+\\gamma V(S_{t+1})$表示根据样本得到的时序差分目标值，$V(S_{t+1})$表示t+1时刻状态值函数的估计值；<br>\n",
    "&emsp;&emsp;（2）右侧的$V(S_{t})$表示$t$时刻状态值函数$V_{\\pi }(s)$的估计值，通过迭代收敛来逼近真实状态值函数；<br>\n",
    "&emsp;&emsp;（3）$R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$为时序差分误差（TD error)，记为$\\delta _t$，表示$t$时刻的估计误差。由于TD误差取决于获得的奖赏和下有一个状态，所以$\\delta _t$在$t$+1时刻才能获得。<br>\n",
    "&emsp;&emsp;对于DP、MC、TD状态值函数更新方法的异同点，从以下几个方面进行阐述。<br>\n",
    "&emsp;&emsp;（1）MC、TD与DP的更新图比较<br>\n",
    "&emsp;&emsp;MC和TD算法对值函数的更新方法称为采样更新或样本更新。所谓的采样更新是通过采样得到一个即时后继状态（或状态-动作对），并使用即时后继状态的价值和迁移得到的奖励来计算目标值，然后更新值函数估计值。<br>\n",
    "&emsp;&emsp;MC和TD算法的采样更新与DP法的期望更新思想不同，采样更新利用下一时刻单一的样本转换，而期望更新利用所有可能的下一状态的分布。<br>\n",
    "<center>\n",
    "<table align=\"center\" width=\"100%\">\n",
    "    <tr>\n",
    "        <th><center><img src='./image/图6.1.jpg' width='230px' height='250px'></center></th>\n",
    "        <th><center><img src='./image/图6.2.jpg' width='400px' height='250px'></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><font size=2.5>图6.1&ensp;TD(0)更新图</font></center></td>\n",
    "        <td><center><font size=2.5>图6.2&ensp;DP更新图</font></center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "&emsp;&emsp;（2）误差分析<br>\n",
    "&emsp;&emsp;对DP、MC、TD的误差来源进行分析。根据值函数计算公式与贝尔曼方程可知：<br>\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s)&=\\mathbb{E}_\\pi(G_t|S_t=s)\\qquad \\qquad \\qquad \\qquad \\ MC法\\\\\n",
    "    &=\\mathbb{E}_\\pi(R_{t+1}+\\gamma G_{t+1}|S_t=s)\\\\\n",
    "    &=\\mathbb{E}_\\pi(R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s)\\qquad \\quad DP法\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n",
    "&emsp;&emsp;DP算法具有自举性，其中的$v_\\pi (S_{t+1})$是状态$S_{t+1}$的真实值函数，在计算过程中是不可知的，通常使用估计值$V_\\pi (S_{t+1})$来代替真实值函数。<br>\n",
    "&emsp;&emsp;在MC算法中，由于回报$G_t$的期望是不可知的，所以通过采样回报的期望来代替真实回报的期望。<br>\n",
    "&emsp;&emsp;在TD算法中，$R_{t+1}+\\gamma v_\\pi (S_{t+1})$是由采样获得的，另外由于自举特性，$v_\\pi (S_{t+1})$也是未知的。<br>\n",
    "&emsp;&emsp;（3）无偏估计和有偏估计<br>\n",
    "&emsp;&emsp;无偏估计是指估计量$\\hat{\\theta}$的数学期望等于被估计参数$\\theta$的真实值，则称此估计量$\\hat{\\theta}$为被估计参数$\\theta$的无偏$\\mathbb{E}(\\hat{\\theta})=\\theta$估计，即样本均值的期望等于总体均值，所以样本均值为无偏估计。<br>\n",
    "&emsp;&emsp;有偏估计是指由样本值求得的估计值参数不等于待估计参数的真实值，$\\mathbb{E}(\\hat{\\theta})\\neq\\theta$。<br>\n",
    "&emsp;&emsp;MC算法是高方差的无偏估计。因为它是以真实回报作为目标值的，所以MC算法是$v_\\pi$的无偏估计。但由于MC算法的目标值依赖的是一系列随机的$s,a,r$序列，所以它的方差比较高。<br>\n",
    "&emsp;&emsp;TD算法是低方差的有偏估计。因为TD算法的目标值使用了自举思想，即利用了下一状态（或状态-动作对）的值函数估计值，所以TD算法是对$v_\\pi$的有偏估计。但由于TD算法的目标值只依赖于一组随机的$s,a,r$序列，所以它的方差低于MC的方差。<br>\n",
    "&emsp;&emsp;（4）MC误差与TD误差<br>\n",
    "&emsp;&emsp;在一个情节中，当状态值函数估计值$V$没有发生变化时，MC误差可写成TD误差之和：<br>\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "G_t-V(S_t)&=R_{t+1}+\\gamma G_{t+1}-V(S_t)+\\gamma V(S_{t+1})-\\gamma V(S_{t+1})\\\\\n",
    "    &=\\bigl(R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\bigl)+\\gamma G_{t+1}-\\gamma V(S_{t+1})\\\\\n",
    "    &=\\delta_t+\\gamma \\bigl(G_{t+1}-V(S_{t+1})\\bigl)\\\\\n",
    "    &=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\bigl(G_{t+2}-V(S_{t+2})\\bigl)\\\\\n",
    "    &=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t+1}\\delta_{T-t+1}+\\gamma^{T-t}\\bigl(G_t-V(S_T)\\bigl)\\\\\n",
    "    &=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t+1}\\delta_{T-t+1}+\\gamma^{T-t}(0-0) \\qquad 终止状态没有奖励，G_t=0;V(S_T)=0\\\\\n",
    "    &=\\sum\\limits_{k=1}^{T-t+1}\\gamma^k \\delta_{t+k}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;算法6.1 给出了用于估计$v_\\pi$的表格型TD(0)算法。<br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" />\n",
    "&emsp;&emsp;<font size=3.5><b>算法6.1</b> 用于估计$v_\\pi$的表格型TD(0)算法</font>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输入：</b></font>\n",
    "<font size=3.5>待评估的策略$\\pi$，折扣因子$\\gamma$，学习率$\\{\\alpha_k\\}^\\infty_{k=0}\\in[0,1]$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>初始化：</b></font>\n",
    "<font size=3.5>1.&emsp;对任意$s\\in\\mathcal{S}^+$，初始化状态值函数，如$V(s)\\gets\\mathbb{R}；V(s^T)\\gets 0$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>2.&emsp;<b>repeat</b> 对每个情节$k$=0，1，2，$\\cdots$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>3.&emsp;</font>&emsp;&emsp;<font size=3.5>初始化状态$S$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>4.&emsp;</font>&emsp;&emsp;<font size=3.5><b>while</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>5.&emsp;</font>&emsp;&emsp;&emsp;&emsp;<font size=3.5>根据策略$\\pi$，选择动作$A$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>6.&emsp;</font>&emsp;&emsp;&emsp;&emsp;<font size=3.5>在状态<i>$S$</i>下执行动作$A$，到达下一状态$S'$，并得到奖赏$R$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>7.&emsp;</font>&emsp;&emsp;&emsp;&emsp;<font size=3.5>$V(S)\\gets V(S)+\\alpha_k[R+\\gamma V(S')-V(S)]$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>8.&emsp;</font>&emsp;&emsp;&emsp;&emsp;<font size=3.5>$S\\gets S'$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>9.&emsp;</font>&emsp;&emsp;<font size=3.5><b>until</b> &emsp;$S=S^T$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输出：</b></font>\n",
    "&emsp;<font size=3.5>$v_\\pi=V$</font><br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" /><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 时序差分控制\n",
    "&emsp;&emsp;TD控制算法和MC控制算法一样，都遵循GPI，且评估的目标都是状态-动作对$(s,a)$的最优动作值函数$q_*(s,a)$。为了平衡探索与利用，TD控制算法分为基于同策略的Sarsa算法和基于异策略的Q-Learning算法。Sarsa算法与Q-learning算法都属于TD(0)算法，它们的收敛性同样遵循MC增量式中的随机近似条件。<br>\n",
    "### 6.2.1 Sarsa算法\n",
    "&emsp;&emsp;Sarsa算法的动作值函数更新迭代式为：<br>\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)+\\alpha [R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)] \\label{6.3}\\tag{6.3}\n",
    "\\end{equation}\n",
    "其中，目标值为$R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})$，TD误差为$\\delta_t=R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)$。</font><br>\n",
    "&emsp;&emsp;从上式可以看出，Sarsa算法每次更新都需要获取五元组$(S_t,A_t,R_t,S_{t+1},A_{t+1})$，也可用$(S,A,R,S',A')$表示，这也是将该算法称为Sarsa的原因。每当从非终止状态$S_t$进行一次转移后，就进行一次更新，但需要注意的是，动作$A$是情节中实际发生的动作，在更新$(S,A)$的动作值函数$Q(S,A)$时，Agent并不实际执行状态$S'$下的动作$A'$，而是将$A'$留到下一个循环中执行。另外在更新终止状态$S_T$的前一个状态$S_{T-1}$时，需要用到$(S_T,A_t)$，但是在终止状态采取任何动作，都原地不动，不会获得奖赏。所以为了保证Sarsa算法能够完整地更新整个情节，当$S_{t+1}$为终止状态时，$Q(S_{t+1},A_{t+1})$定义为0。<br>\n",
    "&emsp;&emsp;由于采用了贪心策略，Sarsa算法在各时间步都隐式地进行了策略改进。像这种在每个样本更新后都进行策略改进的策略迭代算法，也称为完全乐观策略迭代算法。<br>\n",
    "&emsp;&emsp;算法6.2给出了用于估算最有策略$\\pi_*$的Sarsa算法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" />\n",
    "&emsp;&emsp;<font size=3.5><b>算法6.2</b> 估算最优策略$\\pi_*$的Sarsa算法</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 入：</b></font>\n",
    "<font size=3.5>折扣因子$\\gamma$，学习率$\\{ \\alpha_k \\}_{k=0}^\\infty \\in[0,1]$，探索因子$\\{\\epsilon_k\\}^\\infty_{k=0}\\in[0,1]$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>初始化：</b></font>\n",
    "<font size=3.5>1.&emsp;对任意$s\\in \\mathcal{S}^+，a\\in \\mathcal{A}(s)$，初始化动作值函数，如$Q(s,a)\\gets \\mathbb{R}，Q(s^T,a)=0$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>2.&emsp;对$s\\in \\mathcal{S}^+，\\pi(a|s)\\gets$基于动作值函数$Q(s,a)$的$\\epsilon_0-$贪心策略</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>3.&emsp;<b>repeat</b> 对每个情节$k$=1,2,3$\\cdots$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>4.&emsp;&emsp;&emsp;初始化状态$S$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>5.&emsp;&emsp;&emsp;根据策略$\\pi(a|s)$,在状态$S$下选择动作$A$:</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$A\\gets \\begin{cases}\n",
    "以概率1-\\epsilon_k，选择动作a\\in \\mathop{\\arg \\max} \\limits_{a'}Q(S,a')\\\\\n",
    "以概率\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|},在\\mathcal{A}(S)中均匀随机地选择动作\n",
    "\\end{cases} $<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>6.&emsp;<b>while</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>7.&emsp;&emsp;&emsp;执行动作$A$，到达状态$S'$，并得到奖赏$R$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>8.&emsp;&emsp;&emsp;根据策略$\\pi(a|s)$，在状态$S'$下选择动作$A'$：</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$A'\\gets \\begin{cases}\n",
    "以概率1-\\epsilon_k，选择动作a\\in \\mathop{\\arg \\max} \\limits_{a'}Q(S',a')\\\\\n",
    "以概率\\dfrac{\\epsilon_k}{|\\mathcal{A}(S')|},在\\mathcal{A}(S')中均匀随机地选择动作\n",
    "\\end{cases} $<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>9.&emsp;&emsp;&emsp;$Q(S,A)\\gets Q(S,A)+\\alpha_k[R+\\gamma Q(S',A')-Q(S,A)]$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>10.&emsp;&emsp; $A^*\\gets \\mathop{\\arg \\max} \\limits_{a}Q(S,a)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>11.&emsp;&emsp; <b>for</b> $A\\in \\mathcal{A}(S)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>12.&emsp;&emsp;&emsp;&emsp;&emsp; $\\pi(a|S)\\gets \\begin{cases}\n",
    "1-\\epsilon_k+\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad a=A^*\\\\\n",
    "\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad \\ \\ \\quad \\qquad a\\ne A^*\n",
    "\\end{cases} $</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>13.&emsp;&emsp; <b>end for</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>14.&emsp;&emsp; $S\\gets S'，A\\gets A'$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>15.&emsp;<b>until</b> $S=S^T$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 出：</b></font>\n",
    "<font size=3.5>$q_*=Q,\\pi_*=\\pi$</font><br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" /><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;为了能收敛到最优$Q$值函数$Q^*$和最优策略$\\pi^*$，Sarsa算法要求以一定的概率进行探索的同时，探索策略必须逐渐变得贪心。例如在使用$\\epsilon-$贪心策略时，其探索概率$\\epsilon$逐渐衰减到0（算法6.2中用$\\{\\epsilon_k\\}^\\infty_{k=0}$表示衰减过程）。如果使用Boltzmann探索，其探索温度参数$\\tau_k$逐渐衰减到0。这样当Sarsa算法使用具有贪心性质的策略时，也会逐渐变得贪心。<br>\n",
    "&emsp;&emsp;<font><b>例6.1</b></font> 使用Sarsa算法解决例4.1确定环境扫地机器人问题。将动作值函数设置为24$\\times$4的二维数组，且初值都为0。初始学习率参数$\\alpha_0$为0.05，$\\gamma$为0.8，使用$\\epsilon-$贪心策略($\\epsilon_0=0.5$)。<br>\n",
    "&emsp;&emsp;当运行到第3000个情节时，到达状态$S$=21，根据$\\epsilon-$贪心策略，下一个动作向右（$A=Right$，即第3个动作），此时$Q$[21][3]=1.086。执行动作$A$得到奖赏$R$=0,到达的下一个状态$S'=22$，根据$\\epsilon-$贪心策略，下一个状态要采取的动作为向右（$A'=Right$，即第3个动作），此时$Q$[22][3]=1.483，$\\alpha_{3000}$=0.0425。这样更新动作值函数$Q$[21][3]的计算过程为：<br>\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "Q[21][3]&=Q[21][3]+\\alpha_{3000}(R+\\gamma *Q[22][3]-Q[21][3]) \\\\\n",
    "    &=1.086+0.0425*(0+0.8*1.483-1.086)\\\\\n",
    "    &=1.091\\\\\n",
    "    &\\approx 1.09\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "<br>\n",
    "&emsp;&emsp;表6.1给出了几个具有代表性的使用Sarsa算法解决确定环境扫地机器人问题的Q值迭代过程。从表6.1可以看出，当迭代到第20000个情节时。动作值函数已经收敛。在表中$Up,Down,Left,Right$等4个动作值函数或策略之间用“；”号分开，“\\*.\\*\\*”表示此动作值不存在。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.1 基于Sarsa算法的确定环境扫地机器人问题的Q值迭代过程</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"80px\"><p style=\"text-align:center\">&emsp;&emsp;</p></th>\n",
    "        <th><p style=\"text-align:center\">$S_5$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{10}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{18}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{20}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{24}$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.62;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.63;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.49;1.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.59;0.69;*.**;0.55</p></td>\n",
    "        <td><p style=\"text-align:center\">1.76;1.38;1.51;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.74;*.**;0.93</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.67;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.10;0.79;0.00;0.10</p></td>\n",
    "        <td><p style=\"text-align:center\">0.10;0.79;0.00;0.10</p></td>\n",
    "        <td><p style=\"text-align:center\">0.08;0.08;0.08;0.77</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.16;0.00;0.84</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.84;0.16;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.50;1.00;*.**;0.44</p></td>\n",
    "        <td><p style=\"text-align:center\">0.62;0.73;*.**;0.58</p></td>\n",
    "        <td><p style=\"text-align:center\">1.76;1.59;1.62;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.89;*.**;1.09</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.76;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.88;0.00;0.06</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.88;0.00;0.06</p></td>\n",
    "        <td><p style=\"text-align:center\">0.05;0.05;0.05;0.86</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.09;0.00;0.91</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.91;0.09;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.50;1.00;*.**;0.45</p></td>\n",
    "        <td><p style=\"text-align:center\">0.62;0.72;*.**;0.58</p></td>\n",
    "        <td><p style=\"text-align:center\">1.78;1.61;1.69;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.02;*.**;1.21</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.85;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.50;1.00;*.**;0.45</p></td>\n",
    "        <td><p style=\"text-align:center\">0.62;0.72;*.**;0.58</p></td>\n",
    "        <td><p style=\"text-align:center\">1.78;1.61;1.69;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.02;*.**;1.21</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.85;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{*}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarsa算法解决确定环境扫地机器人代码\n",
    "from book_gridword import GridWorldEnv\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = GridWorldEnv()\n",
    "#有效动作空间\n",
    "def vilid_action_space(s):\n",
    "    action_sacpe = []\n",
    "    if s % 5 != 0:#左\n",
    "        action_sacpe.append(0)\n",
    "    if s % 5 != 4:#右\n",
    "        action_sacpe.append(1)\n",
    "    if s <= 19:#上\n",
    "        action_sacpe.append(2)\n",
    "    if s >= 5:#下\n",
    "        action_sacpe.append(3)\n",
    "    return action_sacpe\n",
    "def policy_epsilon_greedy(s, Q, epsilon):#ε贪心策略\n",
    "    Q_s = Q[s]\n",
    "    action = vilid_action_space(s)\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(action)\n",
    "    else:\n",
    "        index_a = np.argmax([Q_s[i] for i in action])\n",
    "        a = action[index_a]\n",
    "    return a\n",
    "def print_dd(s, a, next_s, next_a, print_len, episode_i, Q,e_k,a_k):\n",
    "    for i in range(1):  \n",
    "        if episode_i == int(print_len * (0.1 * i + 1)):\n",
    "            if s == 21 and a == 1 and next_s == 22 and next_a == 1:\n",
    "                print(\"*********************************单步的计算过程**************************************\")\n",
    "                print(\"alpha:\"+str(a_k))\n",
    "                print(\"epsilon:\"+str(e_k))\n",
    "                print(\"state:\" + str(int(print_len * (0.1 * i + 1))))\n",
    "                print(Q[s][a])\n",
    "                print(Q[next_s][a])\n",
    "                print(\"update:\"+str(Q[s, a] + a_k * (0.8 * Q[next_s, next_a] - Q[s, a])))\n",
    "                print(\"************************************************************************************\")\n",
    "\n",
    "def trans(Q_S):#因为环境中动作顺序是左右上下，文章建模的动作顺序是上下左右，所以转换为文章中的顺序（上下左右）进行输出，并保存两位小数\n",
    "    new_Q = []\n",
    "    new_Q.append(round(Q_S[2],2))\n",
    "    new_Q.append(round(Q_S[3],2))\n",
    "    new_Q.append(round(Q_S[0],2))\n",
    "    new_Q.append(round(Q_S[1],2))\n",
    "    return new_Q\n",
    "def print_ff(list_q, Q, episode_i,epsilon_k):\n",
    "    list_s = [5,10,18,20,24]\n",
    "    for em in list_q:\n",
    "        if em == episode_i:\n",
    "            print(\"*******************************情节数:%s*******************************\"%(str(em)))\n",
    "\n",
    "            for state in list_s:\n",
    "                print(\"Q(%d,*) \"%(state) + str(trans(Q[state])))\n",
    "                action = vilid_action_space(state)\n",
    "                len_a = len(action)\n",
    "                e_p = epsilon_k / float(len_a)\n",
    "                prob = []\n",
    "                index_a = np.argmax([Q[state][i] for i in action])\n",
    "                for i in range(4):#计算epsilon\n",
    "                    if i not in action:\n",
    "                        prob.append(0.0)\n",
    "                    else:\n",
    "                        if i == action[index_a]:\n",
    "                            prob.append(1 - epsilon_k + e_p)\n",
    "                        else:\n",
    "                            prob.append(e_p)\n",
    "                print('概率值:' + str(trans(prob)))\n",
    "def Attenuation(epsilon,alpha,episode_sum,episode):#epsilon和alpha衰减函数\n",
    "    epsilon = (float(episode_sum) - float(episode)) / float(episode_sum) * epsilon\n",
    "    alpha = (float(episode_sum) - float(episode)) / float(episode_sum) * alpha\n",
    "    return epsilon, alpha\n",
    "def Sarsa(env, episode_num, alpha, gamma, epsilon):\n",
    "    Q = np.zeros((env.n_width * env.n_height, env.action_space.n))\n",
    "    list_q = [0,1,7500,12500,19999]\n",
    "    for episode_i in range(episode_num):\n",
    "        env.reset()\n",
    "        #print('...........................')\n",
    "        S = env.state\n",
    "        epsilon_k, alpha_k = Attenuation(epsilon,alpha,episode_num,episode_i)\n",
    "        A = policy_epsilon_greedy(S, Q, epsilon_k)\n",
    "        done = False\n",
    "        print_ff(list_q,Q,episode_i,epsilon_k)\n",
    "        while not done:\n",
    "            next_S, R, done, _ = env.step(A)\n",
    "            next_A = policy_epsilon_greedy(next_S, Q, epsilon_k)\n",
    "            print_dd(S,A,next_S,next_A,3000,episode_i,Q,epsilon_k,alpha_k)\n",
    "            Q[S, A] = Q[S, A] + alpha_k * (R + gamma * Q[next_S, next_A] - Q[S, A])\n",
    "            S = next_S\n",
    "            A = next_A\n",
    "            \n",
    "    return Q\n",
    "Q = Sarsa(env, 20000, 0.05, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font><b>例6.2</b></font> 风险投资问题。在进行投资时，预期收益是一个非常重要的参考指标。现在越来越多的人接受概率的观点，但是收益为正的投资一定时理性的吗？假设一种风险投资，当前资产（本金）为$S$，下一个单位时间有0.5的概率变为原有资产的0.9倍，0.5的概率变为原有资产的1.11倍。经过一个时间单位后预期收益率为$\\tfrac{0.5*(1.11S-S)+0.5*(0.9S-S)}{S}*100\\%$=0.5$\\%$。但是在实际情况中，进行2$n$个时间步后连续投资预期收益为${ S*0.9^n*1.11^n=S*0.999^n}$，也就是说，当$n$趋于无穷的情况下，该投资会血本无归。为了使投资更加理性，利用Sarsa算法给出关于在不同本金情况下的投资方案。<br>\n",
    "&emsp;&emsp;该问题的MDP模型为：<br>\n",
    "&emsp;&emsp;（1）状态空间：该问题的状态为当前资产的数目，因此状态空间为连续的实数空间。这样在不影响问题完整性的情况下采取两个措施来减小状态空间。第一个措施是将资产进行离散化，即对资产进行四舍五入。第二个措施是设定最高的资产，即将最大资产设置为本金的5倍。假设本金为10（定义本金为$S_{origin}$），最大资产低于50。状态空间为$\\mathcal{S}=\\{0,1,2,\\cdots \\cdots ,49\\}$。<br>\n",
    "&emsp;&emsp;（2）动作空间：在该问题中一共有两个动作，分别有投资和不投资。用0来表示不投资，用1表示投资。这个问题的动作空间为$\\mathcal{A}(s)=\\{0,1\\}$。<br>\n",
    "&emsp;&emsp;（3）立即奖赏：在这个问题中，如果投资，则立即奖赏为零。如果进行投资，则分4种情况：如果资产增长且投资结果大于本金，则给予+1的奖赏；如果资产减少，且投资结果小于等于本金，给予-1的奖赏；如果资产增加，且投资结果小于本金，基于+0.5的奖赏；如果资产减少，且投资结果大于本金，给予-0.5的奖赏。奖赏函数如下：<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$r(s,a,s')=\\begin{cases}\n",
    "0   \\qquad \\qquad \\quad  \\ \\ if\\ a=0\\\\\n",
    "+0.5\\qquad \\qquad if\\ a=1\\ and\\ s'>s\\ and\\ s'<s_{origin}\\\\\n",
    "+1.0\\qquad \\qquad if\\ a=1\\ and\\ s'>s\\ and\\ s'>s_{origin}\\\\\n",
    "-0.5\\qquad \\qquad if\\ a=1\\ and\\ s'<s\\ and\\ s'>s_{origin}\\\\\n",
    "-1.0\\qquad \\qquad if\\ a=1\\ and\\ s'<s\\ and\\ s'<s_{origin}                                                      \n",
    "\\end{cases}$<br>\n",
    "&emsp;&emsp;使用Sarsa算法解决风险投资问题，首先设置本金为$s_{origin}$=10，将动作值函数设置为50$\\times$2的二维数组，且初值都为0。初始学习率参数$\\alpha_0$为0.05，$\\gamma$为0.8，使用$\\epsilon-$贪心策略（$\\epsilon_0$=0.5）。<br>\n",
    "&emsp;&emsp;实验进行了20000个情节的学习，每个情节都固定1000个学习步的投资。当运行到10000个情节时，到达状态$S$=5（实际资产为5.3906），根据$\\epsilon-$贪心策略，下一个动作为投资，此时$Q$[15][1]=-0.30098。执行动作$A$，投资获得成功，资产增长。但是增长后的资产比初始资产值小，所以得到奖励为$R$=0.5，到达的下一个状态为$S'$=6（实际资产为5.929），根据$\\epsilon-$贪心策略，下一个状态要采取的动作为投资（即第1个动作），此时$Q$[6][1]=-0.5397，$\\alpha_{10000}$=0.025。更新动作值函数$Q$[5][1]的计算过程为：<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;\n",
    "$\n",
    "\\begin{aligned}\n",
    "Q[5][1]&=Q[5][1]+\\alpha_{10000}(R+\\gamma *Q[6][1]-Q[5][1]) \\\\\n",
    "    &=(-0.30098)+0.025*(0.5+0.8*(-0.5397)-(-0.30098))\\\\\n",
    "    &=-0.2917495\\\\\n",
    "    &\\approx -0.29\n",
    "\\end{aligned}$\n",
    "<br>\n",
    "&emsp;&emsp;表6.2给出了几个具有代表性的使用Sarsa算法解决风险投资问题的Q值迭代过程。从表6.2中可以看出当迭代到第20000个情节时，动作值函数已经收敛。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" align=“center” width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.2 基于Sarsa算法的风险投资问题的Q值迭代过程</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"80px\"><p style=\"text-align:center\">&emsp;&emsp;</p></th>\n",
    "        <th><p style=\"text-align:center\">$S_0$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_1$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_3$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_5$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{10}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{25}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{49}$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.03</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.03;-0.06</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_1$</p></td>\n",
    "      <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.25;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.25;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.25</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.17;-0.31</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.13;-0.29</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.17;-0.49</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.26;-0.55</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.05;-0.20</p></td>\n",
    "        <td><p style=\"text-align:center\">0.82;1.08</p></td>\n",
    "        <td><p style=\"text-align:center\">0.36;0.64</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.84;0.16</p></td>\n",
    "        <td><p style=\"text-align:center\">0.84;0.16</p></td>\n",
    "        <td><p style=\"text-align:center\">0.84;0.16</p></td>\n",
    "        <td><p style=\"text-align:center\">0.84;0.16</p></td>\n",
    "        <td><p style=\"text-align:center\">0.84;0.16</p></td>\n",
    "        <td><p style=\"text-align:center\">0.16;0.84</p></td>\n",
    "        <td><p style=\"text-align:center\">0.16;0.84</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.09;-0.25</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.11;-0.39</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.11;0.39</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.1;-0.36</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.47</p></td>\n",
    "        <td><p style=\"text-align:center\">0.92;1.09</p></td>\n",
    "        <td><p style=\"text-align:center\">0.40;0.60</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;0.09</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;0.09</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;0.09</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;0.09</p></td>\n",
    "        <td><p style=\"text-align:center\">0.09;0.91</p></td>\n",
    "        <td><p style=\"text-align:center\">0.09;0.91</p></td>\n",
    "        <td><p style=\"text-align:center\">0.09;0.91</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.07;-0.34</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.04;-0.31</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.02;-0.28</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.01;-0.28</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.17</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;1.24</p></td>\n",
    "        <td><p style=\"text-align:center\">0.42;0.64</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.07;-0.34</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.04;-0.31</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.02;-0.28</p></td>\n",
    "        <td><p style=\"text-align:center\">-0.01;-0.28</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.17</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;1.24</p></td>\n",
    "        <td><p style=\"text-align:center\">0.42;0.64</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{*}$</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;</p></td>\n",
    "    </tr>\n",
    "</table>\n",
    "&emsp;&emsp;通过表6.2可以看出，对于风险投资问题，根据最优策略$\\pi_*$，得出如下结论：当资金小于本金（10）时，不进行投资；当现有资金大于等于本金（10）时，可以进行投资。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarsa算法解决风险投资问题代码\n",
    "import numpy as np\n",
    "from invest import InvestEnv\n",
    "np.random.seed(1)\n",
    "env = InvestEnv()\n",
    "                \n",
    "def trans_q(Q):#输出保留2位小数\n",
    "    new_q = []\n",
    "    new_q = [round(x,2) for x in Q]\n",
    "    return new_q\n",
    "def Attenuation(epsilon,alpha,episode_sum,episode):#epsilon和alpha衰减函数\n",
    "    epsilon = (float(episode_sum) - float(episode)) / float(episode_sum) * epsilon\n",
    "    alpha = (float(episode_sum) - float(episode)) / float(episode_sum) * alpha\n",
    "    return epsilon, alpha\n",
    "#输出函数\n",
    "def print_ff(list_q, Q, episode_i,epsilon_k):\n",
    "    list_s = [0,1,3,5,10,25,49]\n",
    "    for em in list_q:\n",
    "        if em == episode_i:\n",
    "            print(\"*******************************情节数:%s*******************************\"%(str(em)))\n",
    "            for state in list_s:\n",
    "                print(\"Q(%d,*)\"%(state) + str(trans_q(Q[state])))\n",
    "                prob = [epsilon_k / 2.0, epsilon_k / 2.0]\n",
    "                max_a = np.argmax(Q[state])\n",
    "                prob[max_a] = 1 - (epsilon_k / 2.0)\n",
    "                print('概率值' + str(trans_q(prob)))\n",
    "def print_dd(s, a, next_s, next_a, print_len, episode_i, Q,e_k,a_k,P):\n",
    "    if episode_i == print_len:\n",
    "        if s == 5 and a == 1 and next_s == 6 and next_a == 1:\n",
    "            print(\"*********************************单步的计算过程**************************************\")\n",
    "            print(\"alpha:\"+str(a_k))\n",
    "            print(\"epsilon:\"+str(e_k))\n",
    "            print(\"Q_state:\" + str(s))\n",
    "            print(Q[s][a])\n",
    "            print(\"资产：%6f\"%(P))\n",
    "            \n",
    "            print(\"Q_next_state\"+str(next_s))\n",
    "            print(Q[next_s][a])\n",
    "            print(\"update:\"+str(Q[s, a] + a_k * (0.8 * Q[next_s, next_a] - Q[s, a])))\n",
    "            \n",
    "            print(\"************************************************************************************\")\n",
    "\n",
    "#epsilon贪心策略\n",
    "def policy_epsilon_greedy(env, s, Q, epsilon):\n",
    "    Q_s = Q[s]\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(env.action_space)\n",
    "    else:\n",
    "        a = np.argmax(Q_s)\n",
    "    return a\n",
    "\n",
    "def Sarsa(env, episode_num, alpha, gamma, epsilon):\n",
    "    Q = np.zeros((env.state_space, env.action_space))\n",
    "    epsilon = epsilon\n",
    "    count=0\n",
    "    list_q = [0,1,7500,12500,19999,20000]\n",
    "    for episode_i in range(episode_num):\n",
    "        env.reset()\n",
    "        S = env.state\n",
    "        epsilon_k, alpha_k = Attenuation(epsilon,alpha,episode_num,episode_i)\n",
    "        A = policy_epsilon_greedy(env, S, Q, epsilon_k)\n",
    "        print_ff(list_q, Q, episode_i,epsilon_k)\n",
    "        if episode_i == 10000:\n",
    "            print(\"e_k:\"+str(epsilon_k)+\"a_k\" + str(alpha_k))\n",
    "        done = False\n",
    "        for i in range(1000):\n",
    "            next_S, R = env.step(A)\n",
    "            if next_S > 49:\n",
    "                Q[S, A] = Q[S, A] + alpha_k * (R + gamma * 0.0- Q[S, A])\n",
    "                break\n",
    "            next_A = policy_epsilon_greedy(env, next_S, Q, epsilon_k)\n",
    "            #输出某一个\n",
    "            if episode_i == 10000:\n",
    "                P_S = env.getState()\n",
    "                print_dd(S, A, next_S, next_A, 10000, episode_i, Q,epsilon_k,alpha_k,P_S)\n",
    "                if i == 494 or i==495:\n",
    "                    print(P_S)\n",
    "            Q[S, A] = Q[S, A] + alpha_k * (R + gamma * Q[next_S, next_A] - Q[S, A])\n",
    "            S = next_S\n",
    "            A = next_A\n",
    "        if S > 10.0:\n",
    "            count += 1\n",
    "    print(count)\n",
    "    return Q\n",
    "Q = Sarsa(env, 20000, 0.05, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Q-Learning算法\n",
    "&emsp;&emsp;Q-Learning算法的动作值函数更新迭代式为：\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\gets Q(S_t,A_t)+\\alpha[R_{t+1}+\\gamma \\max \\limits_{a}Q(S_{t+1},a) -Q(S_t,A_t)] \\label{6.4}\\tag{6.4}\n",
    "\\end{equation}\n",
    "&emsp;&emsp;从式（6.4）中可以看出，动作值函数$Q$的更新方向是最优动作值函数$q_*$，而与Agent所遵循的行为策略$b$无关。这里在评估动作值函数$Q$时，更新目标位最优动作值函数$q_*$的直接近似，故需要遍历当前状态所有的动作。在所有$(s,a)$都能被无限次访问的前提下，Q-learning就能以1的概率收敛到最优动作值函数和最优策略。<br>\n",
    "&emsp;&emsp;算法6.3给出了估算最优策略的Q-learning算法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" />\n",
    "&emsp;&emsp;<font size=3.5><b>算法6.3</b> 估算最优策略的Q-learning算法。</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 入：</b></font>\n",
    "<font size=3.5>折扣因子$\\gamma$，学习率$\\{\\alpha_k\\}^\\infty_{k=0}\\in[0,1]$，探索因子$\\{\\epsilon_k\\}^\\infty_{k=0}\\in[0,1]$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>初始化：</b></font>\n",
    "<font size=3.5>1.&emsp;对任意$s\\in \\mathcal{S}^+，a\\in \\mathcal{A}(s)$，初始化动作值函数，如$Q(s,a)\\gets \\mathbb{R}，Q(s^T,a)=0$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>2.&emsp;对$s\\in \\mathcal{S}^+，b(a|s)\\gets$基于动作值函数$Q(s,a)$构建$\\epsilon_0-$贪心策略</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>3.&emsp;<b>repeat</b> 对每个情节k=1,2,3$\\cdots$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>4.&emsp;&emsp;&emsp;初始化状态$S$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>5.&emsp;&emsp;&emsp;<b>while</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>6.&emsp;&emsp;&emsp;&emsp;&emsp;根据策略$b(a|s)$,在状态$S$下选择动作$A$:</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$A\\gets \\begin{cases}\n",
    "以概率1-\\epsilon_k，选择动作a\\in \\mathop{\\arg \\max} \\limits_{a'}Q(S,a')\\\\\n",
    "以概率\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|},在\\mathcal{A}(S)中均匀随机地选择动作\n",
    "\\end{cases} $<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>7.&emsp;&emsp;&emsp;&emsp;&emsp;执行动作$A$，到达状态$S'$，并得到奖赏$R$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>8.&emsp;&emsp;&emsp;&emsp;&emsp;$Q(S,A)\\gets Q(S,A)+\\alpha_k[R+\\gamma \\max \\limits_{a}Q(S',a)-Q(S,A)]$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>9.&emsp;&emsp;&emsp;&emsp;&emsp;$A^*\\gets \\mathop{\\arg \\max} \\limits_{a}Q(S,a)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>10.&emsp;&emsp;&emsp;&emsp; <b>for</b> $a\\in \\mathcal{A}(S)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>11.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $b(a|S)\\gets \\begin{cases}\n",
    "1-\\epsilon_k+\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad a=A^*\\\\\n",
    "\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad \\ \\ \\quad \\qquad a\\ne A^*\n",
    "\\end{cases} $</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>12.&emsp;&emsp;&emsp;&emsp; <b>end for</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>13.&emsp;&emsp;&emsp;&emsp; $S\\gets S'$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>14.&emsp;&emsp;&emsp;<b>until</b> $S=S^T$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 出：</b></font>\n",
    "&emsp;<font size=3.5>$q_*=Q,\\pi_*=b$</font><br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" /><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;Q-learning虽然是异策略，但从值函数更新迭代式中可以看出，它并没有使用到重要性采样，在此我们结合n-步TD更新，对其进行说明（由于涉及到第7章的内容，在此仅作简要说明）。<br>\n",
    "&emsp;&emsp;（1）1-步TD更新法<br>\n",
    "&emsp;&emsp;1-步Q-learning更新迭代式为：<br>\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\gets Q(S_t,A_t)-\\alpha \\rho [R_{t+1}+\\gamma \\max \\limits_{a}Q(S_{t+1},a) -Q(S_t,A_t)] \\quad (\\rho=1)\n",
    "\\end{equation}\n",
    "&emsp;&emsp;对($S_t,A_t)$进行单步更新时，$Q$值的更新只与最优动作值函数$q_*$相关，与策略无关（即与下一步动作无关），因此无论是目标策略和行为策略，中括号中的$\\max \\limits_{a}Q(S_t,a)$值都相同。另外，虽然$A_t$是由行为策略$b$产生的，但由于我们所要评估的是($S_t,A_t)$下的$Q$值，因此目标策略和行为策略始终一致，重要性采样比例始终为1，因此在中括号中的$Q(S_t,A_t)$也相当于乘以一个$\\rho$=1的重要性采样比例。<br>\n",
    "&emsp;&emsp;（2）n-步TD更新法<br>\n",
    "&emsp;&emsp;为了说明问题，以简单的2-步TD为例，2-步Q-Learning更新递归式为：<br>\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t) \\gets Q(S_t,A_t)-\\alpha \\rho [R_{t+1}+\\gamma R_{t+2}+\\gamma^2 \\max \\limits_{a'}Q(S_{t+2},a') -Q(S_t,A_t)]\n",
    "\\end{equation}\n",
    "&emsp;&emsp;这里$R_{t+2}$的获取与动作$A_{t+1}$相关，而$A_{t+1}$是由行为$b$获得的，即当时用到$R_{t+1}$以后的信息时，就需要使用重要性采样来处理后序动作产生的的奖赏。这一思想在异策略n-步Sarsa算法的公式中也有体现，即$\\rho_{t+1:t+n-1}$的起始下标为$t$+1。<br>\n",
    "&emsp;&emsp;<font><b>例6.3</b></font> 使用Q-learning算法解决例4.1的确定环境扫地机器人问题。这里的参数设置与例6.1相同，即将动作值函数设置为24$\\times$4的二维数组，且初值都为0。初始学习率参数$\\alpha_0$为0.05，$\\gamma$为0.8，使用$\\epsilon-$贪心策略（$\\epsilon_0$=0.5）。<br>\n",
    "&emsp;&emsp;当运行到11000个情节时，到达状态$S$=15，下一个动作为向下（$A=Down$,即第1个动作），此时$Q$[15][1]=0.983040。执行动作$A$到达下一个状态$S'$=10，得到奖赏$R$=0，$S_{10}$对应的4个动作的$Q$值函数为：$Q$[10]=[1.2288,0.8,\\*.\\*\\*,1.2276]。因为在$Q$[10]中，最大的动作值函数为$Q$[10][0]=1.2288，此时$\\alpha_{11000}$=0.028，所以更新动作值函数$Q$[15][1]的公式为：<br>\n",
    "&emsp;&emsp;\n",
    "$\n",
    "\\begin{aligned}\n",
    "Q[15][1]&=Q[15][1]+\\alpha_{11000}(R+\\gamma *\\max{(Q[10])}-Q[15][1]) \\\\\n",
    "    &=Q[15][1]+\\alpha_{11000}(R+\\gamma *{Q[10][0]}-Q[15][1]) \\\\\n",
    "    &=0.983040+0.028*(0+0.8*1.2288-0.983040)\\\\ \n",
    "    &=0.983040\\\\\n",
    "    &\\approx 0.98\n",
    "\\end{aligned}$\n",
    "<br>\n",
    "&emsp;&emsp;表6.3给出了使用Q-learning算法解决确定环境扫地机器人问题的Q值迭代过程。从表6.3中可以看出当迭代到第25000个情节时，动作值函数已经收敛。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" align=“center” width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.3 基于Q-Learning算法的确定环境扫地机器人问题的Q值迭代过程</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"80px\"><p style=\"text-align:center\">&emsp;&emsp;</p></th>\n",
    "        <th><p style=\"text-align:center\">$S_5$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{10}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{18}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{20}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{24}$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.62;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.63;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.91;1.00;*.**;0.74</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.0</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.91;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.77;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.77;0.12;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.09;0.09;0.09;0.74</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.82;0.00;0.17</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.82;0.17;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.92;1.00;*.**;0.81</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.92;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.08;0.83;0.00;0.08</p></td>\n",
    "        <td><p style=\"text-align:center\">0.83;0.08;0.00;0.08</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.06;0.06;0.81</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.88;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.88;0.12;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{24999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.92;1.00;*.**;0.83</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.92;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{24999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    " <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{25000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.92;1.00;*.**;0.83</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.92;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{25000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{*}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-learning算法\n",
    "from book_gridword import GridWorldEnv\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = GridWorldEnv()\n",
    "#有效动作空间\n",
    "def vilid_action_space(s):\n",
    "    action_sacpe = []\n",
    "    if s % 5 != 0:#左\n",
    "        action_sacpe.append(0)\n",
    "    if s % 5 != 4:#右\n",
    "        action_sacpe.append(1)\n",
    "    if s <= 19:#上\n",
    "        action_sacpe.append(2)\n",
    "    if s >= 5:#下\n",
    "        action_sacpe.append(3)\n",
    "    return action_sacpe\n",
    "def policy_epsilon_greedy(s, Q, epsilon):\n",
    "    Q_s = Q[s]\n",
    "    action = vilid_action_space(s)\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(action)\n",
    "    else:\n",
    "        index_a = np.argmax([Q_s[i] for i in action])\n",
    "        a = action[index_a]\n",
    "    return a\n",
    "\n",
    "def trans1(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(Q_S[2])\n",
    "    new_Q.append(Q_S[3])\n",
    "    new_Q.append(Q_S[0])\n",
    "    new_Q.append(Q_S[1])\n",
    "    return new_Q\n",
    "def trans(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(round(Q_S[2],2))\n",
    "    new_Q.append(round(Q_S[3],2))\n",
    "    new_Q.append(round(Q_S[0],2))\n",
    "    new_Q.append(round(Q_S[1],2))\n",
    "    return new_Q\n",
    "def print_dd(s, a, next_s, print_len, episode_i, Q,e_k,a_k):\n",
    "    for i in range(2):  \n",
    "        if episode_i == int(print_len * (0.1 * i + 1)):\n",
    "            if s == 15 and a == 3 and next_s == 10:\n",
    "                print(\"*********************************计算过程*****************************************\")\n",
    "                print(\"alpha:\"+str(a_k))\n",
    "                print(\"epsilon:\"+str(e_k))\n",
    "                print(\"state:\" + str(int(print_len * (0.1 * i + 1))))\n",
    "                print(\"Q(%d,%d)\"%(s,a))\n",
    "                print(Q[s][a])\n",
    "                print(\"Q(%d,*)\"%(next_s))\n",
    "                print(trans1(Q[next_s]))\n",
    "                print('output:'+str(Q[s][a] + a_k * (0.8 * np.max(Q[next_s]) - Q[s, a])))\n",
    "def print_ff(list_q, Q, episode_i,epsilon_k):\n",
    "    list_s = [5,10,18,20,24]\n",
    "    for em in list_q:\n",
    "        if em == episode_i:\n",
    "            print('****************************************episode: '+ str(em)+'****************************************')\n",
    "            for state in list_s:\n",
    "                print(\"state:%d \"%(state) + str(trans(Q[state])))\n",
    "                action = vilid_action_space(state)\n",
    "                len_a = len(action)\n",
    "                e_p = epsilon_k / float(len_a)\n",
    "                max_a = np.argmax(Q[state])\n",
    "                prob = []\n",
    "                index_a = np.argmax([Q[state][i] for i in action])\n",
    "                for i in range(4):#计算epsilon\n",
    "                    if i not in action:\n",
    "                        prob.append(0.0)\n",
    "                    else:\n",
    "                        if i == action[index_a]:\n",
    "                            prob.append(1 - epsilon_k + e_p)\n",
    "                        else:\n",
    "                            prob.append(e_p)\n",
    "                print('epsilon:' + str(trans(prob)))\n",
    "                \n",
    "def Attenuation(epsilon,alpha,episode_sum,episode):\n",
    "    epsilon = (float(episode_sum) - float(episode)) / float(episode_sum) * epsilon\n",
    "    alpha = (float(episode_sum) - float(episode)) / float(episode_sum) * alpha\n",
    "    return epsilon, alpha\n",
    "\n",
    "def Q_Learning(env, episode_num, alpha, gamma, epsilon):\n",
    "    Q = np.zeros((env.n_width * env.n_height, env.action_space.n))\n",
    "    list_q = [0,1,7500,12500,24999]\n",
    "    for episode_i in range(episode_num):\n",
    "        env.reset()\n",
    "        s = env.state\n",
    "        epsilon_k, alpha_k = Attenuation(epsilon,alpha,episode_num,episode_i)\n",
    "        print_ff(list_q, Q, episode_i,epsilon_k)\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = policy_epsilon_greedy(s, Q, epsilon_k)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            print_dd(s, a, next_s, 10000, episode_i, Q, epsilon_k, alpha_k)\n",
    "            Q[s, a] += alpha_k * (r + gamma * np.max(Q[next_s]) - Q[s, a])\n",
    "            s = next_s\n",
    "    return Q\n",
    "\n",
    "Q = Q_Learning(env, 25000, 0.05, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 期望Sarsa算法\n",
    "&emsp;&emsp;从式（6.3）和式（6.4）中可以看出，Sarsa算法和Q-Learning算法都存在随机选择动作而产生方差的问题。通过对Sarsa算法进行改进，得到一种异策略TD算法。该算法考虑当前策略$\\pi$下所有动作的可能性（概率值），利用动作值函数的期望值取代某一特定动作值函数来更新估计值。该算法称为期望Sarsa（Expected Sarsa）算法。<br>\n",
    "&emsp;&emsp;期望Sarsa的目标值为：<br>\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "G_t &=R_{t+1}+\\gamma \\mathbb{E}[Q(S_{t+1},A_{t+1})|S_{t+1}] \\\\\n",
    "&=R_{t+1}+\\gamma \\sum_a \\pi(a|S_{t+1})Q(S_{t+1},a)\n",
    "\\end{split} \\label{6.5}\\tag{6.5}\n",
    "\\end{equation}\n",
    "&emsp;&emsp;其中，$\\mathbb{E}[Q(S_{t+1},A_{t+1})|S_{t+1}]$表示下一个状态$S_{t+1}$的动作值函数期望值。期望Sarsa动作值函数的更新递归式为：<br>\n",
    "\\begin{equation}\n",
    "Q(S_t,A_t)\\gets Q(S_t,A_t)+\\alpha [R_{t+1}+\\gamma \\sum_a\\pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)]\n",
    " \\label{6.6}\\tag{6.6}\n",
    "\\end{equation}\n",
    "&emsp;&emsp;相比于Sarsa算法，期望Sarsa算法计算更为复杂。但通过计算期望能够有效地消除因随机选择$A_{t+1}$而产生的方差。因此通常情况下，期望Sarsa明显优于Sarsa。另外期望Sarsa还可以使用异策略方法，将Q-learning进行推广，并提升性能。<br>\n",
    "&emsp;&emsp;综上所述，得到Sarsa，Q-learning，期望Sarsa的更新图，如下所示：<br>\n",
    "<center>\n",
    "<table align=\"center\" width=\"100%\">\n",
    "    <tr>\n",
    "        <th><center><img src='./image/图6.3.jpg' width='200px' height='250px'></center></th>\n",
    "        <th><center><img src='./image/图6.4.jpg' width='310px' height='250px'></center></th>\n",
    "        <th><center><img src='./image/图6.5.jpg' width='199px' height='250px'></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><font size=2.5>图6.3&ensp;Sarsa更新图</font></center></td>\n",
    "        <td><center><font size=2.5>图6.4&ensp;Q-Learning更新图</font></center></td>\n",
    "        <td><center><font size=2.5>图6.5&ensp;期望Sarsa更新图</font></center>\n",
    "</td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "&emsp;&emsp;<b>例6.4</b> 使用期望Sarsa算法解决例4.1确定环境扫地机器人问题。这里参数设置与例6.1相同。<br>\n",
    "&emsp;&emsp;当运行到第6000个情节时，到达状态$S$=15，下一个动作为向下（$A=Down$，即第1个动作），此时$Q$[15][1]=0.506253。执行动作$A$得到奖赏为$R$=0，下一个状态为$S'$=10该状态对应的4个动作的$Q$值函数为：$Q$[10]=[0.53319,0.76667,*.**,0.43998]。此时，状态10采取各个动作的概率为$prob=[0.1177,0.777,*.**,0.117]，\\alpha_{6000}$=0.035。更新动作值函数$Q$[15][1]的公式为：<br>\n",
    "&emsp;&emsp;$\n",
    "\\begin{split}\n",
    "Q[15][1]&=Q[15][1]+\\alpha_{6000}* \\biggl (R+\\gamma *\\sum_{a\\in A(10)}\\Bigl (prob[a]*\\bigl (Q[10][a]\\bigl )\\Bigl )-Q[15][1]\\biggl )\\\\\n",
    "&=Q[15][1]+\\alpha_{6000}*\\bigl (R+\\gamma *(prob[0]*Q[10][0]+prob[1]*Q[10][1]+prob[3]*Q[10][3])-Q[15][1]\\bigl ) \\\\\n",
    "&=0.506253+0.035*\\bigl (0+0.8*(0.117*0.53319+0.777*0.76667+0.117*0.43998)-0.506253\\bigl ) \\\\\n",
    "&=0.50677447 \\\\\n",
    "&\\approx 0.51\n",
    "\\end{split} $\n",
    "<br>\n",
    "&emsp;&emsp;表6.4给出了使用期望Sarsa算法解决扫地机器人的问题的Q值迭代过程。从表6.4中可以看出当迭代到第20000个情节时，动作值函数已经收敛。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" align=“center” width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.4 基于期望Sarsa算法的确定环境扫地机器人问题的Q值迭代过程</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"80px\"><p style=\"text-align:center\">&emsp;&emsp;</p></th>\n",
    "        <th><p style=\"text-align:center\">$S_5$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{10}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{18}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{20}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{24}$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.62;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;*.**;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.63;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.50;1.00;*.**;0.49</p></td>\n",
    "        <td><p style=\"text-align:center\">0.56;0.71;*.**;0.50</p></td>\n",
    "        <td><p style=\"text-align:center\">1.68;1.29;1.50;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.78;*.**;0.96</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.69;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{7500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.10;0.79;0.00;0.10</p></td>\n",
    "        <td><p style=\"text-align:center\">0.10;0.79;0.00;0.10</p></td>\n",
    "        <td><p style=\"text-align:center\">0.08;0.08;0.08;0.77</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.16;0.00;0.84</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.84;0.16;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.51;1.00;*.**;0.50</p></td>\n",
    "        <td><p style=\"text-align:center\">0.60;0.73;*.**;0.57</p></td>\n",
    "        <td><p style=\"text-align:center\">1.75;1.44;1.63;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.92;*.**;1.08</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.78;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{12500}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.88;0.00;0.06</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.88;0.00;0.06</p></td>\n",
    "        <td><p style=\"text-align:center\">0.05;0.05;0.05;0.86</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.09;0.00;0.91</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.91;0.09;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.51;1.00;*.**;0.50</p></td>\n",
    "        <td><p style=\"text-align:center\">0.60;0.74;*.**;0.57</p></td>\n",
    "        <td><p style=\"text-align:center\">1.78;1.51;1.67;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.03;*.**;1.20</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.85;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{19999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "<tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.51;1.00;*.**;0.50</p></td>\n",
    "        <td><p style=\"text-align:center\">0.60;0.74;*.**;0.57</p></td>\n",
    "        <td><p style=\"text-align:center\">1.78;1.51;1.67;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.03;*.**;1.20</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;3.00;1.85;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{*}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#期望Sarsa算法\n",
    "from book_gridword import GridWorldEnv\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "np.random.seed(1)\n",
    "env = GridWorldEnv()\n",
    "#有效动作空间\n",
    "def vilid_action_space(s):\n",
    "    action_sacpe = []\n",
    "    if s % 5 != 0:#左\n",
    "        action_sacpe.append(0)\n",
    "    if s % 5 != 4:#右\n",
    "        action_sacpe.append(1)\n",
    "    if s <= 19:#上\n",
    "        action_sacpe.append(2)\n",
    "    if s >= 5:#下\n",
    "        action_sacpe.append(3)\n",
    "    return action_sacpe\n",
    "def policy_epsilon_greedy(s, Q, epsilon):\n",
    "    Q_s = Q[s]\n",
    "    action = vilid_action_space(s)\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(action)\n",
    "    else:\n",
    "        index_a = np.argmax([Q_s[i] for i in action])\n",
    "        a = action[index_a]\n",
    "    return a\n",
    "\n",
    "def compute_epsion(s,Q,epsilon):\n",
    "    max_a = np.argmax(Q[s])\n",
    "    action = vilid_action_space(s)\n",
    "    len_all_a = len(action)\n",
    "    prob_l = [0.0,0.0,0.0,0.0]\n",
    "    for index_a in action:\n",
    "        if index_a == max_a:\n",
    "            prob_l[index_a] = 1.0 - epsilon + (epsilon / len_all_a)\n",
    "        else:\n",
    "            prob_l[index_a] = epsilon / len_all_a\n",
    "    return prob_l\n",
    "def compute_e_q(prob, q_n):\n",
    "    sum = 0.0\n",
    "    for i in range(4):\n",
    "        sum += prob[i] * q_n[i]\n",
    "    return sum\n",
    "\n",
    "def trans1(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(Q_S[2])\n",
    "    new_Q.append(Q_S[3])\n",
    "    new_Q.append(Q_S[0])\n",
    "    new_Q.append(Q_S[1])\n",
    "    return new_Q\n",
    "\n",
    "def print_dd(s, a, next_s, print_len, episode_i, Q, e_k, a_k):\n",
    "    for i in range(50):  \n",
    "        if episode_i == int(print_len * ((0.02 * i)+1)):\n",
    "            if s == 15 and a == 3 and next_s == 10:\n",
    "                print(\"*****************************计算过程***********************************************\")\n",
    "                print(\"alpha:\"+str(a_k))\n",
    "                print(\"epsilon:\"+str(e_k))\n",
    "                print(\"state:\" + str(int(print_len * (1 + (0.02 * i)))))\n",
    "                print(\"Q(%d,%d)\"%(s,a))\n",
    "                print(Q[s][a])\n",
    "                print(\"Q(%d,*)\"%(next_s))\n",
    "                print(trans1(Q[next_s]))\n",
    "                prob_l = compute_epsion(next_s, Q, e_k)\n",
    "                print('概率'+ str(trans1(prob_l)))\n",
    "                Q_e = compute_e_q(prob_l,Q[next_s])\n",
    "                print('update:'+str(Q[s, a] + a_k * (0.8 * Q_e - Q[s, a])))\n",
    "\n",
    "def trans(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(round(Q_S[2],2))\n",
    "    new_Q.append(round(Q_S[3],2))\n",
    "    new_Q.append(round(Q_S[0],2))\n",
    "    new_Q.append(round(Q_S[1],2))\n",
    "    return new_Q\n",
    "def print_ff(list_q, Q, episode_i,epsilon_k):\n",
    "    list_s = [5,10,18,20,24]\n",
    "    for em in list_q:\n",
    "        if em == episode_i:\n",
    "            print(\"**************************\"+'episode: '+ str(em)+\"*********************************************\")\n",
    "            for state in list_s:\n",
    "                print(\"Q(%d,*) \"%(state) + str(trans(Q[state])))\n",
    "                action = vilid_action_space(state)\n",
    "                len_a = len(action)\n",
    "                e_p = epsilon_k / float(len_a)\n",
    "                prob = []\n",
    "                index_a = np.argmax([Q[state][i] for i in action])\n",
    "                for i in range(4):#计算epsilon\n",
    "                    if i not in action:\n",
    "                        prob.append(0.0)\n",
    "                    else:\n",
    "                        if i == action[index_a]:\n",
    "                            prob.append(1 - epsilon_k + e_p)\n",
    "                        else:\n",
    "                            prob.append(e_p)\n",
    "                print('概率值:' + str(trans(prob)))\n",
    "                \n",
    "def Attenuation(epsilon,alpha,episode_sum,episode):\n",
    "    epsilon = (float(episode_sum) - float(episode)) / float(episode_sum) * epsilon\n",
    "    alpha = (float(episode_sum) - float(episode)) / float(episode_sum) * alpha\n",
    "    return epsilon, alpha\n",
    "\n",
    "\n",
    "def  Expectation_sarsa(env, episode_num, alpha, gamma, epsilon):\n",
    "    Q = np.zeros((env.n_width * env.n_height, env.action_space.n))\n",
    "    Q_queue = Queue(maxsize=11)\n",
    "    list_q = [0,1,7500,12500,19999]\n",
    "    for episode_i in range(episode_num):\n",
    "        env.reset()\n",
    "        s = env.state\n",
    "        epsilon_k, alpha_k = Attenuation(epsilon,alpha,episode_num,episode_i)\n",
    "        print_ff(list_q, Q, episode_i,epsilon_k)\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = policy_epsilon_greedy(s, Q, epsilon_k)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            print_dd(s, a, next_s, 6000, episode_i, Q, epsilon_k,alpha_k)\n",
    "            prob_l = compute_epsion(next_s, Q, epsilon_k)\n",
    "            Q_e = compute_e_q(prob_l,Q[next_s])\n",
    "            Q[s, a] += alpha_k * (r + gamma * Q_e - Q[s, a])\n",
    "            s = next_s\n",
    "    return Q\n",
    "\n",
    "Q = Expectation_sarsa(env,20000, 0.05, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 最大化偏差与Double Q-Learning\n",
    "### 6.3.1 最大化偏差\n",
    "&emsp;&emsp;Sarsa和Q-Learning算法都是采用目标策略最大化的思想，即采用基于$\\epsilon-$贪心策略（Sarsa）或者贪心策略（Q-Learning）的目标策略，将值函数估计值中的最大值作为真实值的估计。这样会造成动作值函数估计值相对于真实值在一个正向偏差，把这个偏差叫做最大化偏差。也就是说，状态值$s$被过度估计（overestimation）。假设状态$s$存在多个动作$a$，他们的真实状态值函数$q(s,a)$均为0，但他们的动作值函数估计$Q(s,a)$有正有负，那么此时最大值函数估计值就会出现正值的情况。最大化偏差不会导致算法失败，但是会让收敛速度变慢。<br>\n",
    "&emsp;&emsp;下面通过以下实例对最大化偏差进行说明。<br>\n",
    "&emsp;&emsp;<b>例6.5</b> 考虑最大化偏差问题。Agent从A点出发，只能向左或者向右，不可转变方向。从A向右达到终点，奖励为0；向左到达B，奖励为0；从B往左有很多动作可以选择，执行不同的动作都会到达终点，所获奖赏均服从均值为-0.1方差为1的正态分布。<br>\n",
    "<center><br>\n",
    "        <center><img src='./image/图6.6.jpg'></center><br>\n",
    "<center>图6.6&ensp;最大化偏差问题</center>\n",
    "&emsp;&emsp;从A往右走是固定奖励，没有方差，所以其动作值函数$Q(A,Right)$始终为0；向左走时存在大于0的奖励，所以当向左走时值函数估计值大于0。由于贪心策略改进得到的最优动作是向左的，但这显然不是最优策略。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6.3.2 Double learning\n",
    "&emsp;&emsp;最大化偏差产生的根本原因在于：在每个情节中，相同的样本即用来确定价值最大的动作又用来估计它的价值。通常用采用Double learning思想来解决这一问题。<br>\n",
    "&emsp;&emsp;Double learning核心步骤主要包括以下4个步骤：<br>\n",
    "&emsp;&emsp;（1）对于真实的动作值函数$q(a)$，构建两个独立的动作值函数估计$Q_1(a)$和$Q_2(a)$对其进行估计；<br>\n",
    "&emsp;&emsp;（2）根据$Q_1(a)$获取最优动作$A^*=\\mathop{\\arg \\max} \\limits_{a}Q_1(a)$；<br>\n",
    "&emsp;&emsp;（3）利用$Q_2(a)$来评估$A^*$的动作值函数$Q_2(A^*)=Q_2\\bigl (\\mathop{\\arg \\max} \\limits_{a}Q_1(a)\\bigl )$。在某种意义上对$\\mathbb{E}\\bigl [Q_2(A^*)\\bigl ]=q(A^*)$来说，$Q_2(A^*)$是无偏估计。<br>\n",
    "&emsp;&emsp;（4）通过反转两个估计的角色来获得第2个无偏估计$Q_1\\bigl (\\mathop{\\arg \\max}\\limits_{a}(Q_2(a)\\bigl )$，以此交替更新$Q_1(a)$和$Q_2(a)$。<br>\n",
    "&emsp;&emsp;虽然Double learning同时学习两个值函数估计值，但每次只更新了一个值函数，所以与Q-Learning相比，每步的计算量没有增加，只是需要的存储空间增加了一倍。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.3 Double Q-Learning\n",
    "&emsp;&emsp;Double learning算法可以推广到全马尔可夫决策过程问题（full MDPs）中，将Double learning思想与Q-Learning相结合得到Double Q-Learning算法。在Double Q-Learning中，以50$\\%$的概率利用$Q_1$产生最优动作，然后更新$Q_2$估计值；而另外50$\\%$的概率利用$Q_2$产生最优动作，然后更新$Q_1$估计值。<br>\n",
    "&emsp;&emsp;Doubel Q_Learning算法的动作值函数递归式为：<br>\n",
    "\\begin{equation}\n",
    "Q_1(S_t,A_t)\\gets Q_1(S_t,A_t)+\\alpha \\bigl [R_{t+1}+\\gamma Q_2\\bigl (S_{t+1},\\mathop{\\arg \\max} \\limits_{a}Q_1(S_{t+1},a)\\bigl )-Q_1(S_t,A_t)\\bigl ]\n",
    " \\label{6.7}\\tag{6.7}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;算法6.4给出了估算最优策略的Double Q-learning算法。<br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" />\n",
    "&emsp;&emsp;<font size=3.5><b>算法6.4</b> 估算最优策略的Double Q-learning算法。</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 入：</b></font>\n",
    "<font size=3.5>折扣因子$\\gamma$，学习率$\\{\\alpha_k\\}^\\infty_{k=0}\\in[0,1]$，探索因子$\\{\\epsilon_k\\}^\\infty_{k=0}\\in[0,1]$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>初始化：</b></font>\n",
    "<font size=3.5>1.&emsp;对任意$s\\in \\mathcal{S}^+，a\\in \\mathcal{A}(s)$，初始化动作值函数，如$Q_1(s,a)、Q_2(s,a)\\gets \\mathbb{R}，Q_1(s^T,a)、Q_2(s^T,a)\\gets 0$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>2.&emsp;对$s\\in \\mathcal{S}^+，b(a|s)\\gets$基于动作值函数$Q_1(s,a)+Q_2(s,a)$构建$\\epsilon_0-$贪心策略</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>3.&emsp;设定一个(0,1)的随机数生成器</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>4.&emsp;<b>repeat</b> 对每个情节$k$=1,2,3$\\cdots$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>5.&emsp;&emsp;&emsp;初始化状态$S$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>6.&emsp;&emsp;&emsp;<b>while</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>7.&emsp;&emsp;&emsp;&emsp;&emsp;根据策略$b(a|s)$,在状态$S$下选择动作$A$:</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$A\\gets \\begin{cases}\n",
    "以概率1-\\epsilon_k，选择动作a\\in \\mathop{\\arg \\max} \\limits_{a'}(Q_1(S,a')+Q_2(S,a'))\\\\\n",
    "以概率\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|},在\\mathcal{A}(S)中均匀随机地选择动作\n",
    "\\end{cases} $<br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>8.&emsp;&emsp;&emsp;&emsp;&emsp;执行动作$A$，到达状态$S'$，并得到奖赏$R$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>9.&emsp;&emsp;&emsp;&emsp;&emsp;$prob\\gets$随机数生成器</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>10.&emsp;&emsp;&emsp;&emsp; <b>if</b> $prob<0.5\\ then$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>11.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$Q_1(S,A)\\gets Q_1(S,A)+\\alpha_k\\bigl [R+\\gamma Q_2(S',\\mathop{\\arg \\max} \\limits_{a}Q_1(S',a))-Q_1(S,A)\\bigl ]$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>12.&emsp;&emsp;&emsp;&emsp; <b>else</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>13.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$Q_2(S,A)\\gets Q_2(S,A)+\\alpha_k\\bigl [R+\\gamma Q_1(S',\\mathop{\\arg \\max} \\limits_{a}Q_2(S',a))-Q_2(S,A)\\bigl ]$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>14.&emsp;&emsp;&emsp;&emsp; <b>end if</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>15.&emsp;&emsp;&emsp;&emsp; $A^*\\gets \\mathop{\\arg \\max} \\limits_{a}\\bigl(Q_1(S,a)+Q_2(S,a)\\bigl)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>16.&emsp;&emsp;&emsp;&emsp; <b>for</b> &emsp;$a\\in \\mathcal{A}(S)$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>17.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; $b(a|S)\\gets \\begin{cases}\n",
    "1-\\epsilon_k+\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad a=A^*\\\\\n",
    "\\dfrac{\\epsilon_k}{|\\mathcal{A}(S)|}\\qquad \\qquad \\ \\ \\quad \\qquad a\\ne A^*\n",
    "\\end{cases} $</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>18.&emsp;&emsp;&emsp;&emsp; <b>end for</b></font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>19.&emsp;&emsp;&emsp;&emsp; $S\\gets S'$</font><br>\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<font size=3.5>20.&emsp;&emsp;&emsp;<b>until</b> $S=S^T$</font><br>\n",
    "<hr>\n",
    "&emsp;&emsp;<font size=3.5><b>输 出：</b></font>\n",
    "&emsp;<font size=3.5>$q_*=Q,\\pi_*=b$</font><br>\n",
    "<hr style=\"height:1px;border:none;border-top:1px solid #555555;\" /><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在算法6.4中，行为策略b由$Q_1(s,a)+Q_2(s,a)$共同影响产生，可以在$\\epsilon-$贪心策略的基础上，使用两个值函数估计的平均值作为最优动作选择方式。<br>\n",
    "&emsp;&emsp;Double learning采用不同的样本来学习两个独立的动作值函数。在算法中该思想具体表现在以50$\\%$的概率对更新迭代式进行随机选择。在经过一段时间后，两个动作值函数估计都将收敛于真实值。<br>\n",
    "&emsp;&emsp;<font><b>例6.5</b></font> 使用Double Q-Learning算法解决例4.1确定环境扫地机器人问题。将两个动作值函数$Q_1$和$Q_2$都设置为24x4的二维数组，且初值都为0。初始学习率参数$\\alpha_0$为0.05，$\\gamma$为0.8，使用$\\epsilon-$贪心策略（$\\epsilon_0=0.5$）。<br>\n",
    "&emsp;&emsp;当运行到第8640个情节时，到达状态$S=15$，下一个动作为向下（$A=Down，即第1个动作$），此时随机值小于0.5，更新$Q_1$值，$Q_1$[15][1]=0.98135。执行动作$A$得到奖赏为$R=0$，下一个状态为$S'=10，Q_1$[10]=[1.228,0.8,\\*.\\*\\*,0.9728]，$Q_2$[10]=[1.2285,0.8,\\*.\\*\\*,1.1]。$Q_1$[10]最大动作为0，所以使用$Q_2$[10]的第0个动作值更新动作值函数$Q_1$[15][1]，此时$\\alpha_{8640}=0.0392$。更新迭代式为：<br>\n",
    "&emsp;&emsp;$\n",
    "\\begin{split}\n",
    "Q[15][1]&=Q[15][1]+\\alpha_{8640}*(R+\\gamma *Q_2(10,\\mathop{\\arg \\max} \\limits_{a}Q_1[10][a])-Q[15][1])\\\\\n",
    "&=Q_1[15][1]+\\alpha_{8640}*(R+\\gamma *Q_2[10][0]-Q_1[15][1])\\\\\n",
    "&=0.98135+0.0392*(0+0.8*1.2285-0.98135)\\\\\n",
    "&=0.98141\\\\\n",
    "&\\approx 0.98\n",
    "\\end{split} $\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;表6.5给出了使用Double Q-Learning算法解决扫地机器人问题的$Q$值迭代过程。从表6.5中可以看出当迭代到第40000个情节时，动作值函数已经收敛。这里$Q_i^j$下标$i$表示迭代的情节数，上标$j$（$j$=1,2）表示第$j$个$Q$值。<br>\n",
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" align=“center” width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.5 基于Double Q-Learning算法的确定环境扫地机器人问题的Q值迭代过程</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"80px\"><p style=\"text-align:center\">&emsp;&emsp;</p></th>\n",
    "        <th><p style=\"text-align:center\">$S_5$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{10}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{18}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{20}$</p></td>\n",
    "        <th><p style=\"text-align:center\">$S_{24}$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_0^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_0$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.62;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.05;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_1^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;*.**;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;0.00;0.00;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.17;0.00;0.67</p></td>\n",
    "        <td><p style=\"text-align:center\">0.17;0.67;0.00;0.17</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.12;0.63;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.00;0.75</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.25;0.75;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{10000}^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;1.00;*.**;0.64</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.09</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.34;0.47;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{10000}^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.79;1.00;*.**;0.64</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.17</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.43;0.43;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{10000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.12;0.75;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.75;0.12;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.09;0.09;0.09;0.72</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.81;0.00;0.19</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.81;0.19;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{20000}^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.86;1.0;*.**;0.68</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.8;*.**;1.22</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.9, 0.92;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{20000}^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.81;1.00;*.**;0.69</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.22</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.88;0.66;*.**;</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{20000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.08;0.83;0.00;0.08</p></td>\n",
    "        <td><p style=\"text-align:center\">0.83;0.08;0.00;0.08</p></td>\n",
    "        <td><p style=\"text-align:center\">0.06;0.06;0.06;0.81</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.88;0.00;0.12</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.88;0.12;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "        <td><p style=\"text-align:center\">$\\vdots$</p></td>\n",
    "    </tr>\n",
    "  <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{39999}^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.87;1.00;*.**;0.68</p></td>\n",
    "        <td><p style=\"text-align:center\">0.82;1.00;*.**;0.70</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.93;0.95;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{39999}^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.82;1.00;*.**;0.70</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.22</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.91;0.73;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{39999}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{40000}^1$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.87;1.00;*.**;0.68</p></td>\n",
    "        <td><p style=\"text-align:center\">0.82;1.00;*.**;0.70</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.93;0.95;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$Q_{40000}^2$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.82;1.00;*.**;0.70</p></td>\n",
    "        <td><p style=\"text-align:center\">1.23;0.80;*.**;1.22</p></td>\n",
    "        <td><p style=\"text-align:center\">1.92;1.92;1.92;3.00</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;1.23;*.**;1.23</p></td>\n",
    "        <td><p style=\"text-align:center\">*.**;2.91;0.73;*.**</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{40000}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "   <tr>\n",
    "        <td><p style=\"text-align:center\">$\\pi_{*}$</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">1.00;0.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;0.00;0.00;1.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "        <td><p style=\"text-align:center\">0.00;1.00;0.00;0.00</p></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Q_learning算法\n",
    "from book_gridword import GridWorldEnv\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = GridWorldEnv()\n",
    "def trans1(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(Q_S[2])\n",
    "    new_Q.append(Q_S[3])\n",
    "    new_Q.append(Q_S[0])\n",
    "    new_Q.append(Q_S[1])\n",
    "    return new_Q\n",
    "def print_dd(s, a, next_s, print_len, episode_i, Q1, Q2, e_k, a_k):  \n",
    "    if episode_i == 8640:\n",
    "        if s == 15 and a == 3 and next_s == 10:\n",
    "            print(\"***********************************计算过程****************************************\")\n",
    "            print(\"alpha:\"+str(a_k))\n",
    "            print(\"epsilon:\"+str(e_k))\n",
    "            #print(\"state:\" + str(int(print_len * (1 + (0.02 * i)))))\n",
    "            print(trans1(Q1[next_s]))\n",
    "            print(trans1(Q1[s]))\n",
    "            print(trans1(Q2[next_s]))\n",
    "            print('update:'+str(Q1[s][a]+a_k * (0.8 * Q2[next_s,np.argmax(Q1[next_s])] - Q1[s,a])))\n",
    "\n",
    "def trans(Q_S):\n",
    "    new_Q = []\n",
    "    new_Q.append(round(Q_S[2],2))\n",
    "    new_Q.append(round(Q_S[3],2))\n",
    "    new_Q.append(round(Q_S[0],2))\n",
    "    new_Q.append(round(Q_S[1],2))\n",
    "    return new_Q\n",
    "def print_ff(list_q, Q1,Q2, episode_i,epsilon_k):\n",
    "    list_s = [5,10,18,20,24]\n",
    "    for em in list_q:\n",
    "        if em == episode_i:\n",
    "            print('*******************************episode:%s**************************'%(str(em)))\n",
    "            for state in list_s:\n",
    "                print(\"state Q1:%d \"%(state) + str(trans(Q1[state])))\n",
    "            for state in list_s:\n",
    "                print(\"state Q2:%d \"%(state) + str(trans(Q2[state])))\n",
    "            for state in list_s:\n",
    "                action = vilid_action_space(state)\n",
    "                len_a = len(action)\n",
    "                e_p = epsilon_k / float(len_a)\n",
    "                max_a = np.argmax(Q1[state]+Q2[state])\n",
    "                prob = []\n",
    "                Q = Q1[state]+Q2[state]\n",
    "                index_a = np.argmax([Q[i] for i in action])\n",
    "                for i in range(4):#计算epsilon\n",
    "                    \n",
    "                    if i not in action:\n",
    "                        prob.append(0.0)\n",
    "                    else:\n",
    "                        if i == action[index_a]:\n",
    "                            prob.append(1 - epsilon_k + e_p)\n",
    "                        else:\n",
    "                            prob.append(e_p)\n",
    "                print('epsilon_episode_%d:'%(state) + str(trans(prob)))\n",
    "                \n",
    "def Attenuation(epsilon,alpha,episode_sum,episode):\n",
    "    epsilon = (float(episode_sum) - float(episode)) / float(episode_sum) * epsilon\n",
    "    alpha = (float(episode_sum) - float(episode)) / float(episode_sum) * alpha\n",
    "    return epsilon, alpha\n",
    "\n",
    "#有效动作空间\n",
    "def vilid_action_space(s):\n",
    "    action_sacpe = []\n",
    "    if s % 5 != 0:#左\n",
    "        action_sacpe.append(0)\n",
    "    if s % 5 != 4:#右\n",
    "        action_sacpe.append(1)\n",
    "    if s <= 19:#上\n",
    "        action_sacpe.append(2)\n",
    "    if s >= 5:#下\n",
    "        action_sacpe.append(3)\n",
    "    return action_sacpe\n",
    "def policy_epsilon_greedy(s, Q1, Q2, epsilon):\n",
    "    Q_s = Q1[s] + Q2[s]\n",
    "    action = vilid_action_space(s)\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.choice(action)\n",
    "    else:\n",
    "        index_a = np.argmax([Q_s[i] for i in action])\n",
    "        a = action[index_a]\n",
    "    return a\n",
    "\n",
    "def double_learning(env, episode_num, alpha, gamma, epsilon):\n",
    "    Q1 = np.zeros((env.n_width * env.n_height, env.action_space.n))\n",
    "    Q2 = np.zeros((env.n_width * env.n_height, env.action_space.n))\n",
    "    list_q = [0,1,10000,20000,39999]\n",
    "    for episode_i in range(episode_num):\n",
    "        env.reset()\n",
    "        s = env.state\n",
    "        epsilon_k, alpha_k = Attenuation(epsilon,alpha,episode_num,episode_i)\n",
    "        done = False\n",
    "        print_ff(list_q, Q1, Q2, episode_i, epsilon_k)\n",
    "        while not done:\n",
    "            a = policy_epsilon_greedy(s, Q1, Q2, epsilon_k)\n",
    "            next_s, r, done, _ = env.step(a)\n",
    "            pros = np.random.rand()\n",
    "            if pros < 0.5:\n",
    "                print_dd(s, a, next_s, 8000, episode_i, Q1, Q2, epsilon_k, alpha_k)\n",
    "                Q1[s,a] += alpha_k * (r + gamma * Q2[next_s,np.argmax(Q1[next_s])] - Q1[s,a])\n",
    "            else:\n",
    "                Q2[s,a] += alpha_k * (r + gamma * Q1[next_s,np.argmax(Q2[next_s])] - Q2[s,a])\n",
    "            s = next_s\n",
    "        \n",
    "    return (Q1 + Q2) / 2.0\n",
    "\n",
    "Q = double_learning(env, 40000, 0.05, 0.8, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 DP、MC和TD算法的关系\n",
    "&emsp;&emsp;到目前为止，共介绍了3种用于求解MDP问题的方法。下面对这3种方法进行归纳和对比，如表6.6所示。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\" border=\"3\" bgcolor=\"#DC143C\" cellspacing=\"1\" cellpadding=\"5px\" align=“center” width=\"100%\">\n",
    "    <caption>\n",
    "        <font size=3.5><center>表6.6 MDP问题常用算法归纳对比表</center></font>\n",
    "    </caption>\n",
    "    <tr>\n",
    "        <th width=\"10%\"><p style=\"text-align:center\"><b>类型</b></p></th>\n",
    "        <th width=\"10%\"><p style=\"text-align:center\"><b>算法</b></p></td>\n",
    "        <th width=\"80%\"><p style=\"text-align:center\"><b>特点</b></p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td><p style=\"text-align:center\">基于模型</p></td>\n",
    "        <td><p style=\"text-align:center\">DP</p></td>\n",
    "        <td><p style=\"text-align:left\">（1）环境模型，即状态转移概率$p$已知，通过多次迭代来计算真实的值函数；<br>（2）每次更新值函数时，需要考虑所有可能的状态转移情况；<br>（3）利用了自举思想。</p></td>\n",
    "    </tr>\n",
    "    <td rowspan=\"2\"><p style=\"text-align:center\">免模型</p></td>\n",
    "    <td><p style=\"text-align:center\">MC</p></td>\n",
    "    <td><p style=\"text-align:left\">（1）无需环境模型，可以从经验中学习，通过经验来估计真实的值函数；<br>（2）只能用于完整的情节任务，任务结束后对所有的回报求平均；<br>（3）值函数的估计是相互独立的；<br>（4）可以采取增量式的实现方式，但是只能采取离线学习方法；<br>（5）高收敛性，对起始状态较为敏感。</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td><p style=\"text-align:center\">TD</p></td>\n",
    "    <td><p style=\"text-align:left\">（1）无需环境模型，可以从经验中学习，通过经验来估计真实的值函数；<br>（2）可用于不完整的情节任务，能够单步更新，且可用于持续性任务；<br>（3）利用了自举思想；<br>（4）可以采取在线的、完全增量式的实现方式，能够加快迭代收敛速度；<br>（5）高效，对起始状态更为敏感。</p></td>\n",
    "    </tr>\n",
    "    <td colspan=\"2\"><p style=\"text-align:center\">相同点</p></td>\n",
    "    <td><p style=\"text-align:left\">（1）都是以最大回报或是最优值函数为目标；<br>（2）所有方法都是对未来事件的预测，计算更新值，然后将其作为近似值函数更新目标值。<br></p></td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 穷举式遍历与轨迹采样\n",
    "&emsp;&emsp;DP、MC、TD的学习方式可以分为一下两种类型<br>\n",
    "&emsp;&emsp;（1）穷举式遍历（exhaustive sweep）<br>\n",
    "&emsp;&emsp;通常DP方法需要遍历整个状态（或状态-动作对）空间，每次遍历对所有状态（或状态-动作对）的值函数进行有一次期望更新。当状态过多时，穷举式遍历存在计算资源困境，时间消耗过大等问题，因此DP方法难以运用于大规模任务中。由于大量状态只有在非常糟糕的策略或者非常低的可能性时才会被访问，而他们往往不会对结果产生影响。因此产生了异步TD方法，不再需要遍历整个状态（状态-动作对）空间。<br>\n",
    "&emsp;&emsp;穷举式遍历对每个状态的计算成本都是相同的，但这并非DP的必要属性。原则上，在确保每个状态（或状态-动作对）都能够被访问到的情况下，更新过程的计算成本能够以任意的方式进行分配。<br>\n",
    "&emsp;&emsp;（2）轨迹采样（trajectory sampling）<br>\n",
    "&emsp;&emsp;根据某种分布从状态（状态-动作对）空间中进行采样，根据采样得到的数据对相应的状态（或状态-动作对）的值函数进行依次采样更新，但这也会产生穷举式遍历的计算资源困境。另一种更常用的方法是根据同策略分布对计算资源进行分配，即根据当前策略产生的状态-动作序列的概率分布来分配资源，能够有效处理情节式任务和连续式任务。在任意情况下，样本状态转移和奖赏都由模拟模型提供，样本动作由当前策略产生。换句话说，通过模拟仿真得到完整轨迹，对轨迹中的状态（状态-动作对）进行更新。这种生成采样数据的方式称为轨迹采样。<br>\n",
    "&emsp;&emsp;在轨迹采样方法中，采用同策略分布更新往往要优于随机分布更新。正如在学习下棋时，通过真实棋局对弈的效果总是要比随机落子学习方法更好。此外，同策略分布更能够专注于当前策略很有可能遇到的状态（或状态-动作对），这使得不用对那些无关的状态（或状态-动作对）进行值函数的更新，大大加快了更新的效率；但反之，这也可能造成某一部分状态（或状态-动作对）空间不必要的重复更新。<br>\n",
    "### 6.4.2 期望更新与采样更新\n",
    "&emsp;&emsp;穷举式遍历使用期望更新方法（如DP），轨迹采样使用采样更新方法（如MC、TD）。不论是期望更新还是采样更新，主要区别是值函数更新方法的不同，这也是大部分强化学习方法的差异。以单步更新为例，从3个角度综合分析不同值函数更新方法的差异。<br>\n",
    "&emsp;&emsp;（1）更新状态值函数，还是更新动作值函数；<br>\n",
    "&emsp;&emsp;（2）更新任意给定策略下的值函数，还是更新最优策略下的值函数；<br>\n",
    "&emsp;&emsp;（3）采用期望更新（考虑所有可能发生的转移），还是采样更新（仅考虑采样后产生的单个转移样本）。<br>\n",
    "&emsp;&emsp;（1）、（2）两个角度用于4种近似值函数：$v_\\pi,q_\\pi,v_*,q_*$，3个角度共产生8种情况，其中7种都对应着具体的算法，如图6.7所示：<br>\n",
    "<table align=\"center\" width=\"100%\">\n",
    "    <tr>\n",
    "        <th><center><img src='./image/图6.7.jpg' width='900px' height='400px'></center></th>\n",
    "</tr>\n",
    "</table>\n",
    "<center>图6.7&ensp;单步更新方法更新汇总图</center>\n",
    "&emsp;&emsp;不同的更新方式应用于不同的场景，如第7章将要介绍的Dyna-Q可以使用基于$q_*$的采样更新或基于$q_\\pi$的期望更新。在随机环境问题中，优先遍历使用期望更新方法。在单步采样更新中，由于缺少分布模型，所以只能使用采样更新。<br>\n",
    "&emsp;&emsp;考虑确定环境和随机环境下不同更新方法的差异性。假设状态和动作是离散的，动作值函数$Q$采用表格法来表示，当基于$q_*$对动作值函数进行评估时，期望更新和采样更新的更新递归式分别如式（6.8）和式（6.9）所示：<br>\n",
    "\\begin{equation}\n",
    "Q(s,a)\\gets \\sum \\limits_{s',r}\\hat{p}(s',r|s,a)\\bigl [r+\\gamma \\max \\limits_{a'}Q(S',a')\\bigl ]\n",
    " \\label{6.8}\\tag{6.8}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "Q(s,a)\\gets Q(s,a)+\\alpha \\bigl [R+\\gamma \\max \\limits_{a'}Q(S',a')-Q(s,a)\\bigl ]\n",
    " \\label{6.9}\\tag{6.9}\n",
    "\\end{equation}\n",
    "&emsp;&emsp;（1）确定环境<br>\n",
    "&emsp;&emsp;在确定环境下，由于任意当前状态-动作对都只有唯一的下一个状态，所以期望更新和采样更新本质上相同，此时$\\alpha=1$。<br>\n",
    "&emsp;&emsp;（2）随机环境<br>\n",
    "&emsp;&emsp;在随机环境下，由于存在多个可能的下一个状态，采样更新和期望更新之间差距较大。从式（6.8）可以看出，期望更新建立在分布模型之上，根据所有可能发生的状态转移对值函数$Q$进行更新。这是一种精确计算方法，新$Q(s,a)$正确性仅与即时后继$Q(s,a')$的正确性相关；而采样更新不仅受限于此，还存在采样误差。通常来说，期望更新比同情况下的采样更新效果好。但在随机环境中，期望更新通常会在低概率的状态转移上浪费大量的时间和计算量，而采样更新只考虑其中一种可能发生的状态转移。某种意义上讲，采样更新能更多地受关注于大概率地状态转移，减少更新时间，降低计算难度，且随着采样次数的增加，其误差也会逐渐降低。<br>\n",
    "&emsp;&emsp;实际上，更新操作所需的计算量通常由待评估的状态-动作对地数量决定。对于一组初始状态-动作对$(s,a)$，定义分支因子$b$（branching factor），用以表示所有可能的下一个状态$s'$中，$\\hat{p}(s'|s,a)$>0的数目，则该$(s,a)$的期望更新计算量大约是采样更新的b倍。<br>\n",
    "&emsp;&emsp;在计算量足够的情况下，由于期望更新不存在采样误差，所以它的估计结果通常比b次采样更新的估计结果更好；但如果计算量不足，尤其是在处理大规模状态-动作对的问题时，通常采取采样更新方法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 本章小结\n",
    "&emsp;&emsp;本章介绍了一种新的强化学习方法——时序差分法，并将该算法应用到实际的强化学习问题中。与蒙特卡罗法一样，将问题分为预测问题和控制问题。无论是对于蒙特卡罗控制算法来说，还是对于时序差分控制算法来说，都使用了从动态规划算法中抽象出的广义策略迭代的思想，都是通过不断迭代去更新值函数，使其逐渐收敛到真实值。<br>\n",
    "&emsp;&emsp;广义策略迭代可以分为策略评估和策略改进两个部分组成。策略评估是使用价值函数去准确预测当前策略的回报，而策略改进是根据当前的价值函数对当前的策略进行局部改善。用另外一个角度去看待这两个过程，策略评估实际就是“预测”，策略改进实际就是“控制”。在控制问题中可以将TD控制方法分为同策略方法和异策略方法两类。本章中Sarsa算法属于同策略方法，而期望Sarsa算法和Q-Learning算法属于异策略算法。<br>\n",
    "&emsp;&emsp;时序差分法是目前应用最广泛的强化学习方法。因为时序差分可以在线（on-line）学习，仅仅需要少量的计算就可以从环境交互中产生经验。而且实际问题中，通常不存在完备的环境。而时序差分法恰恰可以从不完备环境中学习经验。本章介绍的TD方法实际是TD方法的一种特例，在第7章中，将扩展本章的几个算法，使它们变得更为复杂，但同时学习效果也会更好。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
