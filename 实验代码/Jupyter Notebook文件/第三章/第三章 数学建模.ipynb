{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>第3章 数学建模</center>\n",
    "&emsp;&emsp;马尔可夫决策过程(Markov Decision Process, MDP)是强化学习的数学理论基础。通过马尔可夫决策过程以概率形式对强化学习任务进行建模，并对强化学习过程中出现的状态、动作、状态转移概率和奖赏等概念进行抽象表达，以实现对强化学习任务的求解，即得到最优策略，获得最大累积奖赏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 马尔可夫决策过程\n",
    "&emsp;&emsp;在介绍马尔可夫决策过程之前，先简单介绍马尔可夫性质以及马尔可夫过程等概念。<br>\n",
    "&emsp;&emsp;**马尔可夫性质**：在某一任务中，如果Agent从环境中得到的下一状态仅依赖于当前状态，而不考虑历史状态，即$P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]$，那么该任务即满足马尔可夫性质。虽然从定义上来看只有当前相邻的两个状态具有关联性，但实际上当前状态蕴含了前面所有历史状态的信息，只不过在已知当前状态的情况下，可以丢弃所有的历史信息。然而，现实中的任务很多无法严格满足马尔可夫性质，为了简化强化学习问题的求解过程，通常假设强化学习所需要解决的任务均满足马尔可夫性质。<br>\n",
    "&emsp;&emsp;马尔可夫过程(Markov Process, MP)：由二元组$(\\mathcal{S},\\mathcal{S})$中的$(S_t,S_{t+1})$组成的马尔可夫链，该链中的所有状态都满足马尔可夫性质。<br>\n",
    "&emsp;&emsp;马尔可夫奖赏过程(Markov Reward Process, MRP)：由三元组$(\\mathcal{S},\\mathcal{P},\\mathcal{R})$组成的马尔可夫过程。根据概率，状态自发地进行转移，其状态转移概率$P$与动作无关，记为$P[S_{t+1}=s'|S_t=s]$。<br>\n",
    "&emsp;&emsp;马尔可夫决策过程：由四元组$(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R})$组成的马尔可夫过程，状态依靠动作进行转移。根据状态空间和动作空间是否有穷，马尔可夫决策过程分为有穷马尔可夫决策过程(finite MDPs)和无穷马尔可夫决策过程(infinite MDPs)。其中四元组$(\\mathcal{S},\\mathcal{A},\\mathcal{P},\\mathcal{R})$定义如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;（1）状态(state)和观测值(observation)<br>\n",
    "&emsp;&emsp;状态空间通常分为两种：用$\\mathcal{S}$(set of all nonterminal states)表示不包含终止状态(goal state)的状态空间；用$\\mathcal{S}^+$(set of all states)表示包含终止状态的状态空间。状态空间可以用集合表示。用$s$表示状态空间中的某一状态，通常用向量表示，可分为离散状态和连续状态两种类型。<br>\n",
    "&emsp;&emsp;有的资料也将状态分为环境状态(environment state)、Agent状态和信息状态(information state)，本书不对状态进行以上区分。另外将状态和观测值视为同一概念，Agent从环境中获得观测值，而对Agent来说，观测值就是它的状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;（2）动作(action)<br>\n",
    "&emsp;&emsp;$\\mathcal{A}$表示动作空间，$\\mathcal{A}(s)$表示状态$s$的动作空间。动作空间可以用集合表示。用$a$表示动作空间中的某一个动作，通常用向量来表示，可分为离散动作和连续动作两种类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;（3）状态转移(state transition)<br>\n",
    "&emsp;&emsp;$P$表示状态转移概率，即在状态$s$下，执行动作$a$转移到$s'$的概率。可以表示为如下两种形式：<br>\n",
    "$$p(s',r|s,a)=P[S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a], \\sum_{s'}\\sum_{r}p(s',r|s,a)=1\\tag{3.1}$$\n",
    "$$p(s'|s,a)=P[S_{t+1}=s'|S_t=s,A_t=a],\\sum_{s'}p(s'|s,a)=1 \\tag{3.2}$$\n",
    "&emsp;&emsp;式（3.1）和式（3.2）都可以用于表示MDP的状态转移概率。式（3.1）即考虑到进入下一状态的随机性，也考虑到到达下一状态获得奖赏的随机性；而式（3.2）只体现出Agent执行动作后进入下一状态的随机性。<br>\n",
    "&emsp;&emsp;在强化学习中，根据状态转移的动态性，可以将环境分为确定环境（deterministic environment）和随机环境（stochastic environment）。通常将$p(s',r|s,a)$恒为1的环境称为确定环境。也就是说，Agent在状态$s$下执行动作$a$后，所到达的下一状态是唯一确定的。因此在确定环境下，执行动作$a$后，状态$s$可以映射为下一个确定的状态$s'$；除此之外其他类型的环境都称为随机环境。即Agent在状态$s$下执行动作$a$后，不一定能到达预期的下一状态$s'$，或者说到达下一状态存在多种可能性。随机环境通常使用式（3.1）或式（3.2）的形式来表达。可以看出，确定环境是随机环境的一个特例，即Agent在状态$s$下执行动作$a$后，以1的概率到达下一状态$s'$，以0的概率到达其他状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;（4）奖赏（reward）<br>\n",
    "&emsp;&emsp;$\\mathcal{R}$表示奖赏空间（set of all reward）。$r(s'|s,a)$表示Agent在状态$s$下采取动作$a$到达状态$s'$时，所获得的的期望奖赏（expected reward），可分为离散奖赏和连续奖赏两种类型。其公式可以表示为：<br>\n",
    "$$r(s,a,s')=\\mathbb{E}[R_{t+1}|S_t=s,A_t=a,S_{t+1}=s']=\\sum_rr\\frac{p(s',r|s,a)}{p(s'|s,a)}\\tag{3.3}$$\n",
    "$$r(s,a)=\\mathbb{E}[R_{t+1}|S_t=s,A_t=a]=\\sum_rr\\sum_{s'}p(s',r|s,a)\\tag{3.4}$$\n",
    "&emsp;&emsp;式（3.3）和式（3.4）都可以用于表示MDP中的奖赏。由于状态转移概率$p(s',r|s,a)$的随机性，Agent在状态$s$下，多次执行动作$a$得到的奖赏的期望，才是真实的奖赏。但是如果遵循先获得奖赏，再到达下一状态这一思想，在实际计算过程中，$r$通常是确定的，此时$r(s,a)$即可表示相应的奖赏。<br>\n",
    "&emsp;&emsp;对于奖赏，可以从两方面进行理解：<br>\n",
    "&emsp;&emsp;（1）先获得奖赏再进入下一状态：奖赏$R_{t+1}$与当前状态$S_t$和动作$A_t$相关。<br>\n",
    "&emsp;&emsp;（2）先进入下一状态再获得奖赏：奖赏$R_{t+1}$与当前状态$S_t$、动作$A_t$和下一状态$S_{t+1}$相关，这也是奖赏用$R_{t+1}$表示的一个重要原因，即与$t+1$时刻到达的状态$S_{t+1}$有关。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.1**&ensp;确定环境下扫地机器人任务的MDP数学建模。<br>\n",
    "&emsp;&emsp;考虑图3.1所描述的确定环境MDP问题：一个扫地机器人，在躲避障碍物的同时，一方面需要到指定的位置收集垃圾，另一方面可以到指定位置给机器人电池充电。<br>\n",
    "<center><img src='./image/图3.1.png' width='200' height='200'></center>\n",
    "<center>图3.1&ensp;扫地机器人环境</center>\n",
    "&emsp;&emsp;扫地机器人任务的MDP数学建模如下：<br>\n",
    "&emsp;&emsp;（1）状态<br>\n",
    "&emsp;&emsp;在该问题中，状态$s$用来描述机器人所在的位置，可以用2维向量表示。为了简化问题，将状态空间离散化为24个不同的状态（图3.1中，共25个小方格，除去$[3,3]$障碍物位置外，每个小方格作为一个状态），按照图3.1中序号出现的先后顺序，用集合表示为：<br>\n",
    "$$\\mathcal{S}^+=\\{S_0:[1,1],S_1:[2,1],S_2:[3,1],...,S_{11}:[2,3],S_{13}:[4,3],...,S_{19}:[5,4],...,S_{23}:[4,5],S_{24}:[5,5]\\}$$\n",
    "&emsp;&emsp;其中，充电桩所在的位置为状态$S_0=[1,1]$，垃圾所在的位置为状态$S_{19}=[5,4]$。坐标[3,3]表示障碍物的位置，机器人不会到达这里，因此不作为状态。通常状态$S_0$和$S_{19}$为终止状态或吸收状态（absorbing state），即一旦机器人到达这两个状态之一，就不会再离开，情节（episode）结束。<br>\n",
    "&emsp;&emsp;（2）动作<br>\n",
    "&emsp;&emsp;动作$a$用来描述机器人运动的方向，可以用2维向量表示。为了简化问题，将动作空间离散化为上、下、左、右4个不同的动作，用集合表示为：<br>\n",
    "$$\\mathcal{A}=\\{上:[0,1],下:[0,-1],左:[-1,0],右:[1,0]\\}$$\n",
    "&emsp;&emsp;对于不同的状态$s$，动作空间可能不同。例如，对于状态$S_0=[1,1]$，它的动作空间为：$\\mathcal{A}(S_0)=\\{[0,1],[1,0]\\}$，即机器人处于该状态时，只能采取上、右动作，但因为$S_0=[1,1]$为吸入状态，无论采取哪个动作，都会保持原地不动；对于状态$S_9=[5,2]$，它的动作空间为：$\\mathcal{A}(S_9)=\\{[-1,0],[0,-1],[0,1]\\}$，即机器人处于该状态时，不能采取向右的动作；对于坐标$[3,3]$周围的4个状态，动作空间中的4个动作都可以采用，但采取指向障碍物的动作时，会保持原地不动。<br>\n",
    "&emsp;&emsp;（3）状态转移<br>\n",
    "&emsp;&emsp;在确定环境下，状态转移可以用两种方式来表示：既可以映射为下一个状态，也可以映射为到达下一个状态的概率。<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;映射为下一个状态：\n",
    "$$f(s,a)=\\begin{cases}\n",
    "s+a,& s\\neq[1,1]且s\\neq[5,4]且s+a\\neq[3,3] \\\\ \n",
    "s,& 其他\n",
    "\\end{cases} \\tag{3.5}$$\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;映射到达下一个状态的概率：\n",
    "$$p(s,a,s')=\\begin{cases}\n",
    "1,& (s+a=s'且s+a\\neq[3,3]) \\\\ & 或((s=[1,1]或s=[5,4]或s+a=[3,3])且s \\neq s') \\\\\n",
    "0,& 其他\n",
    "\\end{cases} \\tag{3.6}$$\n",
    "&emsp;&emsp;（4）奖赏<br>\n",
    "&emsp;&emsp;到达状态$S_{19}=[5,4]$，机器人可以捡到垃圾，并得到+3的奖赏；到达状态$S_0=[1,1]$,机器人充电，并得到+1的奖赏；机器人采取动作向坐标$[3,3]$处移动时，会撞到障碍物，保持原地不动，并得到-10的奖赏；其他情况，奖赏均为0.特别是当机器人到达吸收状态后，无论采取什么动作，只能得到0的奖赏。对应的奖赏函数为：\n",
    "$$r(s,a)= \\begin{cases}\n",
    "+3,& 如果s\\neq[5,4]且s+a=[5,4] \\\\\n",
    "+1,& 如果s\\neq[1,1]且s+a=[1,1] \\\\\n",
    "-10,& 如果s+a=[3,3] \\\\\n",
    "0,& 其他\n",
    "\\end{cases} \\tag{3.7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.2**&ensp;随机环境下扫地机器人任务的MDP数学建模。<br>\n",
    "&emsp;&emsp;重新考虑例3.1的扫地机器人问题。假设由于地面的问题，采取某一动作后，状态转换不再确定。当采取某一动作试图向某一方向移动时，机器人成功转移的概率是0.80，保持原地不动的概率为0.15，移动到相反方向的概率为0.05，具体如图3.2所示，其中$s'$表示下一个状态，$s''$表示相反方向的状态。<br>\n",
    "<center><img src='./image/图3.2.png' width='250'></center>\n",
    "<center>图3.2&ensp;随机环境扫地机器人状态转移示意图</center>\n",
    "&emsp;&emsp;该问题在随机环境下，状态空间、动作空间与确定环境是完全相同的，其随机性主要体现在状态转移函数和奖赏函数上。根据任务的随机性，状态转移只能用概率来表示。具体的状态转移函数可以定义为：\n",
    "$$p(s,a,s')=\\begin{cases}\n",
    "0.80,& 如果s+a=s'且s \\neq s' \\\\\n",
    "0.15,& 如果s=s'且s\\neq[1,1]且s\\neq[5,4] \\\\\n",
    "0.05,& 如果s-a=s'且s \\neq s'\n",
    "\\end{cases}\\tag{3.8}$$\n",
    "&emsp;&emsp;在随机环境下，奖赏的获取不单纯受$(s,a)$的影响，还与下一状态$s'$相关，具体的奖赏函数定义为：\n",
    "$$r(s,a,s')=\\begin{cases}\n",
    "+3,& 如果s\\neq[5,4]且s'=[5,4]\\\\\n",
    "+1,& 如果s\\neq[1,1]且s'=[1,1]\\\\\n",
    "-10,& 如果s+a=[3,3]且s=s'\\\\\n",
    "0,& 其他\n",
    "\\end{cases} \\tag{3.9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2基于模型与无模型\n",
    "&emsp;&emsp;从MDP四元组中可以看出，在强化学习任务求解中，状态转移概率$p$是非常重要的。但获取如例3.2的状态转移概率难度非常大，需要在保证环境完全相同的情况下，重复大量的实验。从状态转移概率是否已知的角度，强化学习可以分为基于模型（model-based）强化学习和无模型（model-Free）强化学习两种：<br>\n",
    "&emsp;&emsp;（1）基于模型：状态转移概率$p$已知，能够通过建立完备的环境模型来模拟真实反馈。相关算法如：动态规划法。<br>\n",
    "&emsp;&emsp;（2）无模型：状态转移概率$p$未知，Agent所处的环境模型是未知的。相关算法：蒙特卡洛法、时序差分法、值函数近似以及策略梯度法。<br>\n",
    "&emsp;&emsp;基于模型的优缺点：<br>\n",
    "&emsp;&emsp;优点：<br>\n",
    "&emsp;&emsp;（1）能够基于模拟经验数据直接模拟真实环境；<br>\n",
    "&emsp;&emsp;（2）具备推理能力，能够直接评估策略的优劣性；<br>\n",
    "&emsp;&emsp;（3）能够与监督学习算法相结合，来求解环境模型。<br>\n",
    "&emsp;&emsp;缺点：<br>\n",
    "&emsp;&emsp;存在二次误差。两次近似误差具体体现在：<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;第一次近似误差：基于真实经验对模型进行学习，得到的模型仅仅是Agent对环境的近似描述。<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;第二次近似误差：基于模拟模型对值函数或策略进行学习时，存在学习误差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3&ensp;求解强化学习任务\n",
    "&emsp;&emsp;对强化学习任务的求解都是以MDP条件为基础的，因此构建基于MDP的强化学习基本框架，如图3.3所示：\n",
    "<center><img src='./image/图3.3.png' width='275'></center>\n",
    "<center>图3.3&ensp;基于MDP的强化学习基本框架</center>\n",
    "&emsp;&emsp;在t时刻，Agent从环境中得到当前状态$S_t$，根据策略$\\pi$执行动作 ，并返回奖赏$R_{t+1}$和下一状态$S_{t+1}$。Agent通过不断地与环境交互进行学习，并在学习过程中不断更新策略，从而经过多次学习后，得到解决问题的最优策略。其中，$R_{t+1}$被理解为存在一定延迟的奖赏，所以不能直接使用。<br>\n",
    "&emsp;&emsp;以吃豆人游戏为例，Agent的目标是在网络中，躲避幽灵、吃掉尽量多的食物，其MDP模型的基本元素构成如下：<br>\n",
    "&emsp;&emsp;（1）吃豆人是Agent，网格世界是环境，吃豆人在网格世界中的位置表示Agent所处状态；<br>\n",
    "&emsp;&emsp;（2）吃豆人的动作空间为上、下、左、右。根据策略，即玩家想法，选择相应动作；<br>\n",
    "&emsp;&emsp;（3）当吃豆人吃掉食物或被幽灵杀死时，将获得奖赏（奖赏也包括惩罚）；<br>\n",
    "&emsp;&emsp;（4）达到回报阈值时，Agent赢得比赛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 策略\n",
    "&emsp;&emsp;在强化学习基本框架中，除了已知的MDP四元组外，还有一个核心要素，即策略（policy）$\\pi$。强化学习的目的就是：在MDP中搜索到最优策略。策略表示状态到动作的映射，即在某一状态下采取动作的概率分布。与状态转移概率不同，策略概率通常是人为设定的。根据概率分布形式，策略可以分为确定策略（deterministic policy）和随机策略（stochastic policy）两种：<br>\n",
    "&emsp;&emsp;（1）确定策略<br>\n",
    "&emsp;&emsp;在确定策略下，Agent在某一状态下只会执行固定一个动作。如确定策略扫地机器人任务，其动作概率分布形如（1,0,0,0），即在当前状态下，只能执行向上走的动作。确定策略可以将状态映射为一个具体的动作。确定策略函数可以表示为：<br>\n",
    "$$a=\\pi(s) \\tag{3.10}$$\n",
    "&emsp;&emsp;式（3.10）表示Agent处于状态$s$时，根据策略$\\pi$，将执行动作$a$。<br>\n",
    "&emsp;&emsp;（2）随机策略<br>\n",
    "&emsp;&emsp;在随机策略下，Agent在一个状态下可能会执行多种动作，其动作概率分布形如（0.2,0.2,0.3,0.3），随机策略将状态映射为执行动作的概率。随机策略函数可以表示为：<br>\n",
    "$$\\pi(a|s)=P(a|s)=P(A_t=a|S_t=s) \\tag{3.11}$$\n",
    "式（3.11）表示Agent处于状态$s$时，根据策略$\\pi$，将执行动作$a$的概率。<br>\n",
    "&emsp;&emsp;动态规划法、蒙特卡洛法和时序差分法都只能获得确定策略，而策略梯度法可以获得随机策略。<br>\n",
    "&emsp;&emsp;MDP应用一个策略产生序列的方法如下：<br>\n",
    "&emsp;&emsp;（1）从初始状态分布中产生一个初始状态$S_i=S_0$；<br>\n",
    "&emsp;&emsp;（2）根据策略$\\pi(a|S_i)$，给出采取的动作$A_i$，并执行该动作$A_i$；<br>\n",
    "&emsp;&emsp;（3）根据奖赏函数和状态转移函数得到奖赏$R_{i+1}$和下一个状态$S_{i+1}$；<br>\n",
    "&emsp;&emsp;（4）$S_i=S_{i+1}$；<br>\n",
    "&emsp;&emsp;（5）不断重复第（2）步到第（4）步的过程，产生一个序列：\n",
    "$$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,S_3,...；$$\n",
    "&emsp;&emsp;（6）如果任务是情节式的，序列将终止于状态 ；如果任务是连续式的，序列将无穷延续。<br>\n",
    "&emsp;&emsp;在实际问题中，策略通常都是随机的。强化学习任务一般都具有两种随机性，即策略$\\pi(a|s)$的随机性和状态转移概率$p(s',r|s,a)$的随机性。策略的随机性可以是人为设定的，而状态转移的随机性是任务本身所固有的特性，如地面湿滑等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 奖赏与回报\n",
    "&emsp;&emsp;归根结底，策略是根据奖赏进行更新的。当给定一个策略$\\pi$时，Agent会依据该策略得到一个状态-动作序列，其形式为：<br>\n",
    "$$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,S_3,...$$\n",
    "&emsp;&emsp;马尔科夫决策过程将回报（return）$G_t$定义为：当前状态$S_t$到终止状态$S_T$的奖赏值之和。其表达式如式（3.12）所示。这里$R$的下标从$t+1$开始：\n",
    "$$G_t=R_{t+1}+R_{t+2}+...+R_T \\tag{3.12}$$\n",
    "&emsp;&emsp;在实际情况中，对于未来的奖赏，总是存在较大的不确定性。为了避免序列过长或连续任务（continuing task）中回报趋于无穷大的情况，引入折扣率（discounting rate）$\\gamma$，用于对未来奖赏赋予折扣，带折扣回报定义如下所示：\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... ,\\gamma\\in [0,1] \\tag{3.13}$$\n",
    "&emsp;&emsp;从式（3.13）可知，距离当前状态越远的未来状态，对当前状态的回报贡献越小。可以用下一状态$S_{t+1}$的回报来表示当前状态$S_t$的回报时，其递归关系式的推导如下所示：\n",
    "$$\\begin{aligned}\n",
    "G_t &= R_{t+1}+\\gamma R_{t+2}+\\gamma^2R_{t+3}+... \\\\\n",
    " &= R_{t+1}+\\gamma (R_{t+2}+\\gamma R_{t+3}+...) \\\\\n",
    " &= R_{t+1}+\\gamma G_{t+1}\n",
    " \\end{aligned} \\tag{3.14}$$\n",
    " &emsp;&emsp;折扣率$\\gamma$存在两种极端情况：当$\\gamma=0$时，未来状态无法对当前回报提供任何价值，也就是说，此时只关注当前状态的奖赏，即Agent“看的不够远”；当$\\gamma=1$时，若环境已知，则能够确定所有的未来状态，也就是说，Agent能够“清晰准确地向后看”，得到准确的回报值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.3**&ensp;设折扣率$\\gamma=0.2$，$T=4$，奖赏序列$R_1=2,R_2=1,R_3=5,R_4=4$，计算各时刻的回报：$G_0,G_1,...,G_4$。<br>\n",
    "&emsp;&emsp;这里假设终止状态的回报为：$G_4=0$。根据式（3.13），可根据下一个状态的回报，直接计算当前状态的回报。所以从最后一个状态开始，反向计算更加简便。计算结果如下：<br>\n",
    "&emsp;&emsp;$G_4=0$<br>\n",
    "&emsp;&emsp;$G_3=R_4+\\gamma * G_4= 4+0.2*0=4$<br>\n",
    "&emsp;&emsp;$G_2=R_3+\\gamma * G_3= 5+0.2*4=5.8$<br>\n",
    "&emsp;&emsp;$G_1=R_2+\\gamma * G_2= 1+0.2*5.8=2.16$<br>\n",
    "&emsp;&emsp;$G_0=R_1+\\gamma * G_1= 2+0.2*2.16=2.432$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.4**&ensp;考虑例3.1的确定环境下扫地机器人任务。机器人每走一步都伴随着奖赏，奖赏由奖赏函数给出。当机器人到达状态$S_{19}=[5,4]$时，得到+3的奖赏；当机器人到达状态$S_0=[1,1]$时，得到+1的奖赏；当前机器人碰到障碍物$[3,3]$时，得到-10的奖赏，即惩罚；其他情况都得到0的中性奖赏。图3.4为机器人的一段移动轨迹，令折扣率为$\\gamma=0.8$，计算轨迹中每个状态的折扣回报。<br>\n",
    "<center><img src='./image/图3.4.png' width='800'></center>\n",
    "<center>图3.4&ensp;扫地机器人的折扣回报示意图</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3&ensp;值函数与贝尔曼方程\n",
    "&emsp;&emsp;在判定一个给定策略$\\pi$的优劣性时，若$p(s',r|s,a)$恒等于1，则每次执行一个动作进入的下一状态是确定的，此时可以直接使用回报作为评判指标。但在实际情况中，由于状态转移概率的存在，环境通常具有随机性，Agent从当前状态到达终止状态可能存在多种不同的状态序列，也就存在多个不同的回报$G_t$，此时无法直接使用$G_t$来判定策略$\\pi$的优劣性，需要以回报的期望作为策略优劣性的评判指标，于是引入值函数（value function）来改进策略。<br>\n",
    "&emsp;&emsp;（1）状态值函数（state-value function）<br>\n",
    "&emsp;&emsp;状态值函数$v_{\\pi}(s)$表示：遵循策略$\\pi$，状态$s$的价值。即在$t$时刻，从状态$s$开始，Agent采取策略$\\pi$得到的期望回报。其计算公式如下所示：\n",
    "$$v_{\\pi}(s)=\\mathbb{E}(G_t|S_t=s) \\tag{3.15}$$\n",
    "&emsp;&emsp;这里，将函数$v_{\\pi}$称为策略$\\pi$的状态值函数。对状态而言，每个状态值函数是由该状态所具有的价值决定的。<br>\n",
    "&emsp;&emsp;（2）动作值函数（action-value function）<br>\n",
    "&emsp;&emsp;动作值函数$q_{\\pi}(s,a)$表示：遵循策略$\\pi$，状态$s$采取动作$a$的价值。即在$t$时刻，从状态$s$开始，执行动作$a$后，Agent采取策略$\\pi$得到的回报期望。其计算公式如下所示：\n",
    "$$q_{\\pi}(s,a)=\\mathbb{E}_{\\pi}(G_t|S_t=s,A_t=a) \\tag{3.16}$$\n",
    "&emsp;&emsp;这里，将函数$q_{\\pi}$称为策略$\\pi$的动作值函数。对状态-动作对而言，每个动作值函数是由每个状态采取动作所具有的价值决定的。<br>\n",
    "&emsp;&emsp;（3）状态值函数的贝尔曼方程<br>\n",
    "&emsp;&emsp;由式（3.15）和式（3.16）可知，动作值函数是在状态值函数的基础上考虑了执行动作$a$所产生的影响。根据这一联系，可以构建值函数之间的递归关系，结合回报递归关系式（3.14），对任意策略$\\pi$，当前状态$s$的价值与其下一个状态$s'$的价值，满足如下关系式：\n",
    "$$\\begin{aligned}\n",
    "v_{\\pi}&=\\mathbb{E}_{\\pi}(G_t|S_t=s) \\\\\n",
    "&=\\mathbb{E}_{\\pi}(R_{t+1}+\\gamma G_{t+1}|S_t=s) \\\\\n",
    "&= \\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma\\mathbb{E}_{\\pi}(G_{t+1}|S_{t+1}=s')] \\\\\n",
    "&= \\sum_a \\pi(a|s) \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\n",
    "\\end{aligned} \\tag{3.17}$$\n",
    "该推导过程使用了期望公式的展开形式。在式（3.17）的最后一行，对每个$a,s',r$三元组，首先计算$\\pi(a|s)p(s',r|s,a)$的概率值，并用该概率对相应的奖赏估计值进行加权，然后对所有可能的取值求和，得到最终期望。式（3.17）称为状态值函数$v_{\\pi}(s)$的贝尔曼方程，用于估计在策略$\\pi$下，状态$s$的期望回报。<br>\n",
    "&emsp;&emsp;这里，$\\sum\\limits_a \\pi(a|s)\\sum\\limits_{s',r}p(s',r|s,a)$也可以写成$\\sum\\limits_{s',r}p(s',r|s,\\pi(s))$的形式。<br>\n",
    "&emsp;&emsp;根据状态值函数贝尔曼方程，可以构建状态值函数更新图（backup diagram），如图3.5所示，这里空心圆表示状态，实心圆表示动作。<br>\n",
    "<center><img src='./image/图3.5.png' width='250'></center>\n",
    "<center>图3.5&ensp;状态值函数更新图</center>\n",
    "&emsp;&emsp;由图3.5可知，状态值函数与动作值函数满足如下关系式：\n",
    "$$v_{\\pi}(s)=\\sum_a \\pi(a|s)q_{\\pi}(s,a) \\tag{3.18}$$\n",
    "式（3.18）表示：Agent遵循策略$\\pi$，值函数$v_{\\pi}$等于状态$s$下所有动作值函数$q_{\\pi}(s,a)$的加权和。其中，策略$\\pi(a|s)$起到权重的作用。<br>\n",
    "&emsp;&emsp;（4）动作值函数的贝尔曼方程<br>\n",
    "&emsp;&emsp;与状态值函数的贝尔曼方程推导方式类似，同理可以得到动作值函数的贝尔曼方程。如下所示：\n",
    "$$\\begin{aligned}\n",
    "q_{\\pi}&= \\mathbb{E}_{\\pi}(G_t|S_t=s,A_t=a) \\\\\n",
    "&=\\mathbb{E}_{\\pi}(R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a) \\\\\n",
    "&=\\mathbb{E}_{\\pi}(R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s,A_t=a) \\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'}\\pi(a'|s')q_{\\pi}(s',a')]\n",
    "\\end{aligned}\\tag{3.19}$$\n",
    "&emsp;&emsp;根据动作值函数的贝尔曼方程，可以构建动作值函数更新图，如图3.6所示：<br>\n",
    "<center><img src='./image/图3.6.png' width='200'></center>\n",
    "<center>图3.6&ensp;动作值函数更新图</center>\n",
    "&emsp;&emsp;由上图可知，动作值函数与状态值函数满足如下关系式：\n",
    "$$q_{\\pi}(s,a)=r+\\gamma \\sum_{s'}p(s'|s,a)v_{\\pi}(s') \\tag{3.20}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.5**&ensp;如图3.7所示，已知$s_1'$、$s_2'$、$s_3'$、$s_4'$的状态值，利用状态值函数的贝尔曼方程，表示$s$的状态值。<br>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>$\\begin{aligned}\n",
    "            v_{\\pi}(s) &= \\mathbb{E}_{\\pi}(G_t|S_t=s) \\\\\n",
    "            &= \\pi(a_1|s)p(s_1',r_1|s,a_1)[r_1+\\gamma v_{\\pi}(s_1')] \\\\\n",
    "            &+ \\pi(a_1|s)p(s_2',r_2|s,a_1)[r_2+\\gamma v_{\\pi}(s_2')] \\\\\n",
    "            &+ \\pi(a_2|s)p(s_3',r_3|s,a_2)[r_3+\\gamma v_{\\pi}(s_3')] \\\\\n",
    "            &+ \\pi(a_2|s)p(s_4',r_4|s,a_2)[r_4+\\gamma v_{\\pi}(s_4')]\n",
    "        \\end{aligned}$</center></th> \n",
    "        <th>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;</th>   \n",
    "        <th><img src='./image/图3.7.png' width='120' align='center'></th>\n",
    "    </tr>\n",
    "        <th></th>\n",
    "        <th></th>\n",
    "        <th><center>图3.7&ensp;状态值函数计算更新图</center></th>\n",
    "</table>\n",
    "&emsp;&emsp;实际上贝尔曼方程给出了每个状态的状态值或每个状态-动作对的动作值之间的关系。因此可以利用解方程组，通过求解，得到实际的状态值或动作值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.6**&ensp;在例3.1确定情况下扫地机器人任务中，采用的随机策略为：$\\pi(a|S_i)=1/|\\mathcal{A}(S_i)|$，$a\\in \\mathcal{A}(S_i)$，这里，$S_i$表示序号为$i$的状态，$|\\mathcal{A}(S_i)|$表示状态$S_i$可以采取的动作数。在折扣率$\\gamma=0.8$的情况下，求扫地机器人任务中每个状态的状态值。<br>\n",
    "&emsp;&emsp;根据状态值函数贝尔曼方程，可以通过联立线性方程组的方法，求解得到：在对应策略$\\pi$下，每个状态的状态值。针对处于状态$S_i$的扫地机器人，可以列出如式（3.21）的贝尔曼方程：\n",
    "$$v_{\\pi}(S_i)=\\sum_{a\\in \\mathcal{A}(S_i)}\\pi(a|S_i)p(S_i,a,s')(r(S_i,a)+\\gamma v_{\\pi}(s')) \\tag{3.21}$$\n",
    "&emsp;&emsp;式（3.21）中，确定环境下$p(S_i,a,s')=1$。状态值$v_{\\pi}(s)$是对状态$s$的累积回报求期望。因为$S_0$和$S_{19}$是终止状态，在这两个状态下无论采取任何动作，都会保持原地不动，且累积奖赏为0，即这两个状态的状态值$v_{\\pi}(S_0)$、$v_{\\pi}(S_{19})$均为0。将每个状态的值函数 作为未知数，它们之间的关系满足贝尔曼方程，于是可以得到如下方程组：\n",
    "$$\\begin{cases}\n",
    "v_{\\pi}(S_1)=\\frac{1}{3}\\times(0+0.8\\times v_{\\pi}(S_6))+\\frac{1}{3}\\times(1+0.8\\times 0)+\\frac{1}{3}\\times (0+0.8\\times v_{\\pi}(S_2)) \\\\\n",
    "v_{\\pi}(S_2)=\\frac{1}{3}\\times(0+0.8\\times v_{\\pi}(S_7))+\\frac{1}{3}\\times(1+0.8\\times v_{\\pi}(S_1))+\\frac{1}{3}\\times (0+0.8\\times v_{\\pi}(S_3)) \\\\\n",
    "\\vdots \\\\\n",
    "v_{\\pi}(S_{11})=\\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_{16})) + \\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_6)) + \\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_{10}) + \\frac{1}{4}\\times (-10+0.8 \\times v_{\\pi}(S_{11})) \\\\\n",
    "\\vdots \\\\\n",
    "v_{\\pi}(S_{13})=\\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_{18})) + \\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_8)) + \\frac{1}{4}\\times (-10+0.8 \\times v_{\\pi}(S_{13})) + \\frac{1}{4}\\times (0+0.8 \\times v_{\\pi}(S_{14})) \\\\\n",
    "\\vdots \\\\\n",
    "v_{\\pi}(S_{23})= \\frac{1}{3}\\times(0+0.8\\times v_{\\pi}(S_{18})) + \\frac{1}{3}\\times(0+0.8\\times v_{\\pi}(S_{22})) + \\frac{1}{3}\\times(0+0.8\\times v_{\\pi}(S_{24}))\\\\\n",
    "v_{\\pi}(S_{24})=\\frac{1}{2}\\times(3+0.8\\times 0) + \\frac{1}{2}\\times(0+0.8\\times v_{\\pi}(S_{23}))\n",
    "\\end{cases}$$\n",
    "&emsp;&emsp;求解方程组，得到状态值，如下图所示：<br>\n",
    "<center><img src='./image/图3.8.png' width='275'></center>\n",
    "<center>图3.8&ensp;基于等概率策略的确定环境扫地机器人任务的状态值</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地机器人在随机策略下的状态值：\n",
      "-1.111 -1.359 -1.615 -0.329 1.368 \n",
      "-1.417 -2.372 -4.369 -0.987 0.000 \n",
      "-1.831 -4.716 None -3.987 -0.300 \n",
      "-0.731 -2.162 -4.649 -2.160 -0.887 \n",
      "0.000 -0.716 -1.772 -1.280 -0.867 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class sweeprobot():\n",
    "    def __init__(self):\n",
    "        # 状态空间\n",
    "        self.S = [[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5]],\n",
    "                  [[1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5]],\n",
    "                  [[2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5]],\n",
    "                  [[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [4, 5]],\n",
    "                  [[4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5]],\n",
    "                  [[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5]]]\n",
    "        # 动作空间\n",
    "        self.A = [[None, None], [-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        # 状态值\n",
    "        self.V = [[None for i in range(6)] for j in range(6)]\n",
    "        self.V[1][1] = 0\n",
    "        self.V[5][4] = 0\n",
    "        # 策略\n",
    "        self.pi = None\n",
    "        self.gamma = 0.8\n",
    "\n",
    "    def reward(self, s, a):\n",
    "        [truth1, truth2] = np.add(s, a) == [5, 4]\n",
    "        [truth3, truth4] = np.add(s, a) == [1, 1]\n",
    "        [truth5, truth6] = np.add(s, a) == [3, 3]\n",
    "        if s != [5, 4] and (truth1 and truth2):\n",
    "            return 3\n",
    "        if s != [1, 1] and (truth3 and truth4):\n",
    "            return 1\n",
    "        if truth5 and truth6:\n",
    "            return -10\n",
    "        return 0\n",
    "\n",
    "    def cal_coefficient(self):\n",
    "        # 该函数用来计算出线性方程组的系数矩阵和向量值\n",
    "        # 首先初始化一个25 * 25的系数矩阵和25个元素的向量\n",
    "        coef_Martix = [[0 for i in range(25)] for j in range(25)]\n",
    "        b = [0 for i in range(25)]\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                # 计算当前状态下的动作数，以用于计算策略pi\n",
    "                count_action = 0\n",
    "                if i - 1 >= 1:\n",
    "                    count_action += 1\n",
    "                if i + 1 <= 5:\n",
    "                    count_action += 1\n",
    "                if j - 1 >= 1:\n",
    "                    count_action += 1\n",
    "                if j + 1 <= 5:\n",
    "                    count_action += 1\n",
    "                self.pi = 1 / count_action\n",
    "                # 具体计算每一个状态值的函数\n",
    "                b_value = 0\n",
    "                coef_CurrentState = 0\n",
    "                # 向上的情况\n",
    "                if i - 1 >= 1:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[1])\n",
    "                    if i - 1 == 3 and j == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef1 = self.pi * self.gamma\n",
    "                        coef_Martix[(i - 1) * 5 + j - 1][((i - 1) - 1) * 5 + j - 1] = coef1\n",
    "                # 向下的情况\n",
    "                if i + 1 <= 5:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[2])\n",
    "                    if i + 1 == 3 and j == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef2 = self.pi * self.gamma\n",
    "                        coef_Martix[(i - 1) * 5 + j - 1][((i + 1) - 1) * 5 + j - 1] = coef2\n",
    "                # 向左的情况\n",
    "                if j - 1 >= 1:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[3])\n",
    "                    if j - 1 == 3 and i == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef3 = self.pi * self.gamma\n",
    "                        coef_Martix[(i - 1) * 5 + j - 1][(i - 1) * 5 + (j - 1) - 1] = coef3\n",
    "                # 向右的情况\n",
    "                if j + 1 <= 5:\n",
    "                    b_value = b_value + self.pi * self.reward(self.S[i][j], self.A[4])\n",
    "                    if j + 1 == 3 and i == 3:\n",
    "                        coef_CurrentState = self.pi * self.gamma\n",
    "                    else:\n",
    "                        coef4 = self.pi * self.gamma\n",
    "                        coef_Martix[(i - 1) * 5 + j - 1][(i - 1) * 5 + (j + 1) - 1] = coef4\n",
    "                # 将左边的移项，所以系数为-1\n",
    "                coef_Martix[(i - 1) * 5 + j - 1][(i - 1) * 5 + j - 1] = -1 + coef_CurrentState\n",
    "                # 同理，将常数项移项需要乘-1\n",
    "                b[(i - 1) * 5 + j - 1] = -1 * b_value\n",
    "        # 因为状态[1,1]和状态[5,4]可以确定其状态值为0,状态[3,3]不存在，所以其实只需求22*22的矩阵和22个元素的向量值\n",
    "        # 把矩阵和向量第[(1-1)*5+1-1]和[(5-1)*5+4-1]删除\n",
    "        del coef_Martix[23]\n",
    "        del b[23]\n",
    "        del coef_Martix[12]\n",
    "        del b[12]\n",
    "        del coef_Martix[0]\n",
    "        del b[0]\n",
    "        # 把矩阵每一行的[(1-1)*5+1-1]和[(5-1)*5+4-1]和[(3-1)*5+3-1]删除\n",
    "        for item in coef_Martix:\n",
    "            del item[23]\n",
    "            del item[12]\n",
    "            del item[0]\n",
    "        return coef_Martix, b\n",
    "\n",
    "    def solve_equation(self, coef_Martix, b):\n",
    "        A = np.array(coef_Martix)\n",
    "        b = np.array(b)\n",
    "        x = np.linalg.solve(A, b)\n",
    "        x = list(x)\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                self.V[i][j] = x.pop(0)\n",
    "\n",
    "    def print_value(self):\n",
    "        # 输出扫地机器人的状态值\n",
    "        print('扫地机器人在随机策略下的状态值：')\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                if self.V[j][6 - i] != None:\n",
    "                    print('%.3f'%self.V[j][6 - i], end=\" \")\n",
    "                else:\n",
    "                    print(self.V[j][6 - i], end=\" \")\n",
    "            print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sr = sweeprobot()\n",
    "    A, b = sr.cal_coefficient()\n",
    "    sr.solve_equation(A, b)\n",
    "    sr.print_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4&ensp;最优策略与最优值函数\n",
    "&emsp;&emsp;由3.3.2和3.3.3节可知，利用强化学习方法解决任务的关键在于：搜索出MDP中的最优策略。所谓的最优策略就是使得值函数最大的策略。在有穷MDP中，由于状态空间和动作空间都是有限的，所以策略也是有限的。通过计算值函数可以精确地得到至少一个最优策略（optimality policy）$\\pi_*$，优于或等效于其他所有策略。对于一个更优策略$\\pi'$，执行该策略时，所有状态的期望回报都大于或等于执行$\\pi$策略的期望回报。也就是说，对于所有$s\\in\\mathcal{S}$，$\\pi'\\ge\\pi$，都存在$v_{\\pi'}(s)\\ge v_{\\pi}(s)$。<br>\n",
    "&emsp;&emsp;最优策略可能不止一个，它们共享相同的状态值函数，称之为最优状态值函数（optimal state-value function），记为$v_*$，定义为：<br>\n",
    "$$v_*(s)=v_{\\pi_*}(s)=\\max_{\\pi}v_{\\pi}(s),s\\in\\mathcal{S} \\tag{3.22}$$\n",
    "式（3.22）表明：在状态$s$处，执行最优策略$\\pi_*$的期望回报，也就是在状态$s$处能够获得的最大价值。同理，最优策略共享相同的最优动作值函数（optimal action-value function），记为$q_*$，定义为：<br>\n",
    "$$q_*(s,a)=q_{\\pi_*}(s,a)=\\max_{\\pi}q_{\\pi}(s,a),s\\in\\mathcal{S},a\\in\\mathcal{A} \\tag{3.23}$$\n",
    "式（3.23）表明：在状态$s$处，执行动作$a$，并在随后的过程中采取最优策略$\\pi_*$得到的期望回报，也就是在状态-动作对$(s,a)$处能够获得的最大价值。<br>\n",
    "&emsp;&emsp;（1）贝尔曼最优方程<br>\n",
    "&emsp;&emsp;对于最优策略值函数$v_*$而言，一方面因为$v_*$是策略的值函数，必定满足贝尔曼方程（3.17）。另一方面又因为它是最优的值函数，因此引入一种不与任何特定策略有关的表达形式来表示它，即贝尔曼最优方程（Bellman optimality equation）。贝尔曼最优方程表示：Agent在采用最优策略$\\pi_*$时，各个状态$s$的价值一定等于在该状态下执行最优动作$a$的期望回报。$v_*$的贝尔曼最优方程定义为：<br>\n",
    "$$\\begin{aligned}\n",
    "v_*(s) &=\\max_{a\\in \\mathcal{A}(s)}q_{\\pi_*}(s,a) \\\\\n",
    "&= \\max_a \\mathbb{E}_{\\pi_*}(R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a) \\\\\n",
    "&=\\max_a \\mathbb{E}(R_{t+1}+\\gamma v_*(S_{t+1})|S_t=s,A_t=a) \\\\\n",
    "&=\\max_a \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]\n",
    "\\end{aligned} \\tag{3.24}$$\n",
    "式（3.24）中最后两行的表达式，为关于$v_*$的贝尔曼最优方程的两种表达形式。同理，$q_*$的贝尔曼最优方程定义为：<br>\n",
    "$$\\begin{aligned}\n",
    "q_*(s,a)&=\\mathbb{E}(R_{t+1}+\\gamma v_*(S_{t+1})|S_t=s,A_t=a) \\\\\n",
    "&=\\mathbb{E}(R_{t+1}+\\gamma \\max_{a'} q_*(S_{t+1},a')|S_t=s,A_t=a) \\\\\n",
    "&=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\max_{a'} q_*(s',a')]\n",
    "\\end{aligned}\\tag{3.25}$$\n",
    "&emsp;&emsp;关于$v_*$和$q_*$的贝尔曼最优方程，更新图分别如图3.9和图3.10所示，弧线表示取最大值，其他部分都与图3.5和图3.6的$v_{\\pi}$、$q_{\\pi}$的更新图相同。<br>\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center><img src='./image/图3.9.png' width='300'></center></th>\n",
    "        <th><center><img src='./image/图3.10.png' width='250'></center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>图3.9&ensp;关于$v_*$的贝尔曼最优方程更新图</center></td>\n",
    "        <td><center>图3.10&ensp;关于$q_*$的贝尔曼最优方程更新图</center></td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "&emsp;&emsp;（2）最优策略<br>\n",
    "&emsp;&emsp;Agent通过与环境不断地交互获得的信息来更新策略，以最终获得最优质函数。一旦获得最优状态值函数$v_*$或最优动作值函数$q_*$，Agent便能得到最优策略。<br>\n",
    "&emsp;&emsp;若已知$v_*$，单步搜索后，最好的动作就是全局最优动作。即对于$v_*$来说，任意贪心策略都是最优策略。贪心用于描述仅基于当前情况或局部条件进行的最优选择，不考虑未来的变化与影响，是一种基于短期视角的方案。由于$v_*$本身包含了未来所有可能的动作产生的回报影响，因此可以将对最优的长期（全局）回报期望的求解，转化为计算每个状态对应的局部变量，以单步搜索方式产生长期的最优动作序列。简单来看，基于贪心策略，Agent总是朝着具有更大状态值函数的状态进行移动。<br>\n",
    "&emsp;&emsp;若已知$q_*$，由于最优动作值函数以最大的长期回报期望来表达每组状态-动作对的局部变量，最优动作的选择无需知道后续状态及对应的价值，Agent可以直接选择最大动作值函数所对应的动作，这一方法也称为贪心动作选择方法，其表达式为：<br>\n",
    "$$A_t=\\mathop{\\arg \\max}_a q_t(s,a) \\tag{3.26}$$\n",
    "&emsp;&emsp;由于受环境的完备性、计算能力和计算资源的限制，在寻找最优策略时，直接对包含max的非线性最优贝尔曼方程组求解，是比较困难的。通常采用迭代方式，如动态规划法，经过一轮一轮的迭代，最终收敛为最优解。第4章将会详细介绍动态规划法。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;**例3.7**&ensp;求解例3.1中确定环境下扫地机器人任务的最优状态值函数，并给出最优策略。<br>\n",
    "&emsp;&emsp;设折扣率$\\gamma = 0.8$。利用式（3.24），可以显式地给出在扫地机器人任务中，状态$S_i$的最优贝尔曼方程为：<br>\n",
    "$$v_*(S_i)=\\max_a p(S_i,a,s')(r(S_i,a) + \\gamma v_*(s')) \\tag{3.27}$$\n",
    "&emsp;&emsp;式（3.27）中，确定环境下$p(S_i,a,s')=1$。将每个状态的值函数$v_{\\pi}(s)$作为未知数，他们之间的关系满足最优贝尔曼方程，于是利用式（3.27），可以得到扫地机器人任务的最优贝尔曼方程组。利用第4章的值迭代算法，可以对贝尔曼最优方程组进行求解，并找到该扫地机器人任务的最优策略，计算结果如图3.11所示：<br>\n",
    "<center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><img src='./image/图3.11(1).png' width='300'></th>\n",
    "        <th><img src='./image/图3.11(2).png' width='300'></th>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "<center>图3.11&ensp;确定环境下扫地机器人任务的最优值函数和最优策略</center>\n",
    "&emsp;&emsp;通过图3.11可以看出，当机器人处于状态$S_6=[2,2]$时，可以采取4个不同的动作。通过一步搜索，计算出上、下、左和右4个动作值，分别为1.229，0.8，0.8，1.229，于是得出状态$S_6=[2,2]$的最优动作为向上和向右2个动作；当机器人处于状态$S_{17}=[3,4]$时，仍可以采取4个不同的动作。通过一步搜索，计算出上、下、左和右4个动作值函数，分别为1.536，-8.080，1.536，2.400，于是给出状态$S_{17}=[3,4]$的最优动作为向右1个动作。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "扫地机器人最优状态值：\n",
      "1.229 1.536 1.920 2.400 3.000 \n",
      "1.536 1.920 2.400 3.000 0.000 \n",
      "1.229 1.536 None 2.400 3.000 \n",
      "1.000 1.229 1.536 1.920 2.400 \n",
      "0.000 1.000 1.229 1.536 1.920 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SweepRobot():\n",
    "    def __init__(self):\n",
    "        # 状态空间\n",
    "        self.S = [[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5]],\n",
    "                  [[1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5]],\n",
    "                  [[2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5]],\n",
    "                  [[3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [4, 5]],\n",
    "                  [[4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5]],\n",
    "                  [[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5]]]\n",
    "        # 动作空间\n",
    "        self.A = [[None, None], [-1, 0], [1, 0], [0, -1], [0, 1]]\n",
    "        # 状态值\n",
    "        self.V = [[None for i in range(6)] for j in range(6)]\n",
    "        self.V[1][1] = 0\n",
    "        self.V[5][4] = 0\n",
    "        # 这回没有策略\n",
    "        self.gamma = 0.8\n",
    "        self.theta = 0.001\n",
    "\n",
    "    def reward(self, s, a):\n",
    "        [truth1, truth2] = np.add(s, a) == [5, 4]\n",
    "        [truth3, truth4] = np.add(s, a) == [1, 1]\n",
    "        [truth5, truth6] = np.add(s, a) == [3, 3]\n",
    "        if s != [5, 4] and (truth1 and truth2):\n",
    "            return 3\n",
    "        if s != [1, 1] and (truth3 and truth4):\n",
    "            return 1\n",
    "        if truth5 and truth6:\n",
    "            return -10\n",
    "        return 0\n",
    "\n",
    "    def cal_optimal_value(self):\n",
    "        # 建立V的副本\n",
    "        copy_V = self.V\n",
    "        # 首先初始化V值,便于计算，都初始化为0\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                [truth1, truth2] = [i == 5, j == 4]\n",
    "                [truth3, truth4] = [i == 1, j == 1]\n",
    "                [truth5, truth6] = [i == 3, j == 3]\n",
    "                if truth1 and truth2:\n",
    "                    continue\n",
    "                if truth3 and truth4:\n",
    "                    continue\n",
    "                if truth5 and truth6:\n",
    "                    continue\n",
    "                self.V[i][j] = 0\n",
    "                copy_V[i][j] = 0\n",
    "        while True:\n",
    "            Delta = 0\n",
    "            for i in range(1, 6):\n",
    "                for j in range(1, 6):\n",
    "                    # 判断是否是终止情况，如果是的话直接计算下一个\n",
    "                    [truth1, truth2] = [i == 5, j == 4]\n",
    "                    [truth3, truth4] = [i == 1, j == 1]\n",
    "                    [truth5, truth6] = [i == 3, j == 3]\n",
    "                    if truth1 and truth2:\n",
    "                        continue\n",
    "                    if truth3 and truth4:\n",
    "                        continue\n",
    "                    if truth5 and truth6:\n",
    "                        continue\n",
    "                    v = self.V[i][j]\n",
    "                    # 因为每个状态的动作空间不一样，所以需要分情况讨论\n",
    "                    max_value = 0\n",
    "                    # 向上的情况\n",
    "                    if i - 1 >= 1:\n",
    "                        if i - 1 == 3 and j == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i - 1][j])\n",
    "                    # 向下的情况\n",
    "                    if i + 1 <= 5:\n",
    "                        if i + 1 == 3 and j == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[2]) + self.gamma * self.V[i + 1][j])\n",
    "                    # 向左的情况\n",
    "                    if j - 1 >= 1:\n",
    "                        if j - 1 == 3 and i == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[3]) + self.gamma * self.V[i][j - 1])\n",
    "                    # 向右的情况\n",
    "                    if j + 1 <= 5:\n",
    "                        if j + 1 == 3 and i == 3:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[1]) + self.gamma * self.V[i][j])\n",
    "                        else:\n",
    "                            max_value = max(max_value, self.reward(self.S[i][j], self.A[4]) + self.gamma * self.V[i][j + 1])\n",
    "                    copy_V[i][j] = max_value\n",
    "                    Delta = max(Delta, abs(v - copy_V[i][j]))\n",
    "            self.V = copy_V\n",
    "            if Delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def print_value(self):\n",
    "        # 输出扫地机器人的状态值\n",
    "        print('扫地机器人最优状态值：')\n",
    "        for i in range(1, 6):\n",
    "            for j in range(1, 6):\n",
    "                if self.V[j][6 - i] != None:\n",
    "                    print('%.3f'%self.V[j][6 - i], end=\" \")\n",
    "                else:\n",
    "                    print(self.V[j][6 - i], end=\" \")\n",
    "            print()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sr = SweepRobot()\n",
    "    sr.cal_optimal_value()\n",
    "    sr.print_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;综上所示，利用最优值函数计算最优策略时，需要通过一步搜索，计算出其所有动作值，通过选取具有最大动作值的动作，得到最优策略。而利用动作值寻找最优策略时，由于动作值中包含了可采取动作的信息，因此只需要选取具有最大动作值的动作，即可以得到最优策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4&ensp;探索与利用\n",
    "&emsp;&emsp;由于强化学习的目的是得到一个最优策略，使得回报最大化。这就导致强化学习的一大矛盾：探索与利用的平衡。一方面，Agent秉持利用机制（exploitation），为了得到最大回报，需要始终采用最优动作，即根据当前的值函数选择最优动作，最大限度地提升回报。另一方面，Agent需要探索机制（exploration），摒弃基于值函数的贪心策略，找到更多可能的动作来获得更好的策略，探索更多的可能性。<br>\n",
    "&emsp;&emsp;通常我们采用具有探索性的策略来解决以上矛盾。常用的两种方法为同策略和异策略。使用这两种方法一方面保证足够的探索性，另一方面充分地利用最优策略。在介绍它们之前，先来了解行为策略和目标策略：<br>\n",
    "&emsp;&emsp;（1）行为策略（behavior policy）：用于产生采样数据的策略，具备探索性，能够覆盖所有情况，通常采用$\\varepsilon$-柔性策略（$\\varepsilon$-soft policy）。<br>\n",
    "&emsp;&emsp;（2）目标策略（target policy）：强化学习任务中待求解的策略，也就是待评估和改进的策略，一般不具备探索性，通常采用确定性贪心策略。<br>\n",
    "&emsp;&emsp;下面对同策略和异策略的特点分别阐述如下：<br>\n",
    "&emsp;&emsp;（1）同策略（on-policy）：行为策略和目标策略相同。通过$\\varepsilon$-贪心策略平衡探索和利用，在保证初始状态-动作对$(S_0,A_0)$不变的前提下，确保每一组$(s,a)$都有可能被遍历到。常用算法为Sarsa和Sarsa($\\lambda$)算法。<br>\n",
    "&emsp;&emsp;（2）异策略（off-policy）：行为策略和目标策略不同。将探索与利用分开，在行为策略中贯彻探索原则：采样数据，得到状态-动作序列；在目标策略中贯彻利用原则：更新值函数并改进目标策略，以得到最优目标策略。常用算法为Q-learning和DQN算法。<br>\n",
    "&emsp;&emsp;同策略方法较为简单，通常会被优先考虑，但异策略方法更为通用，效果也更好，可以用来学习通过传统的非学习型控制器或人类专家生成的数据。异策略方法的优缺点分别阐述如下：<br>\n",
    "&emsp;&emsp;异策略优点：<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;效果好，更具有一般性，可以从示例样本或其他Agent给出的数据样本中进行学习；<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;可以重复利用旧策略；<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;可以在使用一个探索性策略的基础上，学习一个确定性策略；<br>\n",
    "&emsp;&emsp;$\\blacktriangleright$&emsp;可以用一个行为策略进行采样，多个目标策略同时进行学习。<br>\n",
    "&emsp;&emsp;异策略缺点：<br>\n",
    "&emsp;&emsp;由于数据来源于一个不同的策略，存在方差较大、收敛速度慢的缺陷。<br>\n",
    "&emsp;&emsp;$\\bullet$偏差（bias）：<br>\n",
    "&emsp;&emsp;在机器学习中，偏差和欠拟合相关，预测值与样本之间的偏差越大，说明精度越低。在强化学习中，偏差指通过采样数据获得的估计平均值与实际平均值的偏离程度。<br>\n",
    "&emsp;&emsp;$\\bullet$方差（variance）：<br>\n",
    "&emsp;&emsp;在机器学习中，方差和过拟合相关，样本值之间的方差越大，说明样本的置信度越差，模型适用性越差。在强化学习中，方差指单次采样结果相对于均值的波动大小。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5小结\n",
    "&emsp;&emsp;本章主要介绍了强化学习的基础数学理论，以马尔科夫决策过程描述了Agent与环境的交互。状态是Agent选择动作的基础，通过动作的选择，完成状态的转移，并以奖赏评判Agent动作选择的优劣。<br>\n",
    "&emsp;&emsp;有限的状态、动作和收益共同构成了有限马尔科夫决策过程，回报刻画了Agent能获得的全部未来奖赏，对于不同的任务，未来状态的奖赏会有不同的折扣，而Agent的任务就是最大化回报。动作的选择依赖于Agent所采取的策略，而强化学习的目的就是获得最优策略。<br>\n",
    "&emsp;&emsp;引入状态值和动作状态值来描述回报，通过贝尔曼最优方程将马尔可夫决策过程表达抽象化，从而可以相对容易地求解得到最优价值函数。在强化学习问题中，定义环境模型和明确最优值函数是计算最优策略的基础，在后面的章节中，将讨论如何求解最优策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
