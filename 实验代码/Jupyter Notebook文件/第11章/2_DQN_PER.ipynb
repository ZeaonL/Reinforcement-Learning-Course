{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized DQN\n",
    "&emsp;&emsp;在DQN训练算法中，通过经验回放机制来实时处理模型训练过程中得到的转移样本。该机制每次从样本池中等概率地抽取小批量的样本用于训练模型，这使得Agent无法区分出不同转移样本之间的重要性差异，对有价值样本的利用率不高。而且由于经验池的容量（如1000000）远远大于每次随机获取的小批量样本数（如32），这可能会导致一些重要样本被覆盖以及样本重复利用等问题。针对这些问题，Schaul等人提出一种基于优先级采样的深度Q网络（Prioritized Deep Q-Network，Prioritized DQN）方法。<br>\n",
    "&emsp;&emsp;Prioritized DQN方法引入优先级回放（prioritized replay）机制，使用优先级采样方式替代原先的随机采样，提高了有价值转移样本被采样的概率，从而加快Agent学习最优策略的进程。另外，优先级经验回放机制还能在学习初期提高带有正奖赏转移样本的利用率，一定程度上缓解了在复杂问题中存在的稀疏奖赏问题。该算法不仅提高了收敛速度，并且在许多基于视觉感知的视频游戏任务中取得了更优的表现。<br>\n",
    "&emsp;&emsp;Prioritized DQN算法核心在于：根据TD误差，将经验迁移样本按序存储在不同的经验池中。<br>\n",
    "&emsp;&emsp;在小批量样本采样时，不再使用随机采样方法，而是根据需求从不同的经验池中选择迁移样本，这样TD误差越大的样本，被采样的次数可能越多。此外，新得到的经验样本会被设定为最大优先级样本，以保证它至少被训练一次，从而提高Agent的性能。下面主要阐述两种主要的经验回放机制。<br>\n",
    "#### 优先级采样方法\n",
    "&emsp;&emsp;（1）将具有不同奖赏量级的迁移序列分别存放在两个不同的回放记忆单元D1和D2中，其中D1用于存放带有正奖赏的、D2具有较高优先级的迁移序列；用于存放带有负奖赏或零奖赏的、具有较低优先级的迁移序列。<br>\n",
    "&emsp;&emsp;（2）使用一种类似于分层抽样的方法从回放记忆单元中抽取固定数量（如32个）的迁移样本。具体的抽样方式是以概率p从回放记忆单元D1中抽取样本，以概率1-p从回放记忆单元D2中抽取样本。通过这种区别奖赏值量级大小的分层采样，提高了有价值的正奖赏迁移样本的利用率，从而促进了Agent的学习速度。<br>\n",
    "&emsp;&emsp;（3）为缓解一部分迁移样本还未被利用就被回放记忆单元覆盖掉的问题，在迁移序列中增加了每个样本在训练过程中被采样的次数，即迁移样本的形式为。通常那些被频繁回放的转移样本具有较小的TD误差值，这是由于每一次回放都使得该样本所对应的Q值函数更加逼近目标动作值函数。在经验回放机制中，可以优先考虑那些很久未被采样到的迁移样本，因为这类样本对应的TD误差值较大。<br>\n",
    "&emsp;&emsp;在基于离线样本的强化学习算法中，如果收集到的样本不能覆盖整体的样本空间，在某种程度上会导致学习的偏向性。而完全基于优先级的贪婪采样方式会使得转移样本缺乏多样性。具体地，基于优先级的贪婪采样方式倾向于收集那些具有较高优先级的转移样本，以避免扫描整个样本空间。不过该种抽样方式可能会使得一些具有较低优先级的样本失去被采样到的机会，从而导致对应的TD误差项更新过程过于缓慢。上述问题在使用深度神经网络来表示值函数时体现的尤为明显。因此有必要提出一种可以同时保证样本多样性和充分利用样本优先级的经验回放机制。<br>\n",
    "#### 随机优先级方法\n",
    "&emsp;&emsp;由于优先级采样会导致学习的偏向性，因此提出将随机采样与优先级采样相结合的随机优化方法。该方法可使得采样过程介于完全的优先级采样和随机采样之间，从而在优先利用有价值迁移样本的前提下，保证回放样本的多样性。为了使得迁移样本被采样的概率与对应的优先级成正比，并同时保证最低优先级对应的迁移样本具有非零的采样概率。<br>\n",
    "&emsp;&emsp;采用优先级扫描的缺点在于，容易引起损失值发散问题，并产生偏差，但采用随机优先级和重要性采样方法可以减轻这些问题。总体来说，经验回放机制使得参数的更新不再受限于经验样本的顺序，优先级经验回放使算法不受限于经验样本的出现频率。<br>\n",
    "#### Prioritized DQN算法\n",
    "&emsp;&emsp;基于随机优先级的采样方式，提出一种基于随机优先级采样的深度Q学习算法（Deep Q-Learning with Prioritized Sampling，PS-DQN）。该算法将一种高效的优先级采样机制和深度Q学习算法相结合，一定程度上缓解了传统深度Q学习中有价值样本利用率不高的问题，提高了Agent在解决视觉感知决策问题时的性能和稳定性。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f27f8104f90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS9ElEQVR4nO3dfYxldX3H8ffnnHvvzOzsM+B22SVAI8EQDWC3CsE0CtKiNdo0hkBNYxpS/rEtVhOF9g9j0j8waVSSWpuNaGmrgqJWQoyWIsY2aZDloT7wIIgouwUWV1b2gdm5955v/zhnZVhnds7Mnbn3nvl9XsnN3HPuvXN+d37zuefx/r6KCMxs7ctG3QAzGw6H3SwRDrtZIhx2s0Q47GaJcNjNEjFQ2CVdIekxSU9Iun6lGmVmK0/LPc8uKQd+DFwO7AXuA66OiIdXrnlmtlJaA7z2DcATEfEkgKRbgXcBC4a9o4mYZHrRX6xOm2jlIIEGaOFKkQiADPptEfkyf00BWRcUAUX101bHnD4r2qJIpM9mZg4y2z0yb2oGCfsO4Ok503uBN57sBZNM80ZddtJfqlaLfMfp9E/ZQOQZRSeHfLSJj0wUuSg6GYdPb3Fsy/La0z4cTD9XkM8UZLMFebdY4Zbaca/os+0r0GfHmtFn9+351IKPDRL2WiRdC1wLMJlvoLVj58lfkGcUG9dRdFrlWl2jX7WHBJmITPSmxOzG5X66i+KAyPLq90ljv6Zoqrl91p8S3Q3L/U2i+KXIes3vs0HCvg84Y870zmreK0TEbmA3wMYNO2L2zFMX/83S2J0niExEC3rT0N20vE93FTn9DmRdkXVH/yG21h3vs+40zG5eZp9Ftmb6bJCw3wecI+lsypBfBfzJoq8a8Sb5skWgvshmIT+aLXosIVpB0Y5q66SaqWauERpPlH/74/0QZXh1Qngjh6ITL/eTIBr67zqfZYc9InqS/gL4FpADn42IH61Yy8aIiigP0vT7bHpKrHtukf8AwUunZhw9vQx85DF2WypJCzG5P2P6/wIVL38Al30G0YKixZr7cB5onz0ivgF8Y4XaMrYUgfoBfcgPzDK5yPNDosgneGkbKC83J2Ft/eM0mQLah2D9vlmyfrl5HxJFa4KZV0E/K/fL11qPrfoBurUgjh8kFPQn8kVPvYVEf4LxOG1oVnHY6xIUueiuz5idXnybvLteRBZExprbHLRmcthrOn4qp9+u1tqLKNpr6+CONZ/DXocgWqJoi5mtGTOLnT0U9CeCaAeRhTfnbSw47DWEyosziraY3QjHTu07wNY4DvtynBD0bEZks1p017x1RGTdIOsFNPQqLGsuh31QBUwcyFj3bKBFLtJqHSvoHOqTzRaoAV+qsLXFYV8BWRcmDhWLhj3rhoM+LqqtsxiD714Mi8NuSYoc+pPZKz6gi1Z1BmWNnip12C1JRQt6UyeEvcOavqzZYV8B0YLuVLbopnk+Wx6go1eg/pAaZ78hVAa7uy57+dp4Qb8jQmv3VKnDPijBsa1Bb2ru19vm13kRNjwNraOC2XIwBO+7j4CCY1uC/uQr+6w/ERSdtXvVo8M+KEF/XUF/XZ3nZvSfq74b3Vujq48mWEqfrSEOe00qAvVE+zD0Xljejl37kM+zD9PxPmsdgc7B5fZZebZlLfSZw16DioAeZCqY/GWx7BFLWi8F7aPFrwPvTfjV4z77TUMNe2SiP7HMYT5HKKqx8I5/tXWx8+knU7QEAaGMaOqoPQ2Qap+VYyfMb6hh702JX7yuM8xFrpiojuUU7fK2HOrD0W5e/uPFmjwGNFZS7LPeQwOEXdJngXcA+yPitdW8rcBtwFnAU8CVEfHCYr+r6MDhM33OyWy1FCdZl9ZZs/8z8A/Av8yZdz1wd0TcWJV9uh748GK/KOv0mT7jUI1FmtlyZJ2FV6aLhj0ivivprBNmvwt4c3X/FuA71Aj7mesO8E/n/9tiTzOzZfrzdQcWfGy5++zbIuKZ6v6zwLY6L1ovccnkGr4e0WzE1p/kiz0DJy/KypALHraQdK2kPZL2PH/A++tmo7LcsD8naTtA9XP/Qk+MiN0RsSsidp12SvNOu5mtFcsN+x3Ae6v77wW+vjLNMbPVsmjYJX0R+B/gXEl7JV0D3AhcLulx4K3VtJmNsTpH469e4KGT1142s7Ey1CvoDhYZdxxJ7KtGZkN0sFh4Y32oYd8/u4F/fPotw1ykWVL2zz674GNDDXu3n7PvV5uGuUizpHT7C5/xGu5XXA/n6L82D3WRZkk5PCZhbx8NXvXAzDAXaZaUnx9d+Gt5w12zR7O//G829k6SL1+obpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSLqDEt1hqR7JD0s6UeSrqvmb5V0l6THq59bVr+5ZrZcddbsPeCDEXEecBHwPknn8XJVmHOAu6tpMxtTi4Y9Ip6JiAeq+4eAR4AdlFVhbqmedgvwR6vVSDMb3JK+4lqVgboQuJeaVWEkXQtcCzAx4VFqzEal9gE6SeuBrwDvj4gX5z52sqowc4tEdNrTAzXWzJavVtgltSmD/vmI+Go1u3ZVGDMbvTpH4wXcDDwSER+f85Crwpg1SJ199kuAPwV+IOmhat7fUFaB+VJVIeZnwJWr00QzWwl1KsL8N7BQHVhXhTFrCF9BZ5YIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpaIOmPQTUr6nqT/rSrCfLSaf7akeyU9Iek2SZ3Vb66ZLVedNfsx4NKIOB+4ALhC0kXAx4BPRMSrgReAa1avmWY2qDoVYSIiDleT7eoWwKXA7dV8V4QxG3N1x43Pq5Fl9wN3AT8BDkZEr3rKXsqSUPO99lpJeyTtme0eWYk2m9ky1Ap7RPQj4gJgJ/AG4DV1F+CKMGbjYUlH4yPiIHAPcDGwWdLxoah3AvtWuG1mtoLqHI0/TdLm6v4UcDllJdd7gHdXT3NFGLMxV6cizHbgFkk55YfDlyLiTkkPA7dK+jvgQcoSUWY2pupUhPk+ZZnmE+c/Sbn/bmYN4CvozBLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRLhsJslwmE3S4TDbpYIh90sEQ67WSIcdrNEOOxmiXDYzRJRO+zVcNIPSrqzmnZFGLMGWcqa/TrKgSaPc0UYswapWyRiJ/CHwGeqaeGKMGaNUnfN/kngQ0BRTZ+CK8KYNUqdcePfAeyPiPuXswBXhDEbD3XGjb8EeKektwOTwEbgJqqKMNXa3RVhzMZcnSquN0TEzog4C7gK+HZEvAdXhDFrlEHOs38Y+ICkJyj34V0RxmyM1dmM/7WI+A7wneq+K8KYNYivoDNLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRDrtZIhx2s0Q47GaJcNjNEuGwmyXCYTdLRK2RaiQ9BRwC+kAvInZJ2grcBpwFPAVcGREvrE4zzWxQS1mzvyUiLoiIXdX09cDdEXEOcHc1bWZjapDN+HdRVoIBV4QxG3t1wx7Af0i6X9K11bxtEfFMdf9ZYNt8L3RFGLPxUHd02TdFxD5JrwLukvTo3AcjIiTFfC+MiN3AboCNG3bM+xwzW3211uwRsa/6uR/4GuUQ0s9J2g5Q/dy/Wo00s8HVqfU2LWnD8fvA7wM/BO6grAQDrghjNvbqbMZvA75WVmmmBXwhIr4p6T7gS5KuAX4GXLl6zTSzQS0a9qryy/nzzD8AXLYajTKzlecr6MwS4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0uEw26WCIfdLBEOu1kiHHazRDjsZolw2M0SUSvskjZLul3So5IekXSxpK2S7pL0ePVzy2o31syWr+6a/SbgmxHxGsohqh7BFWHMGqXO6LKbgN8DbgaIiNmIOIgrwpg1Sp01+9nA88DnJD0o6TPVkNKuCGPWIHXC3gJeD3w6Ii4EjnDCJntEBGWJqN8QEbsjYldE7Oq0pwdtr5ktU52w7wX2RsS91fTtlOF3RRizBlk07BHxLPC0pHOrWZcBD+OKMGaNUrew418Cn5fUAZ4E/ozyg8IVYcwaolbYI+IhYNc8D7kijFlD+Ao6s0Q47GaJcNjNEuGwmyXCYTdLhMNulgiH3SwRdS+qGUuRiSJXeT8XSBCB+oECVAQq5r1k3yw5jQ57d7rFzNacfht606I3BfksdA4GeTfoHC7oHOyhcODNGh32/oSY2SL6kzC7OehtKMiPZhS5aL0ksh50xALfxzNLi/fZzRLhsJslwmE3S4TDbpYIh90sEQ67WSLqDCV9rqSH5txelPR+F4kwa5Y6Y9A9FhEXRMQFwO8AR4Gv4SIRZo2y1M34y4CfRMTPcJEIs0ZZativAr5Y3a9VJMLMxkPtsFcjy74T+PKJj52sSIQrwpiNh6Ws2d8GPBARz1XTtYpEuCKM2XhYStiv5uVNeHCRCLNGqVuffRq4HPjqnNk3ApdLehx4azVtZmOqbpGII8ApJ8w7gItEmDVGo7/PrqhuRXmjr5fvL3jI0Gx4Qnp54vjdOf+XwxxYpdFhz2cKJg+Ioi1aR0QxIbIudF4Msm7QPlo48DYyIVG0RNHOIIN+JyMyyPrl/ycF5LMFWa8YSnuaHfZjBZMvAIKiXY5Hp+oPSAFZdzh/RLOFFO2M/mRG0YLeVPkznw3yY0HWL8dJzHrDaUujw06A+gESGVFNlzeiGnTS48/ZCEVebnn229CbKldKkVeP9SB/SSf/BSuo0WFXEeSz8ev9onzOPpEivAlvoyXoT4pjG0V/UsycAv2poHVYdF4U+bGgNaPy2yZD0OywV4GWU21jqmiJfkf0J6C3LuhPFVBk5LMCRGTDW7P7++xmiXDYzRLhsJslwmE3S4TDbpaIoR6N73cyDu2cGOYizUYmMjGzVcxugv5k0NvUR1M9uq0WZDnZMaFeTndq5TLR//7C6++hhr03BQdeN7xTDWYjJeht6KHpHnm74LRNR1jX7vLizASHj0zS7WXMbumQH125DezePQs/Ntzz7HnQ29wf6iLNRkZBvr7H5NQsnVaPrVNHWdeaJc8KpKDbzzlciF5nBWOYL3zNyVDDvnn6KH/8u3uGuUizkdrYmmFDPkNbfba2DtNWj6PFBIf6U3Qj55e9aV7qt1dseV+YXvhyvKGGfWf7CDf+1n3DXKTZSGUsvttarOAVoN9tLzzO41DDLkRb+TAXaTb2VjIROsmHS91hqf5a0o8k/VDSFyVNSjpb0r2SnpB0WzX6rJmNqTrln3YAfwXsiojXUn4QXQV8DPhERLwaeAG4ZjUbamaDqXvMvwVMSWoB64BngEuB26vHXRHGbMzVqfW2D/h74OeUIf8VcD9wMCKOj7GxF9ixWo00s8HV2YzfQlnX7WzgdGAauKLuAuZWhHn+gM+xm41Knc34twI/jYjnI6JLOXb8JcDmarMeYCewb74Xz60Ic9opPhJvNip1wv5z4CJJ6ySJcqz4h4F7gHdXz3FFGLMxV2ef/V7KA3EPAD+oXrMb+DDwAUlPUBaQuHkV22lmA6pbEeYjwEdOmP0k8IYVb5GZrQp/n90sEQ67WSIcdrNEOOxmiVAMsTySpOeBI8AvhrbQ1Xcqfj/jai29F6j3fs6MiNPme2CoYQeQtCcidg11oavI72d8raX3AoO/H2/GmyXCYTdLxCjCvnsEy1xNfj/jay29Fxjw/Qx9n93MRsOb8WaJGGrYJV0h6bFq3Lrrh7nsQUk6Q9I9kh6uxuO7rpq/VdJdkh6vfm4ZdVuXQlIu6UFJd1bTjR1bUNJmSbdLelTSI5IubnL/rPTYj0MLu6Qc+BTwNuA84GpJ5w1r+SugB3wwIs4DLgLeV7X/euDuiDgHuLuabpLrgEfmTDd5bMGbgG9GxGuA8ynfVyP7Z1XGfoyIodyAi4FvzZm+AbhhWMtfhffzdeBy4DFgezVvO/DYqNu2hPewkzIAlwJ3AqK8aKM1X5+N8w3YBPyU6jjUnPmN7B/KYd6eBrZSfjv1TuAPBumfYW7GH2/8cY0dt07SWcCFwL3Atoh4pnroWWDbiJq1HJ8EPgQU1fQpNHdswbOB54HPVbsln5E0TUP7J1Zh7EcfoFsiSeuBrwDvj4gX5z4W5cdtI05vSHoHsD8i7h91W1ZIC3g98OmIuJDysuxXbLI3rH8GGvtxPsMM+z7gjDnTC45bN64ktSmD/vmI+Go1+zlJ26vHtwP7R9W+JboEeKekp4BbKTflb6Lm2IJjaC+wN8qRlaAcXen1NLd/Bhr7cT7DDPt9wDnV0cQO5cGGO4a4/IFU4+/dDDwSER+f89AdlGPwQYPG4ouIGyJiZ0ScRdkX346I99DQsQUj4lngaUnnVrOOj5XYyP5hNcZ+HPJBh7cDPwZ+AvztqA+CLLHtb6LcBPw+8FB1ezvlfu7dwOPAfwJbR93WZby3NwN3Vvd/G/ge8ATwZWBi1O1bwvu4ANhT9dG/A1ua3D/AR4FHgR8C/wpMDNI/voLOLBE+QGeWCIfdLBEOu1kiHHazRDjsZolw2M0S4bCbJcJhN0vE/wP/yGfza6UrjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=5):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 经验回放\n",
    "DQN的经验回放采用均匀分布，而均匀分布采样并不能高效利用数据。因为agent的经验就是经历过的数据，但这些数据对于训练并不是具有同等重要的意义，agent在某些状态的学习效率比其他状态的学习效率高。<br>\n",
    "优先回放的基本思想：打破均匀采样，赋予学习效率高的状态以更大的采样权重。<br>\n",
    "如何选择权重？<br>\n",
    "一个理想的标准是agent学习的效率越高，权重越大。TD偏差越大，说明该状态处的值函数与TD目标的差距越大，agent的更新量越大，因此该处的学习效率越高。<br>\n",
    "如果TD-error越大, 就代表我们的预测精度还有很多上升空间, 那么这个样本就越需要被学习, 也就是优先级p越高。<br>\n",
    "<center>$\\theta_{t+1} = \\theta_{t} + \\alpha[r+\\gamma{max_a^{'}Q(s^{'},a^{'};\\theta^{'})-Q(s,a;\\theta)}]\\nabla{Q(s,a;\\theta)}$<br></center>\n",
    "有了 TD-error 就有了优先级 p, 那我们如何有效地根据 p 来抽样呢?<br>\n",
    "如果每次抽样都需要针对 p 对所有样本排序, 这将会是一件非常消耗计算能力的事. 可以采用更高级的算法——SumTree方法。<br>\n",
    "SumTree是一种树形结构，每片树叶存储每个样本的优先级P，每个树枝节点只有两个分叉，节点的值是两个分叉的和，所以SumTree的顶端就是所有p的和。<br>\n",
    "<center><img src=\"./image/图1.png\" height=\"400\" width=\"500\" ></center><br>\n",
    "比如p的总和是42的话, 如果抽6个样本，划分成区间[0-7], [7-14], [14-21], [21-28], [28-35], [35-42]。然后在每个区间里随机选取一个数。<br>\n",
    "(i)比如在区间 [21-28] 里选到了24, 就按照这个 24 从最顶上的42开始向下搜索.。<br>\n",
    "(ii)首先顶端42 下面有两个子结点，先对比左边的子结点, 如果比24大, 那就走左边这条路。<br>\n",
    "(iii)接着再对比 29 下面的左边子结点,13比24小, 那就走右边的路, 并且初始值24根据 13 修改, 变成 24-13 = 11。<br>\n",
    "(iiii)接着拿着 11 和 16 左子结点比，12 比 11 大, 选 12 当做这次选到的 priority, 并且选择 12 对应的一条数据.<br>\n",
    "简而言之，每次我们都选较大的节点数，因为每个父节点都是两个子节点的和，那么父节点较大的数也对应着较大的子节点数。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumTree\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1) #[]一维方程，数量为2 * capacity - 1 M0 = M2+1 M = M2+M0 M=2M0-1\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node 向上更新\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node s为他的优先级 关键代码\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer_PER(object):\n",
    "    # stored as ( s, a, r, s_ ) in SumTree\n",
    "    def __init__(self, memory_size=1000, a = 0.6, e = 0.01):\n",
    "        self.tree =  SumTree(memory_size)\n",
    "        self.memory_size = memory_size\n",
    "        self.prio_max = 0.1\n",
    "        self.a = a\n",
    "        self.e = e\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        p = (np.abs(self.prio_max) + self.e) ** self.a #  proportional priority\n",
    "        self.tree.add(p, data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        priorities = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.tree.get(s)\n",
    "            \n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            priorities.append(p)\n",
    "            idxs.append(idx)\n",
    "        return idxs, np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def update(self, idxs, errors):\n",
    "        self.prio_max = max(self.prio_max, max(np.abs(errors)))  #为了让他保持一个p=0.1，不然容易不再探索这个动作了\n",
    "        for i, idx in enumerate(idxs):\n",
    "            p = (np.abs(errors[i]) + self.e) ** self.a\n",
    "            self.tree.update(idx, p) \n",
    "        \n",
    "    def size(self):\n",
    "        return self.tree.n_entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_PERAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = None, USE_CUDA = False, memory_size = 10000, prio_a = 0.6, prio_e = 0.001, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer_PER(memory_size, a = prio_a, e = prio_e)\n",
    "        self.DQN = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self,idxs, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done,dtype = torch.uint8)  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "        predicted_next_qvalues = self.DQN_target(next_states) # YOUR CODE\n",
    "\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] # YOUR CODE\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values # YOUR CODE\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        errors = (predicted_qvalues_for_actions - target_qvalues_for_actions).detach().cpu().squeeze().tolist()\n",
    "        self.memory_buffer.update(idxs, errors)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idxs = []\n",
    "        segment = self.memory_buffer.tree.total() / batch_size\n",
    "        priorities = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, data = self.memory_buffer.tree.get(s)\n",
    "            \n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "            priorities.append(p)\n",
    "            idxs.append(idx)\n",
    "        return idxs, torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            idxs, states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(idxs, states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:     0, reward:   nan, loss: 0.000000, epsilon: 1.000000, episode:    0\n",
      "frames:  1000, reward: -21.000000, loss: 0.000000, epsilon: 0.967544, episode:    1\n",
      "frames:  2000, reward: -21.000000, loss: 0.000000, epsilon: 0.936152, episode:    2\n",
      "frames:  3000, reward: -20.333333, loss: 0.000000, epsilon: 0.905789, episode:    3\n",
      "frames:  4000, reward: -20.000000, loss: 0.000000, epsilon: 0.876422, episode:    4\n",
      "frames:  5000, reward: -20.200000, loss: 0.000000, epsilon: 0.848017, episode:    5\n",
      "frames:  6000, reward: -20.166667, loss: 0.000000, epsilon: 0.820543, episode:    6\n",
      "frames:  7000, reward: -20.142857, loss: 0.000000, epsilon: 0.793971, episode:    7\n",
      "frames:  8000, reward: -20.250000, loss: 0.000000, epsilon: 0.768269, episode:    8\n",
      "frames:  9000, reward: -20.333333, loss: 0.000000, epsilon: 0.743410, episode:    9\n",
      "frames: 10000, reward: -20.300000, loss: 0.029885, epsilon: 0.719366, episode:   10\n",
      "frames: 11000, reward: -20.000000, loss: 0.045351, epsilon: 0.696110, episode:   11\n",
      "frames: 12000, reward: -20.000000, loss: 0.047894, epsilon: 0.673617, episode:   12\n",
      "frames: 13000, reward: -20.200000, loss: 0.042817, epsilon: 0.651861, episode:   13\n",
      "frames: 14000, reward: -20.300000, loss: 0.043968, epsilon: 0.630818, episode:   14\n",
      "frames: 15000, reward: -20.300000, loss: 0.021812, epsilon: 0.610465, episode:   15\n",
      "frames: 16000, reward: -19.800000, loss: 0.017543, epsilon: 0.590780, episode:   16\n",
      "frames: 17000, reward: -19.900000, loss: 0.028141, epsilon: 0.571740, episode:   17\n",
      "frames: 18000, reward: -19.900000, loss: 0.043398, epsilon: 0.553324, episode:   18\n",
      "frames: 19000, reward: -19.900000, loss: 0.035471, epsilon: 0.535511, episode:   19\n",
      "frames: 20000, reward: -20.000000, loss: 0.014372, epsilon: 0.518283, episode:   20\n",
      "frames: 21000, reward: -20.200000, loss: 0.012481, epsilon: 0.501619, episode:   22\n",
      "frames: 22000, reward: -20.200000, loss: 0.021578, epsilon: 0.485502, episode:   23\n",
      "frames: 23000, reward: -20.200000, loss: 0.024610, epsilon: 0.469913, episode:   24\n",
      "frames: 24000, reward: -20.200000, loss: 0.005791, epsilon: 0.454836, episode:   25\n",
      "frames: 25000, reward: -20.800000, loss: 0.011108, epsilon: 0.440252, episode:   26\n",
      "frames: 26000, reward: -20.700000, loss: 0.004588, epsilon: 0.426147, episode:   27\n",
      "frames: 27000, reward: -20.500000, loss: 0.008895, epsilon: 0.412504, episode:   29\n",
      "frames: 28000, reward: -20.400000, loss: 0.014235, epsilon: 0.399308, episode:   30\n",
      "frames: 29000, reward: -20.500000, loss: 0.002421, epsilon: 0.386545, episode:   31\n",
      "frames: 30000, reward: -20.400000, loss: 0.002550, epsilon: 0.374201, episode:   32\n",
      "frames: 31000, reward: -20.400000, loss: 0.003724, epsilon: 0.362261, episode:   33\n",
      "frames: 32000, reward: -20.500000, loss: 0.004572, epsilon: 0.350712, episode:   34\n",
      "frames: 33000, reward: -20.500000, loss: 0.003480, epsilon: 0.339542, episode:   35\n",
      "frames: 34000, reward: -20.300000, loss: 0.004666, epsilon: 0.328739, episode:   36\n",
      "frames: 35000, reward: -20.300000, loss: 0.003541, epsilon: 0.318289, episode:   37\n",
      "frames: 36000, reward: -20.100000, loss: 0.003233, epsilon: 0.308182, episode:   38\n",
      "frames: 37000, reward: -20.100000, loss: 0.003516, epsilon: 0.298407, episode:   39\n",
      "frames: 38000, reward: -20.100000, loss: 0.021532, epsilon: 0.288952, episode:   40\n",
      "frames: 39000, reward: -19.900000, loss: 0.006457, epsilon: 0.279806, episode:   41\n",
      "frames: 40000, reward: -19.800000, loss: 0.007022, epsilon: 0.270961, episode:   42\n",
      "frames: 41000, reward: -19.500000, loss: 0.011483, epsilon: 0.262406, episode:   43\n",
      "frames: 42000, reward: -19.400000, loss: 0.030803, epsilon: 0.254131, episode:   44\n",
      "frames: 43000, reward: -19.400000, loss: 0.005858, epsilon: 0.246127, episode:   45\n",
      "frames: 44000, reward: -19.600000, loss: 0.007259, epsilon: 0.238386, episode:   46\n",
      "frames: 45000, reward: -19.600000, loss: 0.003587, epsilon: 0.230899, episode:   46\n",
      "frames: 46000, reward: -19.800000, loss: 0.003379, epsilon: 0.223657, episode:   48\n",
      "frames: 47000, reward: -19.900000, loss: 0.005305, epsilon: 0.216652, episode:   49\n",
      "frames: 48000, reward: -20.000000, loss: 0.006827, epsilon: 0.209878, episode:   50\n",
      "frames: 49000, reward: -20.100000, loss: 0.009024, epsilon: 0.203325, episode:   51\n",
      "frames: 50000, reward: -20.300000, loss: 0.007774, epsilon: 0.196987, episode:   52\n",
      "frames: 51000, reward: -20.600000, loss: 0.003495, epsilon: 0.190857, episode:   53\n",
      "frames: 52000, reward: -20.700000, loss: 0.003489, epsilon: 0.184928, episode:   54\n",
      "frames: 53000, reward: -20.700000, loss: 0.009046, epsilon: 0.179193, episode:   55\n",
      "frames: 54000, reward: -20.500000, loss: 0.004692, epsilon: 0.173646, episode:   56\n",
      "frames: 55000, reward: -20.600000, loss: 0.004645, epsilon: 0.168281, episode:   58\n",
      "frames: 56000, reward: -20.500000, loss: 0.003641, epsilon: 0.163092, episode:   59\n",
      "frames: 57000, reward: -20.500000, loss: 0.005237, epsilon: 0.158073, episode:   59\n",
      "frames: 58000, reward: -20.400000, loss: 0.005533, epsilon: 0.153219, episode:   60\n",
      "frames: 59000, reward: -20.300000, loss: 0.006751, epsilon: 0.148523, episode:   61\n",
      "frames: 60000, reward: -20.000000, loss: 0.002274, epsilon: 0.143982, episode:   62\n",
      "frames: 61000, reward: -19.900000, loss: 0.007440, epsilon: 0.139589, episode:   63\n",
      "frames: 62000, reward: -19.600000, loss: 0.005104, epsilon: 0.135341, episode:   64\n",
      "frames: 63000, reward: -19.600000, loss: 0.006135, epsilon: 0.131232, episode:   65\n",
      "frames: 64000, reward: -19.600000, loss: 0.008584, epsilon: 0.127257, episode:   65\n",
      "frames: 65000, reward: -19.700000, loss: 0.012429, epsilon: 0.123413, episode:   66\n",
      "frames: 66000, reward: -19.700000, loss: 0.007594, epsilon: 0.119695, episode:   67\n",
      "frames: 67000, reward: -19.700000, loss: 0.004670, epsilon: 0.116099, episode:   67\n",
      "frames: 68000, reward: -19.500000, loss: 0.008981, epsilon: 0.112621, episode:   68\n",
      "frames: 69000, reward: -19.500000, loss: 0.006521, epsilon: 0.109256, episode:   68\n",
      "frames: 70000, reward: -19.500000, loss: 0.003208, epsilon: 0.106002, episode:   69\n",
      "frames: 71000, reward: -19.200000, loss: 0.007123, epsilon: 0.102855, episode:   70\n",
      "frames: 72000, reward: -19.200000, loss: 0.004311, epsilon: 0.099811, episode:   70\n",
      "frames: 73000, reward: -19.400000, loss: 0.009671, epsilon: 0.096866, episode:   71\n",
      "frames: 74000, reward: -19.700000, loss: 0.014422, epsilon: 0.094019, episode:   72\n",
      "frames: 75000, reward: -19.800000, loss: 0.003477, epsilon: 0.091264, episode:   73\n",
      "frames: 76000, reward: -20.000000, loss: 0.006130, epsilon: 0.088600, episode:   74\n",
      "frames: 77000, reward: -20.000000, loss: 0.005128, epsilon: 0.086023, episode:   74\n",
      "frames: 78000, reward: -19.900000, loss: 0.005118, epsilon: 0.083531, episode:   75\n",
      "frames: 79000, reward: -19.900000, loss: 0.002287, epsilon: 0.081120, episode:   75\n",
      "frames: 80000, reward: -19.800000, loss: 0.003797, epsilon: 0.078789, episode:   76\n",
      "frames: 81000, reward: -19.800000, loss: 0.003222, epsilon: 0.076533, episode:   76\n",
      "frames: 82000, reward: -19.700000, loss: 0.003079, epsilon: 0.074352, episode:   77\n",
      "frames: 83000, reward: -19.700000, loss: 0.004695, epsilon: 0.072243, episode:   77\n",
      "frames: 84000, reward: -19.400000, loss: 0.003084, epsilon: 0.070202, episode:   78\n",
      "frames: 85000, reward: -19.400000, loss: 0.003866, epsilon: 0.068228, episode:   78\n",
      "frames: 86000, reward: -19.200000, loss: 0.005154, epsilon: 0.066319, episode:   79\n",
      "frames: 87000, reward: -19.200000, loss: 0.002942, epsilon: 0.064473, episode:   79\n",
      "frames: 88000, reward: -19.200000, loss: 0.006049, epsilon: 0.062687, episode:   79\n",
      "frames: 89000, reward: -18.800000, loss: 0.001858, epsilon: 0.060960, episode:   80\n",
      "frames: 90000, reward: -18.800000, loss: 0.003310, epsilon: 0.059289, episode:   80\n",
      "frames: 91000, reward: -18.500000, loss: 0.005507, epsilon: 0.057673, episode:   81\n",
      "frames: 92000, reward: -18.500000, loss: 0.003376, epsilon: 0.056110, episode:   81\n",
      "frames: 93000, reward: -18.500000, loss: 0.004061, epsilon: 0.054599, episode:   81\n",
      "frames: 94000, reward: -18.200000, loss: 0.001246, epsilon: 0.053137, episode:   82\n",
      "frames: 95000, reward: -18.200000, loss: 0.002412, epsilon: 0.051722, episode:   82\n",
      "frames: 96000, reward: -18.200000, loss: 0.001668, epsilon: 0.050355, episode:   82\n",
      "frames: 97000, reward: -17.500000, loss: 0.002203, epsilon: 0.049032, episode:   83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 98000, reward: -17.500000, loss: 0.002015, epsilon: 0.047752, episode:   83\n",
      "frames: 99000, reward: -17.500000, loss: 0.002614, epsilon: 0.046514, episode:   83\n",
      "frames: 100000, reward: -17.300000, loss: 0.003183, epsilon: 0.045317, episode:   84\n",
      "frames: 101000, reward: -17.300000, loss: 0.002667, epsilon: 0.044159, episode:   84\n",
      "frames: 102000, reward: -16.900000, loss: 0.001951, epsilon: 0.043040, episode:   85\n",
      "frames: 103000, reward: -16.900000, loss: 0.003351, epsilon: 0.041956, episode:   85\n",
      "frames: 104000, reward: -16.900000, loss: 0.002546, epsilon: 0.040909, episode:   86\n",
      "frames: 105000, reward: -16.900000, loss: 0.001628, epsilon: 0.039895, episode:   86\n",
      "frames: 106000, reward: -16.700000, loss: 0.002948, epsilon: 0.038915, episode:   87\n",
      "frames: 107000, reward: -16.700000, loss: 0.002607, epsilon: 0.037967, episode:   87\n",
      "frames: 108000, reward: -16.700000, loss: 0.005571, epsilon: 0.037050, episode:   88\n",
      "frames: 109000, reward: -16.700000, loss: 0.004036, epsilon: 0.036164, episode:   88\n",
      "frames: 110000, reward: -15.900000, loss: 0.001780, epsilon: 0.035306, episode:   89\n",
      "frames: 111000, reward: -15.900000, loss: 0.001906, epsilon: 0.034476, episode:   89\n",
      "frames: 112000, reward: -15.700000, loss: 0.003055, epsilon: 0.033674, episode:   90\n",
      "frames: 113000, reward: -15.700000, loss: 0.002894, epsilon: 0.032898, episode:   90\n",
      "frames: 114000, reward: -15.700000, loss: 0.001955, epsilon: 0.032147, episode:   90\n",
      "frames: 115000, reward: -15.200000, loss: 0.003181, epsilon: 0.031421, episode:   91\n",
      "frames: 116000, reward: -15.400000, loss: 0.002614, epsilon: 0.030719, episode:   92\n",
      "frames: 117000, reward: -15.400000, loss: 0.002522, epsilon: 0.030039, episode:   92\n",
      "frames: 118000, reward: -15.400000, loss: 0.002596, epsilon: 0.029383, episode:   92\n",
      "frames: 119000, reward: -14.900000, loss: 0.004166, epsilon: 0.028747, episode:   93\n",
      "frames: 120000, reward: -14.900000, loss: 0.003528, epsilon: 0.028132, episode:   93\n",
      "frames: 121000, reward: -14.900000, loss: 0.001897, epsilon: 0.027538, episode:   93\n",
      "frames: 122000, reward: -14.000000, loss: 0.002023, epsilon: 0.026963, episode:   94\n",
      "frames: 123000, reward: -14.000000, loss: 0.001851, epsilon: 0.026407, episode:   94\n",
      "frames: 124000, reward: -14.000000, loss: 0.002040, epsilon: 0.025869, episode:   94\n",
      "frames: 125000, reward: -13.100000, loss: 0.002249, epsilon: 0.025349, episode:   95\n",
      "frames: 126000, reward: -13.100000, loss: 0.001977, epsilon: 0.024846, episode:   95\n",
      "frames: 127000, reward: -12.300000, loss: 0.001587, epsilon: 0.024359, episode:   96\n",
      "frames: 128000, reward: -12.300000, loss: 0.001799, epsilon: 0.023888, episode:   96\n",
      "frames: 129000, reward: -12.200000, loss: 0.002005, epsilon: 0.023433, episode:   97\n",
      "frames: 130000, reward: -12.200000, loss: 0.003613, epsilon: 0.022992, episode:   97\n",
      "frames: 131000, reward: -12.300000, loss: 0.001816, epsilon: 0.022567, episode:   98\n",
      "frames: 132000, reward: -12.300000, loss: 0.004572, epsilon: 0.022155, episode:   98\n",
      "frames: 133000, reward: -12.300000, loss: 0.001676, epsilon: 0.021756, episode:   98\n",
      "frames: 134000, reward: -12.300000, loss: 0.002599, epsilon: 0.021371, episode:   99\n",
      "frames: 135000, reward: -12.300000, loss: 0.002800, epsilon: 0.020998, episode:   99\n",
      "frames: 136000, reward: -12.900000, loss: 0.003432, epsilon: 0.020637, episode:  100\n",
      "frames: 137000, reward: -12.900000, loss: 0.001503, epsilon: 0.020289, episode:  100\n",
      "frames: 138000, reward: -12.100000, loss: 0.003564, epsilon: 0.019951, episode:  101\n",
      "frames: 139000, reward: -12.100000, loss: 0.004006, epsilon: 0.019625, episode:  101\n",
      "frames: 140000, reward: -12.100000, loss: 0.001605, epsilon: 0.019310, episode:  101\n",
      "frames: 141000, reward: -10.800000, loss: 0.002576, epsilon: 0.019004, episode:  102\n",
      "frames: 142000, reward: -10.800000, loss: 0.002310, epsilon: 0.018709, episode:  102\n",
      "frames: 143000, reward: -11.900000, loss: 0.002272, epsilon: 0.018424, episode:  103\n",
      "frames: 144000, reward: -11.900000, loss: 0.002282, epsilon: 0.018147, episode:  103\n",
      "frames: 145000, reward: -12.700000, loss: 0.003935, epsilon: 0.017880, episode:  104\n",
      "frames: 146000, reward: -12.700000, loss: 0.001364, epsilon: 0.017622, episode:  104\n",
      "frames: 147000, reward: -12.700000, loss: 0.002882, epsilon: 0.017372, episode:  104\n",
      "frames: 148000, reward: -13.700000, loss: 0.002790, epsilon: 0.017130, episode:  105\n",
      "frames: 149000, reward: -13.700000, loss: 0.002219, epsilon: 0.016897, episode:  105\n",
      "frames: 150000, reward: -13.700000, loss: 0.006584, epsilon: 0.016671, episode:  105\n",
      "frames: 151000, reward: -12.800000, loss: 0.002360, epsilon: 0.016452, episode:  106\n",
      "frames: 152000, reward: -12.800000, loss: 0.002470, epsilon: 0.016240, episode:  106\n",
      "frames: 153000, reward: -12.800000, loss: 0.001626, epsilon: 0.016036, episode:  106\n",
      "frames: 154000, reward: -11.900000, loss: 0.002049, epsilon: 0.015838, episode:  107\n",
      "frames: 155000, reward: -11.900000, loss: 0.002023, epsilon: 0.015647, episode:  107\n",
      "frames: 156000, reward: -11.900000, loss: 0.000965, epsilon: 0.015461, episode:  107\n",
      "frames: 157000, reward: -10.800000, loss: 0.002365, epsilon: 0.015282, episode:  108\n",
      "frames: 158000, reward: -10.800000, loss: 0.004186, epsilon: 0.015109, episode:  108\n",
      "frames: 159000, reward: -11.000000, loss: 0.002690, epsilon: 0.014942, episode:  109\n",
      "frames: 160000, reward: -11.000000, loss: 0.001920, epsilon: 0.014780, episode:  109\n",
      "frames: 161000, reward: -11.000000, loss: 0.005759, epsilon: 0.014623, episode:  109\n",
      "frames: 162000, reward: -10.500000, loss: 0.001553, epsilon: 0.014471, episode:  110\n",
      "frames: 163000, reward: -10.500000, loss: 0.001237, epsilon: 0.014325, episode:  110\n",
      "frames: 164000, reward: -10.500000, loss: 0.002484, epsilon: 0.014183, episode:  110\n",
      "frames: 165000, reward: -10.100000, loss: 0.011437, epsilon: 0.014046, episode:  111\n",
      "frames: 166000, reward: -10.100000, loss: 0.002005, epsilon: 0.013913, episode:  111\n",
      "frames: 167000, reward: -10.100000, loss: 0.001322, epsilon: 0.013785, episode:  111\n",
      "frames: 168000, reward: -8.700000, loss: 0.002275, epsilon: 0.013661, episode:  112\n",
      "frames: 169000, reward: -8.700000, loss: 0.002625, epsilon: 0.013541, episode:  112\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "eps_decay = 30000\n",
    "frames = 1000000\n",
    "USE_CUDA = True\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "agent = DQN_PERAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\n",
    "\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "# tensorboard\n",
    "summary_writer = SummaryWriter(log_dir = \"DQN_PER\", comment= \"good_makeatari\")\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    " \n",
    "    if i % print_interval == 0:        \n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "        summary_writer.add_scalar(\"Temporal Difference Loss\", loss, i)\n",
    "        summary_writer.add_scalar(\"Mean Reward\", np.mean(all_rewards[-10:]), i)\n",
    "        summary_writer.add_scalar(\"Epsilon\", epsilon, i)\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n",
    "summary_writer.close()\n",
    "torch.save(agent.DQN.state_dict(), \"trained model/DQN_PER_dict.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(i, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
