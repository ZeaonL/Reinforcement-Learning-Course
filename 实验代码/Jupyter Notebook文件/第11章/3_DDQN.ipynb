{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN算法\n",
    "&emsp;&emsp;针对DQN中出现的高估问题， Hasselt等人提出了深度双Q网络算法（Double Deep Q-Network，DDQN）。该算法是将强化学习中的双Q学习应用于DQN中。在强化学习中，双Q学习的提出能在一定程度上缓解Q学习带来的过高估计问题，因此，DDQN方法为解决DQN的高估问题带来一个新的研究方向。<br>\n",
    "DDQN的主要思想是在目标值计算时将动作的选择和评估分离。在更新过程中，利用两个网络来学习两组权重，分别是预测网络的权重w和目标网络的权重$w^{'}$。在DQN中，动作选择和评估都是通过目标网络来实现的。而在DDQN中，计算目标Q值时，采取目标网络获取最优动作，再通过预测网络估计该最优动作的目标Q值，这样就可以将最优动作选择和动作值函数估计分离，采用不同的样本保证独立性。DDQN的目标Q值为：\n",
    "<center>$Y_{t}^{DoubleQ} = R_{t+1} + \\gamma{Q}(S_{t+1},argmax_a(S_{t+1},a;\\theta_t);\\theta_{t}^{'})$</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1c117dc710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=5):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer(object):\n",
    "    def __init__(self, memory_size=1000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.size() - 1)\n",
    "            data = self.buffer[idx]\n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "            \n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y_{t}^{DoubleQ} = R_{t+1} + \\gamma{Q}(S_{t+1},argmax_a(S_{t+1},a;\\theta_t);\\theta_{t}^{'})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer(memory_size)\n",
    "        self.DQN = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done, dtype = torch.uint8)  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states \n",
    "        ## Where DDQN is different from DQN\n",
    "        predicted_next_qvalues_current = self.DQN(next_states)\n",
    "        predicted_next_qvalues_target = self.DQN_target(next_states)\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues_target.gather(1, torch.max(predicted_next_qvalues_current, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values \n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory_buffer.size() - 1)\n",
    "            data = self.memory_buffer.buffer[idx]\n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:     0, reward:   nan, loss: 0.000000, epsilon: 1.000000, episode:    0\n",
      "frames:  1000, reward: -21.000000, loss: 0.000000, epsilon: 0.967544, episode:    1\n",
      "frames:  2000, reward: -21.000000, loss: 0.000000, epsilon: 0.936152, episode:    2\n",
      "frames:  3000, reward: -21.000000, loss: 0.000000, epsilon: 0.905789, episode:    3\n",
      "frames:  4000, reward: -21.000000, loss: 0.000000, epsilon: 0.876422, episode:    4\n",
      "frames:  5000, reward: -20.800000, loss: 0.000000, epsilon: 0.848017, episode:    5\n",
      "frames:  6000, reward: -20.714286, loss: 0.000000, epsilon: 0.820543, episode:    7\n",
      "frames:  7000, reward: -20.625000, loss: 0.000000, epsilon: 0.793971, episode:    8\n",
      "frames:  8000, reward: -20.666667, loss: 0.000000, epsilon: 0.768269, episode:    9\n",
      "frames:  9000, reward: -20.500000, loss: 0.000000, epsilon: 0.743410, episode:   10\n",
      "frames: 10000, reward: -20.400000, loss: 0.000525, epsilon: 0.719366, episode:   11\n",
      "frames: 11000, reward: -20.300000, loss: 0.000220, epsilon: 0.696110, episode:   12\n",
      "frames: 12000, reward: -20.300000, loss: 0.030004, epsilon: 0.673617, episode:   13\n",
      "frames: 13000, reward: -20.300000, loss: 0.015146, epsilon: 0.651861, episode:   14\n",
      "frames: 14000, reward: -20.400000, loss: 0.000184, epsilon: 0.630818, episode:   15\n",
      "frames: 15000, reward: -20.400000, loss: 0.000371, epsilon: 0.610465, episode:   16\n",
      "frames: 16000, reward: -20.500000, loss: 0.000155, epsilon: 0.590780, episode:   18\n",
      "frames: 17000, reward: -20.400000, loss: 0.015927, epsilon: 0.571740, episode:   19\n",
      "frames: 18000, reward: -20.500000, loss: 0.030314, epsilon: 0.553324, episode:   20\n",
      "frames: 19000, reward: -20.400000, loss: 0.030111, epsilon: 0.535511, episode:   21\n",
      "frames: 20000, reward: -20.400000, loss: 0.000089, epsilon: 0.518283, episode:   22\n",
      "frames: 21000, reward: -20.300000, loss: 0.000434, epsilon: 0.501619, episode:   23\n",
      "frames: 22000, reward: -20.300000, loss: 0.015495, epsilon: 0.485502, episode:   24\n",
      "frames: 23000, reward: -20.200000, loss: 0.000146, epsilon: 0.469913, episode:   25\n",
      "frames: 24000, reward: -20.200000, loss: 0.000103, epsilon: 0.454836, episode:   26\n",
      "frames: 25000, reward: -20.100000, loss: 0.023539, epsilon: 0.440252, episode:   28\n",
      "frames: 26000, reward: -20.200000, loss: 0.038738, epsilon: 0.426147, episode:   29\n",
      "frames: 27000, reward: -20.200000, loss: 0.015842, epsilon: 0.412504, episode:   30\n",
      "frames: 28000, reward: -20.100000, loss: 0.015150, epsilon: 0.399308, episode:   31\n",
      "frames: 29000, reward: -20.200000, loss: 0.015466, epsilon: 0.386545, episode:   32\n",
      "frames: 30000, reward: -20.100000, loss: 0.014957, epsilon: 0.374201, episode:   33\n",
      "frames: 31000, reward: -20.000000, loss: 0.015407, epsilon: 0.362261, episode:   34\n",
      "frames: 32000, reward: -20.000000, loss: 0.000343, epsilon: 0.350712, episode:   35\n",
      "frames: 33000, reward: -20.000000, loss: 0.000224, epsilon: 0.339542, episode:   36\n",
      "frames: 34000, reward: -20.100000, loss: 0.014707, epsilon: 0.328739, episode:   37\n",
      "frames: 35000, reward: -20.000000, loss: 0.014829, epsilon: 0.318289, episode:   38\n",
      "frames: 36000, reward: -20.000000, loss: 0.015523, epsilon: 0.308182, episode:   39\n",
      "frames: 37000, reward: -20.000000, loss: 0.000197, epsilon: 0.298407, episode:   40\n",
      "frames: 38000, reward: -20.200000, loss: 0.000804, epsilon: 0.288952, episode:   42\n",
      "frames: 39000, reward: -20.400000, loss: 0.002449, epsilon: 0.279806, episode:   43\n",
      "frames: 40000, reward: -20.500000, loss: 0.011777, epsilon: 0.270961, episode:   44\n",
      "frames: 41000, reward: -20.500000, loss: 0.030183, epsilon: 0.262406, episode:   45\n",
      "frames: 42000, reward: -20.600000, loss: 0.015696, epsilon: 0.254131, episode:   46\n",
      "frames: 43000, reward: -20.600000, loss: 0.000780, epsilon: 0.246127, episode:   48\n",
      "frames: 44000, reward: -20.500000, loss: 0.002093, epsilon: 0.238386, episode:   49\n",
      "frames: 45000, reward: -20.500000, loss: 0.006340, epsilon: 0.230899, episode:   49\n",
      "frames: 46000, reward: -20.200000, loss: 0.001667, epsilon: 0.223657, episode:   50\n",
      "frames: 47000, reward: -20.000000, loss: 0.008505, epsilon: 0.216652, episode:   51\n",
      "frames: 48000, reward: -20.000000, loss: 0.020131, epsilon: 0.209878, episode:   53\n",
      "frames: 49000, reward: -20.000000, loss: 0.019476, epsilon: 0.203325, episode:   54\n",
      "frames: 50000, reward: -20.000000, loss: 0.002957, epsilon: 0.196987, episode:   55\n",
      "frames: 51000, reward: -20.000000, loss: 0.005074, epsilon: 0.190857, episode:   56\n",
      "frames: 52000, reward: -20.100000, loss: 0.001635, epsilon: 0.184928, episode:   58\n",
      "frames: 53000, reward: -20.100000, loss: 0.002207, epsilon: 0.179193, episode:   59\n",
      "frames: 54000, reward: -20.400000, loss: 0.002205, epsilon: 0.173646, episode:   60\n",
      "frames: 55000, reward: -20.600000, loss: 0.005629, epsilon: 0.168281, episode:   61\n",
      "frames: 56000, reward: -20.500000, loss: 0.005300, epsilon: 0.163092, episode:   62\n",
      "frames: 57000, reward: -20.300000, loss: 0.009987, epsilon: 0.158073, episode:   64\n",
      "frames: 58000, reward: -20.300000, loss: 0.001633, epsilon: 0.153219, episode:   65\n",
      "frames: 59000, reward: -20.300000, loss: 0.001137, epsilon: 0.148523, episode:   66\n",
      "frames: 60000, reward: -20.200000, loss: 0.018031, epsilon: 0.143982, episode:   67\n",
      "frames: 61000, reward: -20.200000, loss: 0.002650, epsilon: 0.139589, episode:   68\n",
      "frames: 62000, reward: -20.200000, loss: 0.000918, epsilon: 0.135341, episode:   69\n",
      "frames: 63000, reward: -20.200000, loss: 0.003006, epsilon: 0.131232, episode:   71\n",
      "frames: 64000, reward: -20.200000, loss: 0.001331, epsilon: 0.127257, episode:   72\n",
      "frames: 65000, reward: -20.300000, loss: 0.004255, epsilon: 0.123413, episode:   73\n",
      "frames: 66000, reward: -20.400000, loss: 0.005040, epsilon: 0.119695, episode:   74\n",
      "frames: 67000, reward: -20.300000, loss: 0.005891, epsilon: 0.116099, episode:   75\n",
      "frames: 68000, reward: -20.200000, loss: 0.001742, epsilon: 0.112621, episode:   76\n",
      "frames: 69000, reward: -20.300000, loss: 0.003042, epsilon: 0.109256, episode:   77\n",
      "frames: 70000, reward: -20.400000, loss: 0.004658, epsilon: 0.106002, episode:   79\n",
      "frames: 71000, reward: -20.300000, loss: 0.001782, epsilon: 0.102855, episode:   80\n",
      "frames: 72000, reward: -20.300000, loss: 0.002187, epsilon: 0.099811, episode:   81\n",
      "frames: 73000, reward: -20.300000, loss: 0.000977, epsilon: 0.096866, episode:   82\n",
      "frames: 74000, reward: -20.300000, loss: 0.001179, epsilon: 0.094019, episode:   83\n",
      "frames: 75000, reward: -20.300000, loss: 0.003686, epsilon: 0.091264, episode:   84\n",
      "frames: 76000, reward: -20.400000, loss: 0.014871, epsilon: 0.088600, episode:   85\n",
      "frames: 77000, reward: -20.300000, loss: 0.004752, epsilon: 0.086023, episode:   86\n",
      "frames: 78000, reward: -20.300000, loss: 0.011497, epsilon: 0.083531, episode:   88\n",
      "frames: 79000, reward: -20.300000, loss: 0.001278, epsilon: 0.081120, episode:   89\n",
      "frames: 80000, reward: -20.500000, loss: 0.001375, epsilon: 0.078789, episode:   90\n",
      "frames: 81000, reward: -20.500000, loss: 0.000823, epsilon: 0.076533, episode:   91\n",
      "frames: 82000, reward: -20.500000, loss: 0.004811, epsilon: 0.074352, episode:   92\n",
      "frames: 83000, reward: -20.500000, loss: 0.001273, epsilon: 0.072243, episode:   93\n",
      "frames: 84000, reward: -20.500000, loss: 0.001432, epsilon: 0.070202, episode:   94\n",
      "frames: 85000, reward: -20.600000, loss: 0.001106, epsilon: 0.068228, episode:   95\n",
      "frames: 86000, reward: -20.800000, loss: 0.001793, epsilon: 0.066319, episode:   96\n",
      "frames: 87000, reward: -20.700000, loss: 0.001718, epsilon: 0.064473, episode:   97\n",
      "frames: 88000, reward: -20.700000, loss: 0.002558, epsilon: 0.062687, episode:   98\n",
      "frames: 89000, reward: -20.700000, loss: 0.001170, epsilon: 0.060960, episode:  100\n",
      "frames: 90000, reward: -20.800000, loss: 0.001445, epsilon: 0.059289, episode:  101\n",
      "frames: 91000, reward: -20.900000, loss: 0.002645, epsilon: 0.057673, episode:  102\n",
      "frames: 92000, reward: -20.900000, loss: 0.001306, epsilon: 0.056110, episode:  103\n",
      "frames: 93000, reward: -20.700000, loss: 0.001693, epsilon: 0.054599, episode:  104\n",
      "frames: 94000, reward: -20.600000, loss: 0.003327, epsilon: 0.053137, episode:  105\n",
      "frames: 95000, reward: -20.600000, loss: 0.001316, epsilon: 0.051722, episode:  106\n",
      "frames: 96000, reward: -20.700000, loss: 0.000689, epsilon: 0.050355, episode:  107\n",
      "frames: 97000, reward: -20.700000, loss: 0.003590, epsilon: 0.049032, episode:  108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames: 98000, reward: -20.700000, loss: 0.001503, epsilon: 0.047752, episode:  109\n",
      "frames: 99000, reward: -20.700000, loss: 0.009454, epsilon: 0.046514, episode:  110\n",
      "frames: 100000, reward: -20.700000, loss: 0.001626, epsilon: 0.045317, episode:  111\n",
      "frames: 101000, reward: -20.700000, loss: 0.002577, epsilon: 0.044159, episode:  112\n",
      "frames: 102000, reward: -20.700000, loss: 0.001259, epsilon: 0.043040, episode:  113\n",
      "frames: 103000, reward: -20.900000, loss: 0.003289, epsilon: 0.041956, episode:  114\n",
      "frames: 104000, reward: -20.900000, loss: 0.001674, epsilon: 0.040909, episode:  115\n",
      "frames: 105000, reward: -20.800000, loss: 0.002065, epsilon: 0.039895, episode:  116\n",
      "frames: 106000, reward: -20.700000, loss: 0.001460, epsilon: 0.038915, episode:  117\n",
      "frames: 107000, reward: -20.700000, loss: 0.002657, epsilon: 0.037967, episode:  118\n",
      "frames: 108000, reward: -20.600000, loss: 0.001927, epsilon: 0.037050, episode:  119\n",
      "frames: 109000, reward: -20.500000, loss: 0.001749, epsilon: 0.036164, episode:  120\n",
      "frames: 110000, reward: -20.400000, loss: 0.001994, epsilon: 0.035306, episode:  121\n",
      "frames: 111000, reward: -20.300000, loss: 0.000905, epsilon: 0.034476, episode:  122\n",
      "frames: 112000, reward: -20.200000, loss: 0.001973, epsilon: 0.033674, episode:  123\n",
      "frames: 113000, reward: -20.100000, loss: 0.000856, epsilon: 0.032898, episode:  124\n",
      "frames: 114000, reward: -20.200000, loss: 0.004396, epsilon: 0.032147, episode:  125\n",
      "frames: 115000, reward: -20.300000, loss: 0.005571, epsilon: 0.031421, episode:  127\n",
      "frames: 116000, reward: -20.300000, loss: 0.003758, epsilon: 0.030719, episode:  128\n",
      "frames: 117000, reward: -20.400000, loss: 0.001007, epsilon: 0.030039, episode:  129\n",
      "frames: 118000, reward: -20.400000, loss: 0.001708, epsilon: 0.029383, episode:  130\n",
      "frames: 119000, reward: -20.500000, loss: 0.003299, epsilon: 0.028747, episode:  131\n",
      "frames: 120000, reward: -20.600000, loss: 0.002215, epsilon: 0.028132, episode:  132\n",
      "frames: 121000, reward: -20.700000, loss: 0.002792, epsilon: 0.027538, episode:  133\n",
      "frames: 122000, reward: -20.800000, loss: 0.001621, epsilon: 0.026963, episode:  134\n",
      "frames: 123000, reward: -20.800000, loss: 0.004299, epsilon: 0.026407, episode:  135\n",
      "frames: 124000, reward: -20.900000, loss: 0.001437, epsilon: 0.025869, episode:  136\n",
      "frames: 125000, reward: -20.900000, loss: 0.001853, epsilon: 0.025349, episode:  136\n",
      "frames: 126000, reward: -20.700000, loss: 0.001272, epsilon: 0.024846, episode:  137\n",
      "frames: 127000, reward: -20.700000, loss: 0.004177, epsilon: 0.024359, episode:  137\n",
      "frames: 128000, reward: -20.700000, loss: 0.004786, epsilon: 0.023888, episode:  138\n",
      "frames: 129000, reward: -20.700000, loss: 0.004067, epsilon: 0.023433, episode:  139\n",
      "frames: 130000, reward: -20.800000, loss: 0.004066, epsilon: 0.022992, episode:  140\n",
      "frames: 131000, reward: -20.700000, loss: 0.001547, epsilon: 0.022567, episode:  141\n",
      "frames: 132000, reward: -20.700000, loss: 0.001682, epsilon: 0.022155, episode:  141\n",
      "frames: 133000, reward: -20.600000, loss: 0.002716, epsilon: 0.021756, episode:  142\n",
      "frames: 134000, reward: -20.500000, loss: 0.003025, epsilon: 0.021371, episode:  143\n",
      "frames: 135000, reward: -20.500000, loss: 0.003011, epsilon: 0.020998, episode:  143\n",
      "frames: 136000, reward: -20.400000, loss: 0.002378, epsilon: 0.020637, episode:  144\n",
      "frames: 137000, reward: -20.200000, loss: 0.001435, epsilon: 0.020289, episode:  145\n",
      "frames: 138000, reward: -20.200000, loss: 0.001341, epsilon: 0.019951, episode:  145\n",
      "frames: 139000, reward: -20.000000, loss: 0.003609, epsilon: 0.019625, episode:  146\n",
      "frames: 140000, reward: -20.000000, loss: 0.002755, epsilon: 0.019310, episode:  146\n",
      "frames: 141000, reward: -20.100000, loss: 0.003902, epsilon: 0.019004, episode:  147\n",
      "frames: 142000, reward: -19.900000, loss: 0.001937, epsilon: 0.018709, episode:  148\n",
      "frames: 143000, reward: -19.900000, loss: 0.004146, epsilon: 0.018424, episode:  148\n",
      "frames: 144000, reward: -19.400000, loss: 0.005989, epsilon: 0.018147, episode:  149\n",
      "frames: 145000, reward: -19.400000, loss: 0.008720, epsilon: 0.017880, episode:  149\n",
      "frames: 146000, reward: -19.100000, loss: 0.002522, epsilon: 0.017622, episode:  150\n",
      "frames: 147000, reward: -19.100000, loss: 0.002323, epsilon: 0.017372, episode:  150\n",
      "frames: 148000, reward: -19.000000, loss: 0.001771, epsilon: 0.017130, episode:  151\n",
      "frames: 149000, reward: -19.000000, loss: 0.001679, epsilon: 0.016897, episode:  152\n",
      "frames: 150000, reward: -19.000000, loss: 0.002293, epsilon: 0.016671, episode:  152\n",
      "frames: 151000, reward: -18.900000, loss: 0.001973, epsilon: 0.016452, episode:  153\n",
      "frames: 152000, reward: -18.900000, loss: 0.001710, epsilon: 0.016240, episode:  153\n",
      "frames: 153000, reward: -19.000000, loss: 0.001442, epsilon: 0.016036, episode:  154\n",
      "frames: 154000, reward: -19.000000, loss: 0.002892, epsilon: 0.015838, episode:  155\n",
      "frames: 155000, reward: -19.000000, loss: 0.002362, epsilon: 0.015647, episode:  155\n",
      "frames: 156000, reward: -19.000000, loss: 0.001413, epsilon: 0.015461, episode:  156\n",
      "frames: 157000, reward: -19.000000, loss: 0.001512, epsilon: 0.015282, episode:  156\n",
      "frames: 158000, reward: -19.000000, loss: 0.001725, epsilon: 0.015109, episode:  157\n",
      "frames: 159000, reward: -19.000000, loss: 0.001371, epsilon: 0.014942, episode:  157\n",
      "frames: 160000, reward: -19.000000, loss: 0.001591, epsilon: 0.014780, episode:  158\n",
      "frames: 161000, reward: -19.000000, loss: 0.005641, epsilon: 0.014623, episode:  158\n",
      "frames: 162000, reward: -19.300000, loss: 0.002923, epsilon: 0.014471, episode:  159\n",
      "frames: 163000, reward: -19.300000, loss: 0.003031, epsilon: 0.014325, episode:  160\n",
      "frames: 164000, reward: -19.300000, loss: 0.005111, epsilon: 0.014183, episode:  160\n",
      "frames: 165000, reward: -18.500000, loss: 0.005508, epsilon: 0.014046, episode:  161\n",
      "frames: 166000, reward: -18.500000, loss: 0.000818, epsilon: 0.013913, episode:  161\n",
      "frames: 167000, reward: -17.900000, loss: 0.004867, epsilon: 0.013785, episode:  162\n",
      "frames: 168000, reward: -17.800000, loss: 0.002572, epsilon: 0.013661, episode:  163\n",
      "frames: 169000, reward: -17.800000, loss: 0.001551, epsilon: 0.013541, episode:  163\n",
      "frames: 170000, reward: -17.800000, loss: 0.001698, epsilon: 0.013425, episode:  164\n",
      "frames: 171000, reward: -17.800000, loss: 0.001902, epsilon: 0.013313, episode:  165\n",
      "frames: 172000, reward: -17.800000, loss: 0.002627, epsilon: 0.013204, episode:  165\n",
      "frames: 173000, reward: -17.100000, loss: 0.001088, epsilon: 0.013099, episode:  166\n",
      "frames: 174000, reward: -17.100000, loss: 0.002219, epsilon: 0.012997, episode:  166\n",
      "frames: 175000, reward: -16.200000, loss: 0.001912, epsilon: 0.012899, episode:  167\n",
      "frames: 176000, reward: -16.200000, loss: 0.002643, epsilon: 0.012804, episode:  167\n",
      "frames: 177000, reward: -15.700000, loss: 0.001874, epsilon: 0.012712, episode:  168\n",
      "frames: 178000, reward: -15.400000, loss: 0.000890, epsilon: 0.012623, episode:  169\n",
      "frames: 179000, reward: -15.400000, loss: 0.003190, epsilon: 0.012537, episode:  169\n",
      "frames: 180000, reward: -15.300000, loss: 0.003005, epsilon: 0.012454, episode:  170\n",
      "frames: 181000, reward: -15.300000, loss: 0.003451, epsilon: 0.012374, episode:  170\n",
      "frames: 182000, reward: -16.100000, loss: 0.002396, epsilon: 0.012296, episode:  171\n",
      "frames: 183000, reward: -16.600000, loss: 0.002364, epsilon: 0.012220, episode:  172\n",
      "frames: 184000, reward: -16.600000, loss: 0.003150, epsilon: 0.012148, episode:  172\n",
      "frames: 185000, reward: -16.700000, loss: 0.001475, epsilon: 0.012077, episode:  173\n",
      "frames: 186000, reward: -16.700000, loss: 0.002124, epsilon: 0.012009, episode:  173\n",
      "frames: 187000, reward: -16.000000, loss: 0.001134, epsilon: 0.011943, episode:  174\n",
      "frames: 188000, reward: -16.000000, loss: 0.001896, epsilon: 0.011880, episode:  174\n",
      "frames: 189000, reward: -16.100000, loss: 0.001930, epsilon: 0.011818, episode:  175\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "eps_decay = 30000\n",
    "frames = 1000000\n",
    "USE_CUDA = True\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "agent = DDQNAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\n",
    "\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "# tensorboard\n",
    "summary_writer = SummaryWriter(log_dir = \"DDQN\", comment= \"good_makeatari\")\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "        summary_writer.add_scalar(\"Temporal Difference Loss\", loss, i)\n",
    "        summary_writer.add_scalar(\"Mean Reward\", np.mean(all_rewards[-10:]), i)\n",
    "        summary_writer.add_scalar(\"Epsilon\", epsilon, i)\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n",
    "summary_writer.close()\n",
    "torch.save(agent.DQN.state_dict(), \"trained model/DDQN_dict.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_training(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-100:])))\n",
    "    plt.plot(moving_average(rewards,20))\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss, average on 100 stpes')\n",
    "    plt.plot(moving_average(losses, 100),linewidth=0.2)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(i, all_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
