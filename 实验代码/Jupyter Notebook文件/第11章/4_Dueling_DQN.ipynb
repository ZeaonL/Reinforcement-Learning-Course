{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "&emsp;&emsp;Wang等人在DQN算法的基础上，通过对其网络结构进行改变，提出一种带竞争网络（Dueling network）的竞争DQN（Dueling DQN）算法。该算法采用一种无模型的强化学习神经网络结构，核心思想在于：在神经网络内部把动作值函数分解为状态值函数   和动作优势函数。这样把与动作相关的Q值函数转化为与动作无关的V值函数和与动作相关的优势函数两个部分。Dueling DQN算法的竞争网络结构<br>\n",
    "&emsp;&emsp;在竞争网络结构中，<br>\n",
    "&emsp;&emsp;（1）上路输出状态值函数，表示状态s本身所具有的价值；<br>\n",
    "&emsp;&emsp;（2）下路输出动作优势函数，表示在状态s处选择动作a时所得的到回报，与状态s本身的价值之间的（即所有可能动作带来的平均回报）的差异，也就是：<br>\n",
    "<center>A(s,a)=Q(s,a)-V(s)</center>\n",
    "\n",
    "&emsp;&emsp;（3）通过全连接将上下路合并（求和），输出动作值函数Q(s,a)。<br>\n",
    "&emsp;&emsp;Dueling DQN的出发点在于，多数状态没有必要估计每个动作的价值。在将Q函数的估计分为两步后，先估计状态的价值，在有新动作加入时，就可以基于现有的V函数来学习动作的价值，而不需要从零开始学习。因此，竞争网络能够有效解决奖励偏见（reward-bias）问题，动作越多，Q函数的泛化性也越好。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from IPython.display import clear_output\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1a6032ac10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dueling DQN的出发点在于，多数状态没有估计每个动作价值的必要。在将Q函数的估计分为两步后，先估计状态的价值，在有新动作加入时，就可以基于现有的V函数来学习动作的价值，而不需要从零开始学习。因此，竞争网络能够有效解决奖励偏见（reawra-bias）问题，动作越多，Q函数的泛化性也越好。<br>\n",
    "<center><img src=\"./image/图Dueling.png\"  height=\"500\" width=\"250\"></center><br>\n",
    "这样做会让Q值的学习更稳健一点。这是因为它系统的区分了那些reward是由状态带来的，哪些是由动作带来的。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_outputs):\n",
    "        super(Dueling_DQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_outputs\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer(object):\n",
    "    def __init__(self, memory_size=1000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.size() - 1)\n",
    "            data = self.buffer[idx]\n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "            \n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dueling_DQNAgent: \n",
    "    def __init__(self, input_shape, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer(memory_size)\n",
    "        self.Dueling_DQN = Dueling_DQN(input_shape = input_shape, num_outputs = action_space.n)\n",
    "        self.Dueling_DQN_target = Dueling_DQN(input_shape = input_shape, num_outputs = action_space.n)\n",
    "        self.Dueling_DQN_target.load_state_dict(self.Dueling_DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.Dueling_DQN = self.Dueling_DQN.cuda()\n",
    "            self.Dueling_DQN_target = self.Dueling_DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.Dueling_DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.Dueling_DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done,dtype = torch.uint8)  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.Dueling_DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "        predicted_next_qvalues = self.Dueling_DQN_target(next_states) # YOUR CODE\n",
    "\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] # YOUR CODE\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values # YOUR CODE\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory_buffer.size() - 1)\n",
    "            data = self.memory_buffer.buffer[idx]\n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.Dueling_DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ouyangz/.conda/envs/gym/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:     0, reward:   nan, loss: 0.000000, epsilon: 1.000000, episode:    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:  1000, reward:   nan, loss: 0.000000, epsilon: 0.967544, episode:    0\n",
      "frames:  2000, reward: -21.000000, loss: 0.000000, epsilon: 0.936152, episode:    1\n",
      "frames:  3000, reward: -20.000000, loss: 0.000000, epsilon: 0.905789, episode:    3\n",
      "frames:  4000, reward: -20.250000, loss: 0.000000, epsilon: 0.876422, episode:    4\n",
      "frames:  5000, reward: -20.400000, loss: 0.000000, epsilon: 0.848017, episode:    5\n",
      "frames:  6000, reward: -20.500000, loss: 0.000000, epsilon: 0.820543, episode:    6\n",
      "frames:  7000, reward: -20.625000, loss: 0.000000, epsilon: 0.793971, episode:    8\n",
      "frames:  8000, reward: -20.666667, loss: 0.000000, epsilon: 0.768269, episode:    9\n",
      "frames:  9000, reward: -20.700000, loss: 0.000000, epsilon: 0.743410, episode:   10\n",
      "frames: 10000, reward: -21.000000, loss: 0.000380, epsilon: 0.719366, episode:   12\n",
      "frames: 11000, reward: -20.900000, loss: 0.000253, epsilon: 0.696110, episode:   13\n",
      "frames: 12000, reward: -20.900000, loss: 0.000311, epsilon: 0.673617, episode:   14\n",
      "frames: 13000, reward: -20.800000, loss: 0.015007, epsilon: 0.651861, episode:   15\n",
      "frames: 14000, reward: -20.700000, loss: 0.000336, epsilon: 0.630818, episode:   16\n",
      "frames: 15000, reward: -20.700000, loss: 0.029459, epsilon: 0.610465, episode:   17\n",
      "frames: 16000, reward: -20.700000, loss: 0.015039, epsilon: 0.590780, episode:   19\n",
      "frames: 17000, reward: -20.700000, loss: 0.000496, epsilon: 0.571740, episode:   20\n",
      "frames: 18000, reward: -20.700000, loss: 0.010144, epsilon: 0.553324, episode:   21\n",
      "frames: 19000, reward: -20.600000, loss: 0.010362, epsilon: 0.535511, episode:   22\n",
      "frames: 20000, reward: -20.700000, loss: 0.003606, epsilon: 0.518283, episode:   23\n",
      "frames: 21000, reward: -20.700000, loss: 0.001801, epsilon: 0.501619, episode:   24\n",
      "frames: 22000, reward: -20.700000, loss: 0.011820, epsilon: 0.485502, episode:   26\n",
      "frames: 23000, reward: -20.600000, loss: 0.018337, epsilon: 0.469913, episode:   27\n",
      "frames: 24000, reward: -20.400000, loss: 0.013542, epsilon: 0.454836, episode:   28\n",
      "frames: 25000, reward: -20.300000, loss: 0.001952, epsilon: 0.440252, episode:   29\n",
      "frames: 26000, reward: -20.200000, loss: 0.000701, epsilon: 0.426147, episode:   30\n",
      "frames: 27000, reward: -20.200000, loss: 0.004472, epsilon: 0.412504, episode:   31\n",
      "frames: 28000, reward: -20.100000, loss: 0.008285, epsilon: 0.399308, episode:   32\n",
      "frames: 29000, reward: -19.900000, loss: 0.002054, epsilon: 0.386545, episode:   33\n",
      "frames: 30000, reward: -19.500000, loss: 0.002494, epsilon: 0.374201, episode:   34\n",
      "frames: 31000, reward: -19.600000, loss: 0.004305, epsilon: 0.362261, episode:   35\n",
      "frames: 32000, reward: -19.600000, loss: 0.006986, epsilon: 0.350712, episode:   36\n",
      "frames: 33000, reward: -19.800000, loss: 0.002577, epsilon: 0.339542, episode:   38\n",
      "frames: 34000, reward: -19.800000, loss: 0.001871, epsilon: 0.328739, episode:   39\n",
      "frames: 35000, reward: -19.900000, loss: 0.002620, epsilon: 0.318289, episode:   40\n",
      "frames: 36000, reward: -19.900000, loss: 0.008640, epsilon: 0.308182, episode:   41\n",
      "frames: 37000, reward: -20.100000, loss: 0.004966, epsilon: 0.298407, episode:   42\n",
      "frames: 38000, reward: -20.700000, loss: 0.000938, epsilon: 0.288952, episode:   44\n",
      "frames: 39000, reward: -20.700000, loss: 0.001820, epsilon: 0.279806, episode:   45\n",
      "frames: 40000, reward: -20.600000, loss: 0.002399, epsilon: 0.270961, episode:   46\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "eps_decay = 30000\n",
    "frames = 1000000\n",
    "USE_CUDA = True\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "frame = env.reset()\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "input_shape = frame._force().transpose(2,0,1).shape\n",
    "agent = Dueling_DQNAgent(input_shape = input_shape, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "# tensorboard\n",
    "summary_writer = SummaryWriter(log_dir = \"Dueling_DQN\", comment= \"good_makeatari\")\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "        summary_writer.add_scalar(\"Temporal Difference Loss\", loss, i)\n",
    "        summary_writer.add_scalar(\"Mean Reward\", np.mean(all_rewards[-10:]), i)\n",
    "        summary_writer.add_scalar(\"Epsilon\", epsilon, i)\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.Dueling_DQN_target.load_state_dict(agent.Dueling_DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n",
    "summary_writer.close()\n",
    "torch.save(agent.Dueling_DQN.state_dict(), \"trained model/Dueling_DQN_dict.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def plot_training(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-100:])))\n",
    "    plt.plot(moving_average(rewards,20))\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss, average on 100 stpes')\n",
    "    plt.plot(moving_average(losses, 100),linewidth=0.2)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(i, all_rewards, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
